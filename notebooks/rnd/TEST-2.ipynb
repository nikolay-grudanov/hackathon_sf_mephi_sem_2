{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+rocm6.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'  # Для AMD GPU\n",
    "import torch\n",
    "print(torch.__version__)  # Должно вывести: 2.5.1+rocm6.2\n",
    "print(torch.cuda.is_available())  # Проверка работы ROCm (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ruwordnet import RuWordNet\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c76005bd8c467e8360b203f5e4ff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268212d224e84bce9e4e1e1e61a3a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56253f2ebabb46ee8cadd7c77a6bfbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141ab79268b34e59b51bd22838d6e670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6001adebf76420d80c52724911c6a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87d853acf004b02a3c2039a15bbbc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4967db531646df91bf7abbdd6cd91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3074a5e674e0cb2543b9fc08ef362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c490ce579cde4a43bf2e969fd81e8046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d470790030c466ca029f96478b52704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04ee0b9f38452f96d9e1ca6025c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализация компонентов\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "# Инициализация RuWordNet\n",
    "wn = RuWordNet()  \n",
    "sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для предобработки текста: очистка, лемматизация, генерация эмбеддингов.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_synonyms=True):\n",
    "        # Инициализация компонентов Natasha\n",
    "        self.segmenter = segmenter\n",
    "        self.morph_tagger = morph_tagger\n",
    "        self.syntax_parser = syntax_parser\n",
    "        self.morph_vocab = morph_vocab\n",
    "        \n",
    "        # Настройки для синонимов\n",
    "        self.use_synonyms = use_synonyms\n",
    "        self.wn = wn if use_synonyms else None\n",
    "        \n",
    "        # Кэш для ускорения повторных обработок\n",
    "        self.lemma_cache = {}\n",
    "        self.synonym_cache = {}\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Очистка текста от лишних символов и нормализация\"\"\"\n",
    "        # Приведение к нижнему регистру и замена ё\n",
    "        text = text.lower().replace(\"ё\", \"е\")\n",
    "        \n",
    "        # Удаление URL-ссылок\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "        \n",
    "        # Удаление эмодзи и специальных символов\n",
    "        emoji_pattern = re.compile(\n",
    "            '['\n",
    "            u'\\U0001F600-\\U0001F64F'  # эмоции\n",
    "            u'\\U0001F300-\\U0001F5FF'  # символы\n",
    "            u'\\U0001F680-\\U0001F6FF'  # транспорт\n",
    "            u'\\U0001F700-\\U0001F77F'  # алхимия\n",
    "            u'\\U0001F780-\\U0001F7FF'  # геометрические фигуры\n",
    "            u'\\U0001F800-\\U0001F8FF'  # дополнительные символы\n",
    "            u'\\U0001F900-\\U0001F9FF'  # дополнительные символы-2\n",
    "            u'\\U0001FA00-\\U0001FA6F'  # шахматы\n",
    "            u'\\U0001FA70-\\U0001FAFF'  # дополнительные символы-3\n",
    "            u'\\U00002702-\\U000027B0'  # Dingbats\n",
    "            u'\\U000024C2-\\U0001F251'  # Enclosed\n",
    "            ']+', \n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "        text = emoji_pattern.sub(' ', text)\n",
    "        \n",
    "        # Удаление HTML-сущностей и специальных символов\n",
    "        text = re.sub(r'&[a-z]+;', ' ', text)\n",
    "        \n",
    "        # Удаление пунктуации, цифр и не-буквенных символов\n",
    "        text = re.sub(r'[^a-zа-я\\s]', ' ', text)\n",
    "        \n",
    "        # Удаление телеграм-упоминаний и бот-команд\n",
    "        text = re.sub(r'@\\w+|/\\w+', ' ', text)\n",
    "        \n",
    "        # Удаление лишних пробелов и обрезка\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"Лемматизация текста с помощью Natasha\"\"\"\n",
    "        # Проверка кэша\n",
    "        if text in self.lemma_cache:\n",
    "            return self.lemma_cache[text]\n",
    "            \n",
    "        # Обработка с Natasha\n",
    "        doc = Doc(text)\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)\n",
    "        \n",
    "        # Получение лемм\n",
    "        lemmas = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos != 'PUNCT':\n",
    "                token.lemmatize(self.morph_vocab)\n",
    "                lemmas.append(token.lemma)\n",
    "        \n",
    "        self.lemma_cache[text] = lemmas\n",
    "        return lemmas\n",
    "    \n",
    "    def get_synonyms(self, word, max_synonyms=3):\n",
    "        \"\"\"Получение синонимов из RuWordNet\"\"\"\n",
    "        if not self.use_synonyms:\n",
    "            return []\n",
    "            \n",
    "        # Проверка кэша\n",
    "        cache_key = f\"{word}_{max_synonyms}\"\n",
    "        if cache_key in self.synonym_cache:\n",
    "            return self.synonym_cache[cache_key]\n",
    "            \n",
    "        try:\n",
    "            synsets = self.wn.get_synsets(word)\n",
    "            synonyms = []\n",
    "            \n",
    "            if synsets:\n",
    "                for synset in synsets[:2]:\n",
    "                    for sense in synset.senses:\n",
    "                        if sense.word != word and sense.word not in synonyms:\n",
    "                            synonyms.append(sense.word)\n",
    "                            if len(synonyms) >= max_synonyms:\n",
    "                                break\n",
    "                    if len(synonyms) >= max_synonyms:\n",
    "                        break\n",
    "                        \n",
    "            self.synonym_cache[cache_key] = synonyms\n",
    "            return synonyms\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    def create_bigrams(self, tokens):\n",
    "        \"\"\"Создание биграмм из токенов\"\"\"\n",
    "        if len(tokens) < 2:\n",
    "            return []\n",
    "        return [' '.join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    \n",
    "    def process_document(self, text):\n",
    "        \"\"\"\n",
    "        Полная обработка документа с сохранением позиций токенов.\n",
    "        Возвращает структуру данных с информацией для индексации.\n",
    "        \"\"\"\n",
    "        # Очистка и лемматизация\n",
    "        clean_text = self.clean_text(text)\n",
    "        lemmas = self.lemmatize(clean_text)\n",
    "        \n",
    "        # Создание биграмм\n",
    "        bigrams = self.create_bigrams(lemmas)\n",
    "        \n",
    "        # Формирование результата с сохранением информации о позициях\n",
    "        tokens = []\n",
    "        token_positions = []\n",
    "        token_types = []  # 'unigram' или 'bigram'\n",
    "        \n",
    "        # Добавление униграмм\n",
    "        for i, lemma in enumerate(lemmas):\n",
    "            tokens.append(lemma)\n",
    "            token_positions.append(i)\n",
    "            token_types.append('unigram')\n",
    "        \n",
    "        # Добавление биграмм\n",
    "        for i, bigram in enumerate(bigrams):\n",
    "            tokens.append(bigram)\n",
    "            token_positions.append(i)  # Позиция начала биграммы\n",
    "            token_types.append('bigram')\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'clean_text': clean_text,\n",
    "            'lemmas': lemmas,\n",
    "            'tokens': tokens,\n",
    "            'token_positions': token_positions,\n",
    "            'token_types': token_types\n",
    "        }\n",
    "    \n",
    "    def process_query(self, text):\n",
    "        \"\"\"\n",
    "        Обработка поискового запроса с расширением синонимами.\n",
    "        \"\"\"\n",
    "        clean_text = self.clean_text(text)\n",
    "        lemmas = self.lemmatize(clean_text)\n",
    "        \n",
    "        # Расширение запроса синонимами\n",
    "        expanded_terms = lemmas.copy()\n",
    "        if self.use_synonyms:\n",
    "            for lemma in lemmas:\n",
    "                synonyms = self.get_synonyms(lemma)\n",
    "                expanded_terms.extend(synonyms)\n",
    "        \n",
    "        return {\n",
    "            'original_query': text,\n",
    "            'clean_query': clean_text,\n",
    "            'lemmas': lemmas,\n",
    "            'expanded_terms': expanded_terms\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor(use_synonyms=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\n",
      "твой лучший секс спрятан здесь делюсь каналом дипломированного сексолога крис взломала код классного секса мастерски раскрепощает знает миллион горячих техник и лучшие девайсы для взрослых самые полезные посты здесь отрезвляющий пост я все сама прокачай наездницу ролевая игра vip кинотеатр техника оральных ласк как занимается сeксом неудобная женщина кстати крис провела трехдневный безоплатный онлайн интенсив от бревна до богини совместно с врачом и владельцем секс шопа скорее смотри записи пока не удалила здесь жарче чем в аду\n",
      "['твой', 'хороший', 'секс', 'спрятать', 'здесь', 'делиться', 'канал', 'дипломированный', 'сексолог', 'крис']\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\"\n",
    "clean1 = preprocessor.process_document(txt1)\n",
    "print(clean1.get('original_text'))\n",
    "print(clean1.get('clean_text'))\n",
    "print(clean1.get('lemmas')[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "конфеты\n",
      "конфеты\n",
      "['конфета']\n",
      "['конфета']\n"
     ]
    }
   ],
   "source": [
    "query1 = preprocessor.process_query('конфеты')\n",
    "print(query1.get('original_query'))\n",
    "print(query1.get('clean_query'))\n",
    "print(query1.get('lemmas'))\n",
    "print(query1.get('expanded_terms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"114650-A\", title=\"БЛАГОЗВУЧНЫЙ\") ['БЛАГОЗВУЧНЫЙ', 'КРАСИВЫЙ', 'ПРИЯТНЫЙ НА СЛУХ', 'КРАСИВЫЙ НА СЛУХ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.get_synonyms('красивый')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(id=\"115077-A\", title=\"КРАСИВЫЙ НА ВИД\"),\n",
       " Synset(id=\"119209-A\", title=\"НРАВСТВЕННЫЙ, ЭТИЧНЫЙ\"),\n",
       " Synset(id=\"114650-A\", title=\"БЛАГОЗВУЧНЫЙ\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.get_synsets('красивый')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"126228-N\", title=\"СРЕДНЕВЕКОВЫЙ ЗАМОК\")\n",
      "Synset(id=\"114707-N\", title=\"ЗАМОК ДЛЯ ЗАПИРАНИЯ\")\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('замок'):\n",
    "    print(sense.synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"4454-N\", title=\"СОБАКА\") ['СОБАКА', 'ПЕС', 'СОБАЧКА', 'СОБАЧОНКА', 'ПСИНА', 'ЧЕТВЕРОНОГИЙ ДРУГ', 'ПЕСИК']\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('собака'):\n",
    "    print(sense.synset, [synonym.name for synonym in sense.synset.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"115077-A\", title=\"КРАСИВЫЙ НА ВИД\") ['КРАСИВЫЙ', 'ЭСТЕТИЧНЫЙ', 'КРАСИВЫЙ НА ВИД', 'ВНЕШНЕ КРАСИВЫЙ', 'КРАСИВЫЙ ВНЕШНЕ', 'КРАСИВЕЙШИЙ']\n",
      "Synset(id=\"119209-A\", title=\"НРАВСТВЕННЫЙ, ЭТИЧНЫЙ\") ['МОРАЛЬНЫЙ', 'КРАСИВЫЙ', 'ЧИСТЫЙ', 'НРАВСТВЕННЫЙ', 'ЭТИЧНЫЙ', 'ВЫСОКОНРАВСТВЕННЫЙ', 'ЧИСТЕЙШИЙ', 'ВЫСОКОМОРАЛЬНЫЙ', 'ВЫСОКОПОРЯДОЧНЫЙ', 'НРАВСТВЕННО ЧИСТЫЙ', 'НЕЗАМУТНЕННЫЙ', 'ЧИСТЕНЬКИЙ']\n",
      "Synset(id=\"114650-A\", title=\"БЛАГОЗВУЧНЫЙ\") ['БЛАГОЗВУЧНЫЙ', 'КРАСИВЫЙ', 'ПРИЯТНЫЙ НА СЛУХ', 'КРАСИВЫЙ НА СЛУХ']\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('красивый'):\n",
    "    print(sense.synset, [synonym.name for synonym in sense.synset.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms__(word):\n",
    "    synonyms = []\n",
    "    for synset in wn.get_synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synset' object has no attribute 'lemmas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m synonyms \u001b[38;5;241m=\u001b[39m [synonym\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m wn\u001b[38;5;241m.\u001b[39mget_synsets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mворона\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m synonym \u001b[38;5;129;01min\u001b[39;00m synset\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m synonyms \u001b[38;5;241m=\u001b[39m [synonym\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m wn\u001b[38;5;241m.\u001b[39mget_synsets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mворона\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m synonym \u001b[38;5;129;01min\u001b[39;00m \u001b[43msynset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmas\u001b[49m()]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Synset' object has no attribute 'lemmas'"
     ]
    }
   ],
   "source": [
    "synonyms = [synonym.name() for synset in wn.get_synsets(\"ворона\") for synonym in synset.lemmas()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissVectorStore:\n",
    "    \"\"\"\n",
    "    Класс для хранения и поиска векторов с использованием FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, embedding_dim=None):\n",
    "        \"\"\"\n",
    "        Инициализация хранилища векторов.\n",
    "        \n",
    "        Args:\n",
    "            model: модель для определения размерности эмбеддингов\n",
    "            embedding_dim: явно указанная размерность эмбеддингов\n",
    "        \"\"\"\n",
    "        # Если размерность не указана явно, определим с помощью модели\n",
    "        if embedding_dim is None and model is not None:\n",
    "            # Получаем размерность из модели, создав тестовый эмбеддинг\n",
    "            test_embedding = model.encode(\"тестовый текст\")\n",
    "            self.embedding_dim = test_embedding.shape[0]\n",
    "        else:\n",
    "            self.embedding_dim = embedding_dim or 768  # По умолчанию для BERT\n",
    "            \n",
    "        # Создание индекса FAISS\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        self.documents = []\n",
    "        self.token_metadata = []\n",
    "    \n",
    "    def add_document(self, doc_data, model):\n",
    "        \"\"\"\n",
    "        Добавление документа в индекс.\n",
    "        \n",
    "        Args:\n",
    "            doc_data: обработанные данные документа\n",
    "            model: модель для создания эмбеддингов\n",
    "        \"\"\"\n",
    "        # Сохраняем документ\n",
    "        doc_id = len(self.documents)\n",
    "        self.documents.append({\n",
    "            'id': doc_id,\n",
    "            'original_text': doc_data['original_text'],\n",
    "            'clean_text': doc_data['clean_text'],\n",
    "            'lemmas': doc_data['lemmas']\n",
    "        })\n",
    "        \n",
    "        # Создаем эмбеддинги для каждого токена\n",
    "        tokens = doc_data['tokens']\n",
    "        if not tokens:\n",
    "            return\n",
    "        \n",
    "        # Получаем эмбеддинги    \n",
    "        embeddings = model.encode(tokens)\n",
    "        \n",
    "        # Обеспечиваем правильную форму для одиночных эмбеддингов\n",
    "        if isinstance(embeddings, list):\n",
    "            embeddings = np.array(embeddings)\n",
    "        if len(embeddings.shape) == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "            \n",
    "        # Проверяем размерность\n",
    "        if embeddings.shape[1] != self.embedding_dim:\n",
    "            raise ValueError(f\"Размерность эмбеддингов ({embeddings.shape[1]}) не соответствует размерности индекса ({self.embedding_dim})\")\n",
    "        \n",
    "        # Добавляем метаданные токенов\n",
    "        for i, token in enumerate(tokens):\n",
    "            self.token_metadata.append({\n",
    "                'doc_id': doc_id,\n",
    "                'token': token,\n",
    "                'position': doc_data['token_positions'][i],\n",
    "                'type': doc_data['token_types'][i]\n",
    "            })\n",
    "        \n",
    "        # Добавляем эмбеддинги в индекс FAISS\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearcher:\n",
    "    \"\"\"\n",
    "    Класс для семантического поиска текста с использованием FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: TextPreprocessor, vector_store: FaissVectorStore, model):\n",
    "        \"\"\"\n",
    "        Инициализация поисковика.\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: класс для предобработки текста\n",
    "            vector_store: хранилище векторов\n",
    "            model: модель для эмбеддингов\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vector_store = vector_store\n",
    "        self.model = model\n",
    "    \n",
    "    def search(self, query: str, k: int = 5, threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Выполнение поиска по запросу.\n",
    "        \n",
    "        Args:\n",
    "            query: поисковый запрос\n",
    "            k: количество результатов\n",
    "            threshold: порог сходства\n",
    "            \n",
    "        Returns:\n",
    "            list: найденные совпадения\n",
    "        \"\"\"\n",
    "        # Обработка запроса\n",
    "        processed_query = self.preprocessor.process_query(query)\n",
    "        \n",
    "        # Получение эмбеддинга запроса\n",
    "        query_embedding = self.model.encode(processed_query['clean_query'])\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Поиск ближайших векторов в индексе\n",
    "        D, I = self.vector_store.index.search(query_embedding.astype(np.float32), k=min(k, self.vector_store.index.ntotal))\n",
    "        \n",
    "        # Формирование результатов\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(D[0], I[0])):\n",
    "            if score < threshold:\n",
    "                continue\n",
    "                \n",
    "            # Получение метаданных\n",
    "            metadata = self.vector_store.token_metadata[idx]\n",
    "            doc_id = metadata['doc_id']\n",
    "            document = self.vector_store.documents[doc_id]\n",
    "            \n",
    "            # Формирование результата\n",
    "            result = {\n",
    "                'document': document['original_text'],\n",
    "                'token': metadata['token'],\n",
    "                'position': metadata['position'],\n",
    "                'confidence': float(score),\n",
    "                'is_bigram': metadata['type'] == 'bigram',\n",
    "                'doc_id': doc_id\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Сортировка по убыванию уверенности\n",
    "        results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FaissVectorStore' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     vector_store\u001b[38;5;241m.\u001b[39madd_document(doc_data, sbert_model)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Сохранение индекса\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_search_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FaissVectorStore' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# Инициализация классов\n",
    "preprocessor = TextPreprocessor(use_synonyms=True)\n",
    "\n",
    "# Сначала создаем модель\n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "\n",
    "# Затем создаем хранилище векторов, передавая модель для определения размерности\n",
    "vector_store = FaissVectorStore(model=sbert_model)\n",
    "\n",
    "# Создаем поисковик\n",
    "searcher = SemanticSearcher(preprocessor, vector_store, sbert_model)\n",
    "\n",
    "# Добавление документов в индекс\n",
    "documents = [\n",
    "    \"он продал свой портрет\",\n",
    "    \"он вообще не собирается переезжать в другое государство\"\n",
    "]\n",
    "\n",
    "# Обработка и индексация документов\n",
    "for document in documents:\n",
    "    doc_data = preprocessor.process_document(document)\n",
    "    vector_store.add_document(doc_data, sbert_model)\n",
    "\n",
    "# Сохранение индекса\n",
    "vector_store.save(\"semantic_search_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Размерность эмбеддингов (1024) не соответствует размерности индекса (768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     14\u001b[0m     doc_data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mprocess_document(document)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Сохранение индекса\u001b[39;00m\n\u001b[1;32m     18\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_search_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 60\u001b[0m, in \u001b[0;36mFaissVectorStore.add_document\u001b[0;34m(self, doc_data, model)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Проверяем размерность\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mРазмерность эмбеддингов (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) не соответствует размерности индекса (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Добавляем метаданные токенов\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "\u001b[0;31mValueError\u001b[0m: Размерность эмбеддингов (1024) не соответствует размерности индекса (768)"
     ]
    }
   ],
   "source": [
    "# Инициализация классов\n",
    "preprocessor = TextPreprocessor(use_synonyms=True)\n",
    "vector_store = FaissVectorStore(embedding_dim=768)  # Размерность эмбеддингов SBERT\n",
    "searcher = SemanticSearcher(preprocessor, vector_store, sbert_model)\n",
    "\n",
    "# Добавление документов в индекс\n",
    "documents = [\n",
    "    \"он продал свой портрет\",\n",
    "    \"он вообще не собирается переезжать в другое государство\"\n",
    "]\n",
    "\n",
    "# Обработка и индексация документов\n",
    "for document in documents:\n",
    "    doc_data = preprocessor.process_document(document)\n",
    "    vector_store.add_document(doc_data, sbert_model)\n",
    "\n",
    "# Сохранение индекса\n",
    "vector_store.save(\"semantic_search_index\")\n",
    "\n",
    "# Поиск по запросам\n",
    "results1 = searcher.search(\"картина\", k=3)\n",
    "results2 = searcher.search(\"страна\", k=3)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Результаты для запроса 'картина':\")\n",
    "for r in results1:\n",
    "    print(f\"Найдено '{r['token']}' в документе '{r['document']}' на позиции {r['position']} с уверенностью {r['confidence']:.2f}\")\n",
    "\n",
    "print(\"\\nРезультаты для запроса 'страна':\")\n",
    "for r in results2:\n",
    "    print(f\"Найдено '{r['token']}' в документе '{r['document']}' на позиции {r['position']} с уверенностью {r['confidence']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очищенный текст: твой лучший секс спрятан здесь делюсь каналом дипломированного сексолога крис взломала код классного секса мастерски раскрепощает знает миллион горячих техник и лучшие девайсы для взрослых самые полезные посты здесь отрезвляющий пост я все сама прокачай наездницу ролевая игра vip кинотеатр техника оральных ласк как занимается сeксом неудобная женщина кстати крис провела трехдневный безоплатный онлайн интенсивот бревна до богини совместно с врачом и владельцем сексшопа скорее смотри записи пока не удалила httpstmesekretskris1048 здесь жарче чем в аду\n",
      "\n",
      "Токены первого предложения:\n",
      "твой -> твой (DET)\n",
      "лучший -> хороший (ADJ)\n",
      "секс -> секс (NOUN)\n",
      "спрятан -> спрятать (VERB)\n",
      "здесь -> здесь (ADV)\n",
      "делюсь -> делиться (VERB)\n",
      "каналом -> канал (NOUN)\n",
      "дипломированного -> дипломированный (ADJ)\n",
      "сексолога -> сексолог (ADJ)\n",
      "крис -> крис (NOUN)\n",
      "взломала -> взломать (VERB)\n",
      "код -> код (NOUN)\n",
      "классного -> классный (ADJ)\n",
      "секса -> секс (NOUN)\n",
      "мастерски -> мастерски (ADV)\n",
      "раскрепощает -> раскрепощать (NOUN)\n",
      "знает -> знать (VERB)\n",
      "миллион -> миллион (NOUN)\n",
      "горячих -> горячий (ADJ)\n",
      "техник -> техника (NOUN)\n",
      "и -> и (CCONJ)\n",
      "лучшие -> хороший (ADJ)\n",
      "девайсы -> девайс (NOUN)\n",
      "для -> для (ADP)\n",
      "взрослых -> взрослый (NOUN)\n",
      "самые -> самый (ADJ)\n",
      "полезные -> полезный (ADJ)\n",
      "посты -> пост (NOUN)\n",
      "здесь -> здесь (ADV)\n",
      "отрезвляющий -> отрезвлять (ADJ)\n",
      "пост -> пост (NOUN)\n",
      "я -> я (PRON)\n",
      "все -> весь (PRON)\n",
      "сама -> сам (ADJ)\n",
      "прокачай -> прокачать (ADJ)\n",
      "наездницу -> наездница (ADJ)\n",
      "ролевая -> ролевый (ADJ)\n",
      "игра -> игра (NOUN)\n",
      "vip -> vip (X)\n",
      "кинотеатр -> кинотеатр (NOUN)\n",
      "техника -> техника (NOUN)\n",
      "оральных -> оральный (ADJ)\n",
      "ласк -> ласка (NOUN)\n",
      "как -> как (SCONJ)\n",
      "занимается -> заниматься (VERB)\n",
      "с -> с (ADP)\n",
      "e -> e (X)\n",
      "ксом -> ксом (ADJ)\n",
      "неудобная -> неудобный (ADJ)\n",
      "женщина -> женщина (NOUN)\n",
      "кстати -> кстати (ADV)\n",
      "крис -> крис (NOUN)\n",
      "провела -> провести (VERB)\n",
      "трехдневный -> трехдневный (ADJ)\n",
      "безоплатный -> безоплатный (ADJ)\n",
      "онлайн -> онлайн (ADV)\n",
      "интенсивот -> интенсивот (ADJ)\n",
      "бревна -> бревно (NOUN)\n",
      "до -> до (ADP)\n",
      "богини -> богиня (NOUN)\n",
      "совместно -> совместно (ADV)\n",
      "с -> с (ADP)\n",
      "врачом -> врач (NOUN)\n",
      "и -> и (CCONJ)\n",
      "владельцем -> владелец (NOUN)\n",
      "сексшопа -> сексшоп (NOUN)\n",
      "скорее -> скорый (ADV)\n",
      "смотри -> смотри (ADV)\n",
      "записи -> запись (NOUN)\n",
      "пока -> пока (ADV)\n",
      "не -> не (PART)\n",
      "удалила -> удалить (VERB)\n",
      "httpstmesekretskris -> httpstmesekretskris (X)\n",
      "1048 -> 1048 (NUM)\n",
      "здесь -> здесь (ADV)\n",
      "жарче -> жарче (ADJ)\n",
      "чем -> чем (SCONJ)\n",
      "в -> в (ADP)\n",
      "аду -> ад (NOUN)\n",
      "\n",
      "Извлеченные сущности:\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor,\n",
    "    PER\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для лингвистической предобработки русскоязычных текстов с использованием Natasha\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Инициализация компонентов Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Предобработка текста: нижний регистр, удаление пунктуации, нормализация пробелов\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "        text = ' '.join(text.split())  # Нормализация пробелов\n",
    "        return text\n",
    "    \n",
    "    def process(self, text):\n",
    "        \"\"\"\n",
    "        Основной метод обработки текста\n",
    "        \n",
    "        Возвращает структуру:\n",
    "        {\n",
    "            \"clean_text\": str,\n",
    "            \"sentences\": [\n",
    "                {\n",
    "                    \"text\": str,\n",
    "                    \"tokens\": [\n",
    "                        {\n",
    "                            \"text\": str,\n",
    "                            \"lemma\": str,\n",
    "                            \"pos\": str,\n",
    "                            \"start\": int,\n",
    "                            \"stop\": int\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"entities\": [\n",
    "                {\n",
    "                    \"text\": str,\n",
    "                    \"normal\": str,\n",
    "                    \"type\": str,\n",
    "                    \"start\": int,\n",
    "                    \"stop\": int\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Шаг 1: Предобработка\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Шаг 2: Сегментация на предложения\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # Шаг 3: Токенизация (выполняется автоматически при сегментации)\n",
    "        \n",
    "        # Шаг 4: Морфологический анализ\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # Шаг 5: Лемматизация\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Шаг 6: Извлечение сущностей\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        # Шаг 7: Нормализация сущностей\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # Формирование выходной структуры\n",
    "        return self._build_output(doc, cleaned_text)\n",
    "    \n",
    "    def _build_output(self, doc, cleaned_text):\n",
    "        \"\"\"Формирует структуру данных для LLM\"\"\"\n",
    "        output = {\n",
    "            \"clean_text\": cleaned_text,\n",
    "            \"sentences\": [],\n",
    "            \"entities\": []\n",
    "        }\n",
    "        \n",
    "        # Обработка предложений\n",
    "        for sent in doc.sents:\n",
    "            sentence_data = {\n",
    "                \"text\": sent.text,\n",
    "                \"tokens\": []\n",
    "            }\n",
    "            \n",
    "            for token in sent.tokens:\n",
    "                token_data = {\n",
    "                    \"text\": token.text,\n",
    "                    \"lemma\": token.lemma,\n",
    "                    \"pos\": token.pos,\n",
    "                    \"start\": token.start,\n",
    "                    \"stop\": token.stop\n",
    "                }\n",
    "                sentence_data[\"tokens\"].append(token_data)\n",
    "            \n",
    "            output[\"sentences\"].append(sentence_data)\n",
    "        \n",
    "        # Обработка сущностей\n",
    "        for span in doc.spans:\n",
    "            entity_data = {\n",
    "                \"text\": span.text,\n",
    "                \"normal\": span.normal,\n",
    "                \"type\": span.type,\n",
    "                \"start\": span.start,\n",
    "                \"stop\": span.stop\n",
    "            }\n",
    "            output[\"entities\"].append(entity_data)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = TextPreprocessor()\n",
    "    # sample_text = \"Москва — столица России. Владимир Путин посетил завод в Подмосковье.\"\n",
    "    sample_text = \"Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\"\n",
    "    \n",
    "    result = preprocessor.process(sample_text)\n",
    "    print(\"Очищенный текст:\", result[\"clean_text\"])\n",
    "    print(\"\\nТокены первого предложения:\")\n",
    "    for token in result[\"sentences\"][0][\"tokens\"]:\n",
    "        print(f\"{token['text']} -> {token['lemma']} ({token['pos']})\")\n",
    "    \n",
    "    print(\"\\nИзвлеченные сущности:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"{entity['text']} -> {entity['normal']} ({entity['type']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, Doc)\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class NatashaPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для предобработки текста с использованием библиотеки Natasha.\n",
    "    \n",
    "    Выполняет предобработку русского текста для семантического поиска,\n",
    "    включая приведение к нижнему регистру, удаление пунктуации, \n",
    "    морфологический анализ и лемматизацию.\n",
    "    \n",
    "    Результатом является структура данных, которую можно передать \n",
    "    языковой модели для создания эмбеддингов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация предобработчика текста.\n",
    "        Создает необходимые компоненты библиотеки Natasha.\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.embedding = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.embedding)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.embedding)\n",
    "        self.ner_tagger = NewsNERTagger(self.embedding)\n",
    "\n",
    "    def preprocess(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Выполняет полную предобработку текста.\n",
    "        \n",
    "        Процесс обработки включает:\n",
    "        1. Предобработка (нижний регистр, удаление пунктуации)\n",
    "        2. Сегментация (разделение на предложения)\n",
    "        3. Токенизация (разделение на слова)\n",
    "        4. Морфоанализ (части речи, граммемы)\n",
    "        5. Лемматизация (нормальная форма)\n",
    "        6. NER (извлечение именованных сущностей)\n",
    "        7. Нормализация сущностей\n",
    "        \n",
    "        Args:\n",
    "            text (str): Исходный текст для обработки.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Структура данных с обработанным текстом.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: Если длина документа превышает 30 слов.\n",
    "        \"\"\"\n",
    "        # Проверка длины документа\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"Длина документа превышает максимально допустимые 30 слов\")\n",
    "        \n",
    "        # 1. Предобработка: перевод в нижний регистр, удаление пунктуации\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[\\W_]+', ' ', text)  # удаляем пунктуацию и спецсимволы\n",
    "\n",
    "        # Создаем объект Doc для Natasha\n",
    "        doc = Doc(text)\n",
    "\n",
    "        # 2. Сегментация\n",
    "        doc.segment(self.segmenter)\n",
    "\n",
    "        # 3. Токенизация\n",
    "        doc.tokens\n",
    "\n",
    "        # 4. Морфоанализ\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "\n",
    "        # 5. Лемматизация\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize()\n",
    "\n",
    "        # 6. NER\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "\n",
    "        # 7. Нормализация сущностей (приведение именованных сущностей к нормальной форме)\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_tagger)\n",
    "            entities.append({'text': span.normal, 'type': span.type, 'start': span.start, 'stop': span.stop})\n",
    "\n",
    "        # Формируем структуру для передачи LLM (список лемм и сущностей с позициями)\n",
    "        lemmas = [token.lemma for token in doc.tokens]\n",
    "        \n",
    "        # Добавляем информацию о позициях токенов и словосочетаниях\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            tokens_info.append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma,\n",
    "                'start': token.start,\n",
    "                'stop': token.stop\n",
    "            })\n",
    "        \n",
    "        # Формируем словосочетания (до 2 слов)\n",
    "        phrases = []\n",
    "        # Одиночные слова\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                'text': token['text'],\n",
    "                'lemma': token['lemma'],\n",
    "                'start': token['start'],\n",
    "                'stop': token['stop'],\n",
    "                'length': 1\n",
    "            })\n",
    "        \n",
    "        # Словосочетания из 2 слов\n",
    "        for i in range(len(tokens_info) - 1):\n",
    "            phrases.append({\n",
    "                'text': f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                'lemma': f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                'start': tokens_info[i]['start'],\n",
    "                'stop': tokens_info[i+1]['stop'],\n",
    "                'length': 2\n",
    "            })\n",
    "\n",
    "        result = {\n",
    "            'lemmas': lemmas,\n",
    "            'entities': entities,\n",
    "            'tokens': tokens_info,\n",
    "            'phrases': phrases\n",
    "        }\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NatashaPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DocToken.lemmatize() missing 1 required positional argument: 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mПривет, как дела?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 73\u001b[0m, in \u001b[0;36mNatashaPreprocessor.preprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 5. Лемматизация\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mtokens:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 6. NER\u001b[39;00m\n\u001b[1;32m     76\u001b[0m doc\u001b[38;5;241m.\u001b[39mtag_ner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tagger)\n",
      "\u001b[0;31mTypeError\u001b[0m: DocToken.lemmatize() missing 1 required positional argument: 'vocab'"
     ]
    }
   ],
   "source": [
    "x.preprocess(\"Привет, как дела?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Леммы: ['москва', 'столица', 'россия', 'владимир', 'путин', 'провести', 'совещание']\n",
      "\n",
      "Сущности:\n",
      "\n",
      "Словосочетания:\n",
      "Москва  столиц -> москва столица\n",
      " столиц  Росси -> столица россия\n",
      " Росси . Владим -> россия владимир\n",
      ". Владим р Пут -> владимир путин\n",
      "р Пут н пров -> путин провести\n",
      "н пров л совещан -> провести совещание\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor,\n",
    "    NewsSyntaxParser,\n",
    "    PER,\n",
    "    LOC,\n",
    "    ORG\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Полнофункциональный препроцессор текста для семантического поиска\n",
    "    с соблюдением требований хакатона.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Нормализация с сохранением позиций для дефисов/пунктуации\"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum():\n",
    "                cleaned.append(char.lower())\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            elif char.isspace():\n",
    "                cleaned.append(' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                # Сохраняем позиции для пунктуации/дефисов, но не включаем в очищенный текст\n",
    "                original_to_clean.append(None)\n",
    "\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    \n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"Преобразует позиции из очищенного текста в исходный\"\"\"\n",
    "        orig_start = None\n",
    "        orig_stop = None\n",
    "        \n",
    "        for i, val in enumerate(mapping):\n",
    "            if val == start:\n",
    "                orig_start = i\n",
    "            if val == stop-1:\n",
    "                orig_stop = i+1\n",
    "            if orig_start is not None and orig_stop is not None:\n",
    "                break\n",
    "                \n",
    "        return (orig_start, orig_stop) if orig_start is not None and orig_stop is not None else (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"Основной метод обработки с ограничениями\"\"\"\n",
    "        # Проверка длины документа\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"Документ превышает максимальную длину в 30 слов\")\n",
    "        \n",
    "        # Предобработка с сохранением позиций\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Обработка через Natasha\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        doc.parse_syntax(NewsSyntaxParser(self.emb))\n",
    "\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        for span in doc.spans:\n",
    "            # Нормализуем ВСЕ сущности, а не только PER\n",
    "            span.normalize(self.morph_vocab)\n",
    "            \n",
    "            # Добавляем обработку для всех типов\n",
    "            if span.type in [PER, LOC, ORG]:\n",
    "                if span.type == PER:\n",
    "                    span.extract_fact(self.names_extractor)\n",
    "                # Добавляем сущность в результат\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,  # <- Исправлено: было span.type\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # Формирование структуры\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"Создает финальную структуру данных\"\"\"\n",
    "        # Собираем все токены\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            orig_start, orig_stop = self._get_original_positions(\n",
    "                token.start, token.stop, pos_mapping\n",
    "            )\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[orig_start:orig_stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": orig_start,\n",
    "                \"stop\": orig_stop\n",
    "            })\n",
    "        \n",
    "        # Формируем словосочетания (1-2 слова)\n",
    "        phrases = []\n",
    "        used_spans = set()\n",
    "        \n",
    "        # Одиночные слова\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        \n",
    "        # Пары слов\n",
    "        for i in range(len(tokens_info)-1):\n",
    "            phrase = {\n",
    "                \"type\": \"phrase\",\n",
    "                \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i+1]['stop'],\n",
    "                \"length\": 2\n",
    "            }\n",
    "            phrases.append(phrase)\n",
    "        \n",
    "        # Сущности из NER\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "            entities.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": span.normal,\n",
    "                \"type\": span.type,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop,\n",
    "                \"length\": len(span.tokens)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"Москва - столица России. Владимир Путин провел совещание.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"Леммы:\", result[\"lemmas\"])\n",
    "        print(\"\\nСущности:\")\n",
    "        for ent in result[\"entities\"]:\n",
    "            print(f\"{ent['text']} ({ent['type']}): {ent['lemma']}\")\n",
    "        \n",
    "        print(\"\\nСловосочетания:\")\n",
    "        for phrase in result[\"phrases\"]:\n",
    "            if phrase[\"length\"] == 2:\n",
    "                print(f\"{phrase['text']} -> {phrase['lemma']}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Леммы: ['москва', 'столица', 'россия', 'президент', 'владимир', 'путин', 'провести', 'совещание']\n",
      "\n",
      "Сущности:\n",
      "\n",
      "Словосочетания:\n",
      "Москва  столиц -> москва столица\n",
      " столиц  Росси -> столица россия\n",
      " Росси  Президе -> россия президент\n",
      " Президе т Владим -> президент владимир\n",
      "т Владим р Пут -> владимир путин\n",
      "р Пут н пров -> путин провести\n",
      "н пров л совещан -> провести совещание\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для предобработки текста с учетом требований хакатона:\n",
    "    - Ограничение длины документа (30 слов)\n",
    "    - Извлечение сущностей (PER, LOC, ORG)\n",
    "    - Формирование словосочетаний (1-2 слова)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Инициализация компонентов Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # Убрал параметр labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Нормализация текста с сохранением позиций:\n",
    "        - Приведение к нижнему регистру\n",
    "        - Удаление пунктуации (кроме пробелов)\n",
    "        - Возвращает очищенный текст и маппинг позиций\n",
    "        \"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"\n",
    "        Преобразует позиции из очищенного текста в исходный\n",
    "        с учетом удаленных символов\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ищем первую не-None позицию в диапазоне\n",
    "            orig_start = next(mapping[i] for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            # Ищем последнюю не-None позицию\n",
    "            orig_stop = next(mapping[i] for i in reversed(range(stop)) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"\n",
    "        Основной метод обработки текста:\n",
    "        - Проверяет длину документа\n",
    "        - Выполняет полный цикл обработки через Natasha\n",
    "        - Возвращает структурированные данные\n",
    "        \"\"\"\n",
    "        # Проверка длины документа\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"Документ превышает максимальную длину в 30 слов\")\n",
    "        \n",
    "        # Предобработка текста\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Обработка через Natasha\n",
    "        doc.segment(self.segmenter)       # Сегментация на предложения\n",
    "        doc.tag_morph(self.morph_tagger)  # Морфологический анализ\n",
    "        doc.parse_syntax(self.syntax_parser)  # Синтаксический анализ (ВАЖНО для NER)\n",
    "        doc.tag_ner(self.ner_tagger)      # Извлечение сущностей\n",
    "        \n",
    "        # Лемматизация токенов\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Нормализация сущностей\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # Формирование выходной структуры\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"\n",
    "        Формирует итоговую структуру данных:\n",
    "        - Леммы\n",
    "        - Токены с позициями\n",
    "        - Словосочетания\n",
    "        - Сущности\n",
    "        \"\"\"\n",
    "        # Сбор информации о токенах\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            orig_start, orig_stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[orig_start:orig_stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": orig_start,\n",
    "                \"stop\": orig_stop\n",
    "            })\n",
    "        \n",
    "        # Формирование словосочетаний (1-2 слова)\n",
    "        phrases = []\n",
    "        \n",
    "        # 1. Одиночные слова\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        \n",
    "        # 2. Пары слов (только внутри одного предложения)\n",
    "        current_sentence_end = 0\n",
    "        for sent in doc.sents:\n",
    "            sentence_tokens = [t for t in tokens_info if t[\"start\"] >= current_sentence_end]\n",
    "            if sentence_tokens:\n",
    "                current_sentence_end = sentence_tokens[-1][\"stop\"]\n",
    "                \n",
    "                # Генерируем пары только внутри предложения\n",
    "                for i in range(len(sentence_tokens) - 1):\n",
    "                    token1 = sentence_tokens[i]\n",
    "                    token2 = sentence_tokens[i+1]\n",
    "                    phrases.append({\n",
    "                        \"type\": \"phrase\",\n",
    "                        \"text\": f\"{token1['text']} {token2['text']}\",\n",
    "                        \"lemma\": f\"{token1['lemma']} {token2['lemma']}\",\n",
    "                        \"start\": token1[\"start\"],\n",
    "                        \"stop\": token2[\"stop\"],\n",
    "                        \"length\": 2\n",
    "                    })\n",
    "        \n",
    "        # Извлечение сущностей\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "            \n",
    "            # Сравниваем с названиями типов в виде строк\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,  # тип уже является строкой\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    sample_text = \"Москва — столица России. Президент Владимир Путин провел совещание.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(sample_text)\n",
    "        print(\"Леммы:\", result[\"lemmas\"])\n",
    "        print(\"\\nСущности:\")\n",
    "        for ent in result[\"entities\"]:\n",
    "            print(f\"{ent['text']} ({ent['type']}): {ent['lemma']}\")\n",
    "        \n",
    "        print(\"\\nСловосочетания:\")\n",
    "        for phrase in result[\"phrases\"]:\n",
    "            if phrase[\"length\"] == 2:\n",
    "                print(f\"{phrase['text']} -> {phrase['lemma']}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemmas': ['москва', 'столица', 'россия', 'владимир', 'путин', 'провести', 'совещание'], 'tokens': [{'text': 'Москва', 'lemma': 'москва', 'pos': 'VERB', 'start': 0, 'stop': 6}, {'text': ' столиц', 'lemma': 'столица', 'pos': 'NOUN', 'start': 8, 'stop': 15}, {'text': ' Росси', 'lemma': 'россия', 'pos': 'NOUN', 'start': 16, 'stop': 22}, {'text': 'Москва — столица России. Владим', 'lemma': 'владимир', 'pos': 'NOUN', 'start': None, 'stop': 31}, {'text': 'р Пут', 'lemma': 'путин', 'pos': 'PROPN', 'start': 32, 'stop': 37}, {'text': 'н пров', 'lemma': 'провести', 'pos': 'VERB', 'start': 38, 'stop': 44}, {'text': 'л совещан', 'lemma': 'совещание', 'pos': 'NOUN', 'start': 45, 'stop': 54}], 'phrases': [{'type': 'word', 'text': 'Москва', 'lemma': 'москва', 'start': 0, 'stop': 6, 'length': 1}, {'type': 'word', 'text': ' столиц', 'lemma': 'столица', 'start': 8, 'stop': 15, 'length': 1}, {'type': 'word', 'text': ' Росси', 'lemma': 'россия', 'start': 16, 'stop': 22, 'length': 1}, {'type': 'word', 'text': 'Москва — столица России. Владим', 'lemma': 'владимир', 'start': None, 'stop': 31, 'length': 1}, {'type': 'word', 'text': 'р Пут', 'lemma': 'путин', 'start': 32, 'stop': 37, 'length': 1}, {'type': 'word', 'text': 'н пров', 'lemma': 'провести', 'start': 38, 'stop': 44, 'length': 1}, {'type': 'word', 'text': 'л совещан', 'lemma': 'совещание', 'start': 45, 'stop': 54, 'length': 1}, {'type': 'phrase', 'text': 'Москва  столиц', 'lemma': 'москва столица', 'start': 0, 'stop': 15, 'length': 2}, {'type': 'phrase', 'text': ' столиц  Росси', 'lemma': 'столица россия', 'start': 8, 'stop': 22, 'length': 2}, {'type': 'phrase', 'text': ' Росси Москва — столица России. Владим', 'lemma': 'россия владимир', 'start': 16, 'stop': 31, 'length': 2}, {'type': 'phrase', 'text': 'Москва — столица России. Владим р Пут', 'lemma': 'владимир путин', 'start': None, 'stop': 37, 'length': 2}, {'type': 'phrase', 'text': 'р Пут н пров', 'lemma': 'путин провести', 'start': 32, 'stop': 44, 'length': 2}, {'type': 'phrase', 'text': 'н пров л совещан', 'lemma': 'провести совещание', 'start': 38, 'stop': 54, 'length': 2}], 'entities': [], 'original_text': 'Москва — столица России. Владимир Путин провел совещание.', 'clean_text': 'москва  столица россии владимир путин провел совещание'}\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # Исправлено: убран параметр labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        try:\n",
    "            orig_start = mapping[start]\n",
    "            orig_stop = mapping[stop-1] + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except (IndexError, StopIteration):\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"Документ превышает 30 слов\")\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        phrases = []\n",
    "        # Одиночные слова\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        # Биграммы\n",
    "        for i in range(len(tokens_info)-1):\n",
    "            phrases.append({\n",
    "                \"type\": \"phrase\",\n",
    "                \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i+1]['stop'],\n",
    "                \"length\": 2\n",
    "            })\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:  # Сравнение по строковым значениям\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"Москва — столица России. Владимир Путин провел совещание.\"\n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(result)\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сущности: []\n",
      "Леммы: ['москва', 'столица', 'россия', 'владимир', 'путин', 'провести', 'совещание']\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)  # Добавлен синтаксический анализ\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        try:\n",
    "            orig_start = next(i for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            orig_stop = next(i for i in reversed(range(stop, len(mapping))) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"Документ превышает 30 слов\")\n",
    "        \n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Обязательные этапы обработки\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)  # Критически важно для NER\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Нормализация сущностей\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        \n",
    "        # Формирование словосочетаний\n",
    "        phrases = []\n",
    "        for i in range(len(tokens_info)):\n",
    "            phrases.append({\n",
    "                \"text\": tokens_info[i]['text'],\n",
    "                \"lemma\": tokens_info[i]['lemma'],\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i]['stop'],\n",
    "                \"length\": 1\n",
    "            })\n",
    "            if i < len(tokens_info)-1:\n",
    "                phrases.append({\n",
    "                    \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                    \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                    \"start\": tokens_info[i]['start'],\n",
    "                    \"stop\": tokens_info[i+1]['stop'],\n",
    "                    \"length\": 2\n",
    "                })\n",
    "        \n",
    "        # Извлечение сущностей\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [t['lemma'] for t in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# Пример использования с проверкой\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"Москва — столица России. Владимир Путин провел совещание.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"Сущности:\", [(ent[\"text\"], ent[\"type\"]) for ent in result[\"entities\"]])\n",
    "        print(\"Леммы:\", result[\"lemmas\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сущности: []\n",
      "Леммы: ['москва', 'столица', 'россия', 'владимир', 'путин', 'провести', 'совещание']\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Инициализация компонентов Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # Исправлено: убран параметр labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Нормализация текста с сохранением позиций\"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"Корректный маппинг позиций с учетом удаленных символов\"\"\"\n",
    "        try:\n",
    "            orig_start = next(i for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            orig_stop = next(i for i in reversed(range(stop)) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"Основной метод обработки текста\"\"\"\n",
    "        # Проверка длины документа\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"Документ превышает 30 слов\")\n",
    "        \n",
    "        # Предобработка текста\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Полный цикл обработки\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)  # Обязательно для NER\n",
    "        doc.tag_ner(self.ner_tagger)  # Критически важный шаг\n",
    "        \n",
    "        # Лемматизация\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Обработка сущностей\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':  # Сравнение по строке\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"Формирование итоговой структуры\"\"\"\n",
    "        # Токены\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        \n",
    "        # Словосочетания\n",
    "        phrases = []\n",
    "        for i in range(len(tokens_info)):\n",
    "            # Одиночные слова\n",
    "            phrases.append({\n",
    "                \"text\": tokens_info[i]['text'],\n",
    "                \"lemma\": tokens_info[i]['lemma'],\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i]['stop'],\n",
    "                \"length\": 1\n",
    "            })\n",
    "            # Биграммы\n",
    "            if i < len(tokens_info)-1:\n",
    "                phrases.append({\n",
    "                    \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                    \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                    \"start\": tokens_info[i]['start'],\n",
    "                    \"stop\": tokens_info[i+1]['stop'],\n",
    "                    \"length\": 2\n",
    "                })\n",
    "        \n",
    "        # Сущности\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:  # Фильтрация по строковым типам\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [t['lemma'] for t in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"Москва — столица России. Владимир Путин провел совещание.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"Сущности:\", [(ent[\"text\"], ent[\"type\"]) for ent in result[\"entities\"]])\n",
    "        print(\"Леммы:\", result[\"lemmas\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "19823.33s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"info\"\n"
     ]
    }
   ],
   "source": [
    "!pip info natasha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
