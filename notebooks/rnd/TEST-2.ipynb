{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+rocm6.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'  # –î–ª—è AMD GPU\n",
    "import torch\n",
    "print(torch.__version__)  # –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏: 2.5.1+rocm6.2\n",
    "print(torch.cuda.is_available())  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã ROCm (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ruwordnet import RuWordNet\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c76005bd8c467e8360b203f5e4ff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268212d224e84bce9e4e1e1e61a3a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56253f2ebabb46ee8cadd7c77a6bfbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141ab79268b34e59b51bd22838d6e670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6001adebf76420d80c52724911c6a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87d853acf004b02a3c2039a15bbbc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4967db531646df91bf7abbdd6cd91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3074a5e674e0cb2543b9fc08ef362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c490ce579cde4a43bf2e969fd81e8046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d470790030c466ca029f96478b52704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04ee0b9f38452f96d9e1ca6025c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RuWordNet\n",
    "wn = RuWordNet()  \n",
    "sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∫–∞, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_synonyms=True):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha\n",
    "        self.segmenter = segmenter\n",
    "        self.morph_tagger = morph_tagger\n",
    "        self.syntax_parser = syntax_parser\n",
    "        self.morph_vocab = morph_vocab\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤\n",
    "        self.use_synonyms = use_synonyms\n",
    "        self.wn = wn if use_synonyms else None\n",
    "        \n",
    "        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫\n",
    "        self.lemma_cache = {}\n",
    "        self.synonym_cache = {}\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\"\"\"\n",
    "        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –∑–∞–º–µ–Ω–∞ —ë\n",
    "        text = text.lower().replace(\"—ë\", \"–µ\")\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ URL-—Å—Å—ã–ª–æ–∫\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        emoji_pattern = re.compile(\n",
    "            '['\n",
    "            u'\\U0001F600-\\U0001F64F'  # —ç–º–æ—Ü–∏–∏\n",
    "            u'\\U0001F300-\\U0001F5FF'  # —Å–∏–º–≤–æ–ª—ã\n",
    "            u'\\U0001F680-\\U0001F6FF'  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n",
    "            u'\\U0001F700-\\U0001F77F'  # –∞–ª—Ö–∏–º–∏—è\n",
    "            u'\\U0001F780-\\U0001F7FF'  # –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏–≥—É—Ä—ã\n",
    "            u'\\U0001F800-\\U0001F8FF'  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\n",
    "            u'\\U0001F900-\\U0001F9FF'  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-2\n",
    "            u'\\U0001FA00-\\U0001FA6F'  # —à–∞—Ö–º–∞—Ç—ã\n",
    "            u'\\U0001FA70-\\U0001FAFF'  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-3\n",
    "            u'\\U00002702-\\U000027B0'  # Dingbats\n",
    "            u'\\U000024C2-\\U0001F251'  # Enclosed\n",
    "            ']+', \n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "        text = emoji_pattern.sub(' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        text = re.sub(r'&[a-z]+;', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, —Ü–∏—Ñ—Ä –∏ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        text = re.sub(r'[^a-z–∞-—è\\s]', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–ª–µ–≥—Ä–∞–º-—É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ –±–æ—Ç-–∫–æ–º–∞–Ω–¥\n",
    "        text = re.sub(r'@\\w+|/\\w+', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –æ–±—Ä–µ–∑–∫–∞\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Natasha\"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞\n",
    "        if text in self.lemma_cache:\n",
    "            return self.lemma_cache[text]\n",
    "            \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å Natasha\n",
    "        doc = Doc(text)\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)\n",
    "        \n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–µ–º–º\n",
    "        lemmas = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos != 'PUNCT':\n",
    "                token.lemmatize(self.morph_vocab)\n",
    "                lemmas.append(token.lemma)\n",
    "        \n",
    "        self.lemma_cache[text] = lemmas\n",
    "        return lemmas\n",
    "    \n",
    "    def get_synonyms(self, word, max_synonyms=3):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏–∑ RuWordNet\"\"\"\n",
    "        if not self.use_synonyms:\n",
    "            return []\n",
    "            \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞\n",
    "        cache_key = f\"{word}_{max_synonyms}\"\n",
    "        if cache_key in self.synonym_cache:\n",
    "            return self.synonym_cache[cache_key]\n",
    "            \n",
    "        try:\n",
    "            synsets = self.wn.get_synsets(word)\n",
    "            synonyms = []\n",
    "            \n",
    "            if synsets:\n",
    "                for synset in synsets[:2]:\n",
    "                    for sense in synset.senses:\n",
    "                        if sense.word != word and sense.word not in synonyms:\n",
    "                            synonyms.append(sense.word)\n",
    "                            if len(synonyms) >= max_synonyms:\n",
    "                                break\n",
    "                    if len(synonyms) >= max_synonyms:\n",
    "                        break\n",
    "                        \n",
    "            self.synonym_cache[cache_key] = synonyms\n",
    "            return synonyms\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    def create_bigrams(self, tokens):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –±–∏–≥—Ä–∞–º–º –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤\"\"\"\n",
    "        if len(tokens) < 2:\n",
    "            return []\n",
    "        return [' '.join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    \n",
    "    def process_document(self, text):\n",
    "        \"\"\"\n",
    "        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.\n",
    "        \"\"\"\n",
    "        # –û—á–∏—Å—Ç–∫–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "        clean_text = self.clean_text(text)\n",
    "        lemmas = self.lemmatize(clean_text)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–≥—Ä–∞–º–º\n",
    "        bigrams = self.create_bigrams(lemmas)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–∏—Ü–∏—è—Ö\n",
    "        tokens = []\n",
    "        token_positions = []\n",
    "        token_types = []  # 'unigram' –∏–ª–∏ 'bigram'\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–Ω–∏–≥—Ä–∞–º–º\n",
    "        for i, lemma in enumerate(lemmas):\n",
    "            tokens.append(lemma)\n",
    "            token_positions.append(i)\n",
    "            token_types.append('unigram')\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∏–≥—Ä–∞–º–º\n",
    "        for i, bigram in enumerate(bigrams):\n",
    "            tokens.append(bigram)\n",
    "            token_positions.append(i)  # –ü–æ–∑–∏—Ü–∏—è –Ω–∞—á–∞–ª–∞ –±–∏–≥—Ä–∞–º–º—ã\n",
    "            token_types.append('bigram')\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'clean_text': clean_text,\n",
    "            'lemmas': lemmas,\n",
    "            'tokens': tokens,\n",
    "            'token_positions': token_positions,\n",
    "            'token_types': token_types\n",
    "        }\n",
    "    \n",
    "    def process_query(self, text):\n",
    "        \"\"\"\n",
    "        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏.\n",
    "        \"\"\"\n",
    "        clean_text = self.clean_text(text)\n",
    "        lemmas = self.lemmatize(clean_text)\n",
    "        \n",
    "        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏\n",
    "        expanded_terms = lemmas.copy()\n",
    "        if self.use_synonyms:\n",
    "            for lemma in lemmas:\n",
    "                synonyms = self.get_synonyms(lemma)\n",
    "                expanded_terms.extend(synonyms)\n",
    "        \n",
    "        return {\n",
    "            'original_query': text,\n",
    "            'clean_query': clean_text,\n",
    "            'lemmas': lemmas,\n",
    "            'expanded_terms': expanded_terms\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor(use_synonyms=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà\n",
      "—Ç–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å –¥–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞ –∫—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞ –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö —Å–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å –æ—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç —è –≤—Å–µ —Å–∞–º–∞ –ø—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É —Ä–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ vip –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä —Ç–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ –∫–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞ –∫—Å—Ç–∞—Ç–∏ –∫—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤ –æ—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –±–æ–≥–∏–Ω–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å —à–æ–ø–∞ —Å–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏ –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ –∑–¥–µ—Å—å –∂–∞—Ä—á–µ —á–µ–º –≤ –∞–¥—É\n",
      "['—Ç–≤–æ–π', '—Ö–æ—Ä–æ—à–∏–π', '—Å–µ–∫—Å', '—Å–ø—Ä—è—Ç–∞—Ç—å', '–∑–¥–µ—Å—å', '–¥–µ–ª–∏—Ç—å—Å—è', '–∫–∞–Ω–∞–ª', '–¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '—Å–µ–∫—Å–æ–ª–æ–≥', '–∫—Ä–∏—Å']\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà\"\n",
    "clean1 = preprocessor.process_document(txt1)\n",
    "print(clean1.get('original_text'))\n",
    "print(clean1.get('clean_text'))\n",
    "print(clean1.get('lemmas')[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–∫–æ–Ω—Ñ–µ—Ç—ã\n",
      "–∫–æ–Ω—Ñ–µ—Ç—ã\n",
      "['–∫–æ–Ω—Ñ–µ—Ç–∞']\n",
      "['–∫–æ–Ω—Ñ–µ—Ç–∞']\n"
     ]
    }
   ],
   "source": [
    "query1 = preprocessor.process_query('–∫–æ–Ω—Ñ–µ—Ç—ã')\n",
    "print(query1.get('original_query'))\n",
    "print(query1.get('clean_query'))\n",
    "print(query1.get('lemmas'))\n",
    "print(query1.get('expanded_terms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"114650-A\", title=\"–ë–õ–ê–ì–û–ó–í–£–ß–ù–´–ô\") ['–ë–õ–ê–ì–û–ó–í–£–ß–ù–´–ô', '–ö–†–ê–°–ò–í–´–ô', '–ü–†–ò–Ø–¢–ù–´–ô –ù–ê –°–õ–£–•', '–ö–†–ê–°–ò–í–´–ô –ù–ê –°–õ–£–•']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.get_synonyms('–∫—Ä–∞—Å–∏–≤—ã–π')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(id=\"115077-A\", title=\"–ö–†–ê–°–ò–í–´–ô –ù–ê –í–ò–î\"),\n",
       " Synset(id=\"119209-A\", title=\"–ù–†–ê–í–°–¢–í–ï–ù–ù–´–ô, –≠–¢–ò–ß–ù–´–ô\"),\n",
       " Synset(id=\"114650-A\", title=\"–ë–õ–ê–ì–û–ó–í–£–ß–ù–´–ô\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.get_synsets('–∫—Ä–∞—Å–∏–≤—ã–π')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"126228-N\", title=\"–°–†–ï–î–ù–ï–í–ï–ö–û–í–´–ô –ó–ê–ú–û–ö\")\n",
      "Synset(id=\"114707-N\", title=\"–ó–ê–ú–û–ö –î–õ–Ø –ó–ê–ü–ò–†–ê–ù–ò–Ø\")\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('–∑–∞–º–æ–∫'):\n",
    "    print(sense.synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"4454-N\", title=\"–°–û–ë–ê–ö–ê\") ['–°–û–ë–ê–ö–ê', '–ü–ï–°', '–°–û–ë–ê–ß–ö–ê', '–°–û–ë–ê–ß–û–ù–ö–ê', '–ü–°–ò–ù–ê', '–ß–ï–¢–í–ï–†–û–ù–û–ì–ò–ô –î–†–£–ì', '–ü–ï–°–ò–ö']\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('—Å–æ–±–∞–∫–∞'):\n",
    "    print(sense.synset, [synonym.name for synonym in sense.synset.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"115077-A\", title=\"–ö–†–ê–°–ò–í–´–ô –ù–ê –í–ò–î\") ['–ö–†–ê–°–ò–í–´–ô', '–≠–°–¢–ï–¢–ò–ß–ù–´–ô', '–ö–†–ê–°–ò–í–´–ô –ù–ê –í–ò–î', '–í–ù–ï–®–ù–ï –ö–†–ê–°–ò–í–´–ô', '–ö–†–ê–°–ò–í–´–ô –í–ù–ï–®–ù–ï', '–ö–†–ê–°–ò–í–ï–ô–®–ò–ô']\n",
      "Synset(id=\"119209-A\", title=\"–ù–†–ê–í–°–¢–í–ï–ù–ù–´–ô, –≠–¢–ò–ß–ù–´–ô\") ['–ú–û–†–ê–õ–¨–ù–´–ô', '–ö–†–ê–°–ò–í–´–ô', '–ß–ò–°–¢–´–ô', '–ù–†–ê–í–°–¢–í–ï–ù–ù–´–ô', '–≠–¢–ò–ß–ù–´–ô', '–í–´–°–û–ö–û–ù–†–ê–í–°–¢–í–ï–ù–ù–´–ô', '–ß–ò–°–¢–ï–ô–®–ò–ô', '–í–´–°–û–ö–û–ú–û–†–ê–õ–¨–ù–´–ô', '–í–´–°–û–ö–û–ü–û–†–Ø–î–û–ß–ù–´–ô', '–ù–†–ê–í–°–¢–í–ï–ù–ù–û –ß–ò–°–¢–´–ô', '–ù–ï–ó–ê–ú–£–¢–ù–ï–ù–ù–´–ô', '–ß–ò–°–¢–ï–ù–¨–ö–ò–ô']\n",
      "Synset(id=\"114650-A\", title=\"–ë–õ–ê–ì–û–ó–í–£–ß–ù–´–ô\") ['–ë–õ–ê–ì–û–ó–í–£–ß–ù–´–ô', '–ö–†–ê–°–ò–í–´–ô', '–ü–†–ò–Ø–¢–ù–´–ô –ù–ê –°–õ–£–•', '–ö–†–ê–°–ò–í–´–ô –ù–ê –°–õ–£–•']\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('–∫—Ä–∞—Å–∏–≤—ã–π'):\n",
    "    print(sense.synset, [synonym.name for synonym in sense.synset.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms__(word):\n",
    "    synonyms = []\n",
    "    for synset in wn.get_synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synset' object has no attribute 'lemmas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m synonyms \u001b[38;5;241m=\u001b[39m [synonym\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m wn\u001b[38;5;241m.\u001b[39mget_synsets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–≤–æ—Ä–æ–Ω–∞\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m synonym \u001b[38;5;129;01min\u001b[39;00m synset\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m synonyms \u001b[38;5;241m=\u001b[39m [synonym\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m wn\u001b[38;5;241m.\u001b[39mget_synsets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–≤–æ—Ä–æ–Ω–∞\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m synonym \u001b[38;5;129;01min\u001b[39;00m \u001b[43msynset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmas\u001b[49m()]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Synset' object has no attribute 'lemmas'"
     ]
    }
   ],
   "source": [
    "synonyms = [synonym.name() for synset in wn.get_synsets(\"–≤–æ—Ä–æ–Ω–∞\") for synonym in synset.lemmas()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissVectorStore:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, embedding_dim=None):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤.\n",
    "        \n",
    "        Args:\n",
    "            model: –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "            embedding_dim: —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        \"\"\"\n",
    "        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞ —è–≤–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª–∏–º —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏\n",
    "        if embedding_dim is None and model is not None:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏, —Å–æ–∑–¥–∞–≤ —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "            test_embedding = model.encode(\"—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç\")\n",
    "            self.embedding_dim = test_embedding.shape[0]\n",
    "        else:\n",
    "            self.embedding_dim = embedding_dim or 768  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è BERT\n",
    "            \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ FAISS\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        self.documents = []\n",
    "        self.token_metadata = []\n",
    "    \n",
    "    def add_document(self, doc_data, model):\n",
    "        \"\"\"\n",
    "        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å.\n",
    "        \n",
    "        Args:\n",
    "            doc_data: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "            model: –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        \"\"\"\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "        doc_id = len(self.documents)\n",
    "        self.documents.append({\n",
    "            'id': doc_id,\n",
    "            'original_text': doc_data['original_text'],\n",
    "            'clean_text': doc_data['clean_text'],\n",
    "            'lemmas': doc_data['lemmas']\n",
    "        })\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        tokens = doc_data['tokens']\n",
    "        if not tokens:\n",
    "            return\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏    \n",
    "        embeddings = model.encode(tokens)\n",
    "        \n",
    "        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        if isinstance(embeddings, list):\n",
    "            embeddings = np.array(embeddings)\n",
    "        if len(embeddings.shape) == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "            \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "        if embeddings.shape[1] != self.embedding_dim:\n",
    "            raise ValueError(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({embeddings.shape[1]}) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ ({self.embedding_dim})\")\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        for i, token in enumerate(tokens):\n",
    "            self.token_metadata.append({\n",
    "                'doc_id': doc_id,\n",
    "                'token': token,\n",
    "                'position': doc_data['token_positions'][i],\n",
    "                'type': doc_data['token_types'][i]\n",
    "            })\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –∏–Ω–¥–µ–∫—Å FAISS\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearcher:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: TextPreprocessor, vector_store: FaissVectorStore, model):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞.\n",
    "        \n",
    "        Args:\n",
    "            preprocessor: –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "            vector_store: —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "            model: –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vector_store = vector_store\n",
    "        self.model = model\n",
    "    \n",
    "    def search(self, query: str, k: int = 5, threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.\n",
    "        \n",
    "        Args:\n",
    "            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "            k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            threshold: –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "            \n",
    "        Returns:\n",
    "            list: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è\n",
    "        \"\"\"\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "        processed_query = self.preprocessor.process_query(query)\n",
    "        \n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "        query_embedding = self.model.encode(processed_query['clean_query'])\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ\n",
    "        D, I = self.vector_store.index.search(query_embedding.astype(np.float32), k=min(k, self.vector_store.index.ntotal))\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(D[0], I[0])):\n",
    "            if score < threshold:\n",
    "                continue\n",
    "                \n",
    "            # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "            metadata = self.vector_store.token_metadata[idx]\n",
    "            doc_id = metadata['doc_id']\n",
    "            document = self.vector_store.documents[doc_id]\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "            result = {\n",
    "                'document': document['original_text'],\n",
    "                'token': metadata['token'],\n",
    "                'position': metadata['position'],\n",
    "                'confidence': float(score),\n",
    "                'is_bigram': metadata['type'] == 'bigram',\n",
    "                'doc_id': doc_id\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "        results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FaissVectorStore' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     vector_store\u001b[38;5;241m.\u001b[39madd_document(doc_data, sbert_model)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_search_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FaissVectorStore' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "preprocessor = TextPreprocessor(use_synonyms=True)\n",
    "\n",
    "# –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "\n",
    "# –ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤, –ø–µ—Ä–µ–¥–∞–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "vector_store = FaissVectorStore(model=sbert_model)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤–∏–∫\n",
    "searcher = SemanticSearcher(preprocessor, vector_store, sbert_model)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å\n",
    "documents = [\n",
    "    \"–æ–Ω –ø—Ä–æ–¥–∞–ª —Å–≤–æ–π –ø–æ—Ä—Ç—Ä–µ—Ç\",\n",
    "    \"–æ–Ω –≤–æ–æ–±—â–µ –Ω–µ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –ø–µ—Ä–µ–µ–∑–∂–∞—Ç—å –≤ –¥—Ä—É–≥–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ\"\n",
    "]\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "for document in documents:\n",
    "    doc_data = preprocessor.process_document(document)\n",
    "    vector_store.add_document(doc_data, sbert_model)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞\n",
    "vector_store.save(\"semantic_search_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (1024) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ (768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     14\u001b[0m     doc_data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mprocess_document(document)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞\u001b[39;00m\n\u001b[1;32m     18\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_search_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 60\u001b[0m, in \u001b[0;36mFaissVectorStore.add_document\u001b[0;34m(self, doc_data, model)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "\u001b[0;31mValueError\u001b[0m: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (1024) –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ (768)"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "preprocessor = TextPreprocessor(use_synonyms=True)\n",
    "vector_store = FaissVectorStore(embedding_dim=768)  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ SBERT\n",
    "searcher = SemanticSearcher(preprocessor, vector_store, sbert_model)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å\n",
    "documents = [\n",
    "    \"–æ–Ω –ø—Ä–æ–¥–∞–ª —Å–≤–æ–π –ø–æ—Ä—Ç—Ä–µ—Ç\",\n",
    "    \"–æ–Ω –≤–æ–æ–±—â–µ –Ω–µ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –ø–µ—Ä–µ–µ–∑–∂–∞—Ç—å –≤ –¥—Ä—É–≥–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ\"\n",
    "]\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "for document in documents:\n",
    "    doc_data = preprocessor.process_document(document)\n",
    "    vector_store.add_document(doc_data, sbert_model)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞\n",
    "vector_store.save(\"semantic_search_index\")\n",
    "\n",
    "# –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º\n",
    "results1 = searcher.search(\"–∫–∞—Ä—Ç–∏–Ω–∞\", k=3)\n",
    "results2 = searcher.search(\"—Å—Ç—Ä–∞–Ω–∞\", k=3)\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '–∫–∞—Ä—Ç–∏–Ω–∞':\")\n",
    "for r in results1:\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ '{r['token']}' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{r['document']}' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {r['position']} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {r['confidence']:.2f}\")\n",
    "\n",
    "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '—Å—Ç—Ä–∞–Ω–∞':\")\n",
    "for r in results2:\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ '{r['token']}' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ '{r['document']}' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {r['position']} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {r['confidence']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: —Ç–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å –¥–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞ –∫—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞ –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö —Å–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å –æ—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç —è –≤—Å–µ —Å–∞–º–∞ –ø—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É —Ä–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ vip –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä —Ç–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ –∫–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞ –∫—Å—Ç–∞—Ç–∏ –∫—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤–æ—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –±–æ–≥–∏–Ω–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å—à–æ–ø–∞ —Å–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏ –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ httpstmesekretskris1048 –∑–¥–µ—Å—å –∂–∞—Ä—á–µ —á–µ–º –≤ –∞–¥—É\n",
      "\n",
      "–¢–æ–∫–µ–Ω—ã –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:\n",
      "—Ç–≤–æ–π -> —Ç–≤–æ–π (DET)\n",
      "–ª—É—á—à–∏–π -> —Ö–æ—Ä–æ—à–∏–π (ADJ)\n",
      "—Å–µ–∫—Å -> —Å–µ–∫—Å (NOUN)\n",
      "—Å–ø—Ä—è—Ç–∞–Ω -> —Å–ø—Ä—è—Ç–∞—Ç—å (VERB)\n",
      "–∑–¥–µ—Å—å -> –∑–¥–µ—Å—å (ADV)\n",
      "–¥–µ–ª—é—Å—å -> –¥–µ–ª–∏—Ç—å—Å—è (VERB)\n",
      "–∫–∞–Ω–∞–ª–æ–º -> –∫–∞–Ω–∞–ª (NOUN)\n",
      "–¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ -> –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π (ADJ)\n",
      "—Å–µ–∫—Å–æ–ª–æ–≥–∞ -> —Å–µ–∫—Å–æ–ª–æ–≥ (ADJ)\n",
      "–∫—Ä–∏—Å -> –∫—Ä–∏—Å (NOUN)\n",
      "–≤–∑–ª–æ–º–∞–ª–∞ -> –≤–∑–ª–æ–º–∞—Ç—å (VERB)\n",
      "–∫–æ–¥ -> –∫–æ–¥ (NOUN)\n",
      "–∫–ª–∞—Å—Å–Ω–æ–≥–æ -> –∫–ª–∞—Å—Å–Ω—ã–π (ADJ)\n",
      "—Å–µ–∫—Å–∞ -> —Å–µ–∫—Å (NOUN)\n",
      "–º–∞—Å—Ç–µ—Ä—Å–∫–∏ -> –º–∞—Å—Ç–µ—Ä—Å–∫–∏ (ADV)\n",
      "—Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç -> —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞—Ç—å (NOUN)\n",
      "–∑–Ω–∞–µ—Ç -> –∑–Ω–∞—Ç—å (VERB)\n",
      "–º–∏–ª–ª–∏–æ–Ω -> –º–∏–ª–ª–∏–æ–Ω (NOUN)\n",
      "–≥–æ—Ä—è—á–∏—Ö -> –≥–æ—Ä—è—á–∏–π (ADJ)\n",
      "—Ç–µ—Ö–Ω–∏–∫ -> —Ç–µ—Ö–Ω–∏–∫–∞ (NOUN)\n",
      "–∏ -> –∏ (CCONJ)\n",
      "–ª—É—á—à–∏–µ -> —Ö–æ—Ä–æ—à–∏–π (ADJ)\n",
      "–¥–µ–≤–∞–π—Å—ã -> –¥–µ–≤–∞–π—Å (NOUN)\n",
      "–¥–ª—è -> –¥–ª—è (ADP)\n",
      "–≤–∑—Ä–æ—Å–ª—ã—Ö -> –≤–∑—Ä–æ—Å–ª—ã–π (NOUN)\n",
      "—Å–∞–º—ã–µ -> —Å–∞–º—ã–π (ADJ)\n",
      "–ø–æ–ª–µ–∑–Ω—ã–µ -> –ø–æ–ª–µ–∑–Ω—ã–π (ADJ)\n",
      "–ø–æ—Å—Ç—ã -> –ø–æ—Å—Ç (NOUN)\n",
      "–∑–¥–µ—Å—å -> –∑–¥–µ—Å—å (ADV)\n",
      "–æ—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π -> –æ—Ç—Ä–µ–∑–≤–ª—è—Ç—å (ADJ)\n",
      "–ø–æ—Å—Ç -> –ø–æ—Å—Ç (NOUN)\n",
      "—è -> —è (PRON)\n",
      "–≤—Å–µ -> –≤–µ—Å—å (PRON)\n",
      "—Å–∞–º–∞ -> —Å–∞–º (ADJ)\n",
      "–ø—Ä–æ–∫–∞—á–∞–π -> –ø—Ä–æ–∫–∞—á–∞—Ç—å (ADJ)\n",
      "–Ω–∞–µ–∑–¥–Ω–∏—Ü—É -> –Ω–∞–µ–∑–¥–Ω–∏—Ü–∞ (ADJ)\n",
      "—Ä–æ–ª–µ–≤–∞—è -> —Ä–æ–ª–µ–≤—ã–π (ADJ)\n",
      "–∏–≥—Ä–∞ -> –∏–≥—Ä–∞ (NOUN)\n",
      "vip -> vip (X)\n",
      "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä -> –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä (NOUN)\n",
      "—Ç–µ—Ö–Ω–∏–∫–∞ -> —Ç–µ—Ö–Ω–∏–∫–∞ (NOUN)\n",
      "–æ—Ä–∞–ª—å–Ω—ã—Ö -> –æ—Ä–∞–ª—å–Ω—ã–π (ADJ)\n",
      "–ª–∞—Å–∫ -> –ª–∞—Å–∫–∞ (NOUN)\n",
      "–∫–∞–∫ -> –∫–∞–∫ (SCONJ)\n",
      "–∑–∞–Ω–∏–º–∞–µ—Ç—Å—è -> –∑–∞–Ω–∏–º–∞—Ç—å—Å—è (VERB)\n",
      "—Å -> —Å (ADP)\n",
      "e -> e (X)\n",
      "–∫—Å–æ–º -> –∫—Å–æ–º (ADJ)\n",
      "–Ω–µ—É–¥–æ–±–Ω–∞—è -> –Ω–µ—É–¥–æ–±–Ω—ã–π (ADJ)\n",
      "–∂–µ–Ω—â–∏–Ω–∞ -> –∂–µ–Ω—â–∏–Ω–∞ (NOUN)\n",
      "–∫—Å—Ç–∞—Ç–∏ -> –∫—Å—Ç–∞—Ç–∏ (ADV)\n",
      "–∫—Ä–∏—Å -> –∫—Ä–∏—Å (NOUN)\n",
      "–ø—Ä–æ–≤–µ–ª–∞ -> –ø—Ä–æ–≤–µ—Å—Ç–∏ (VERB)\n",
      "—Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π -> —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π (ADJ)\n",
      "–±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π -> –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π (ADJ)\n",
      "–æ–Ω–ª–∞–π–Ω -> –æ–Ω–ª–∞–π–Ω (ADV)\n",
      "–∏–Ω—Ç–µ–Ω—Å–∏–≤–æ—Ç -> –∏–Ω—Ç–µ–Ω—Å–∏–≤–æ—Ç (ADJ)\n",
      "–±—Ä–µ–≤–Ω–∞ -> –±—Ä–µ–≤–Ω–æ (NOUN)\n",
      "–¥–æ -> –¥–æ (ADP)\n",
      "–±–æ–≥–∏–Ω–∏ -> –±–æ–≥–∏–Ω—è (NOUN)\n",
      "—Å–æ–≤–º–µ—Å—Ç–Ω–æ -> —Å–æ–≤–º–µ—Å—Ç–Ω–æ (ADV)\n",
      "—Å -> —Å (ADP)\n",
      "–≤—Ä–∞—á–æ–º -> –≤—Ä–∞—á (NOUN)\n",
      "–∏ -> –∏ (CCONJ)\n",
      "–≤–ª–∞–¥–µ–ª—å—Ü–µ–º -> –≤–ª–∞–¥–µ–ª–µ—Ü (NOUN)\n",
      "—Å–µ–∫—Å—à–æ–ø–∞ -> —Å–µ–∫—Å—à–æ–ø (NOUN)\n",
      "—Å–∫–æ—Ä–µ–µ -> —Å–∫–æ—Ä—ã–π (ADV)\n",
      "—Å–º–æ—Ç—Ä–∏ -> —Å–º–æ—Ç—Ä–∏ (ADV)\n",
      "–∑–∞–ø–∏—Å–∏ -> –∑–∞–ø–∏—Å—å (NOUN)\n",
      "–ø–æ–∫–∞ -> –ø–æ–∫–∞ (ADV)\n",
      "–Ω–µ -> –Ω–µ (PART)\n",
      "—É–¥–∞–ª–∏–ª–∞ -> —É–¥–∞–ª–∏—Ç—å (VERB)\n",
      "httpstmesekretskris -> httpstmesekretskris (X)\n",
      "1048 -> 1048 (NUM)\n",
      "–∑–¥–µ—Å—å -> –∑–¥–µ—Å—å (ADV)\n",
      "–∂–∞—Ä—á–µ -> –∂–∞—Ä—á–µ (ADJ)\n",
      "—á–µ–º -> —á–µ–º (SCONJ)\n",
      "–≤ -> –≤ (ADP)\n",
      "–∞–¥—É -> –∞–¥ (NOUN)\n",
      "\n",
      "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor,\n",
    "    PER\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "        text = ' '.join(text.split())  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        return text\n",
    "    \n",
    "    def process(self, text):\n",
    "        \"\"\"\n",
    "        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "        \n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É:\n",
    "        {\n",
    "            \"clean_text\": str,\n",
    "            \"sentences\": [\n",
    "                {\n",
    "                    \"text\": str,\n",
    "                    \"tokens\": [\n",
    "                        {\n",
    "                            \"text\": str,\n",
    "                            \"lemma\": str,\n",
    "                            \"pos\": str,\n",
    "                            \"start\": int,\n",
    "                            \"stop\": int\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"entities\": [\n",
    "                {\n",
    "                    \"text\": str,\n",
    "                    \"normal\": str,\n",
    "                    \"type\": str,\n",
    "                    \"start\": int,\n",
    "                    \"stop\": int\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # –®–∞–≥ 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –®–∞–≥ 2: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # –®–∞–≥ 3: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)\n",
    "        \n",
    "        # –®–∞–≥ 4: –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # –®–∞–≥ 5: –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –®–∞–≥ 6: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        # –®–∞–≥ 7: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "        return self._build_output(doc, cleaned_text)\n",
    "    \n",
    "    def _build_output(self, doc, cleaned_text):\n",
    "        \"\"\"–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM\"\"\"\n",
    "        output = {\n",
    "            \"clean_text\": cleaned_text,\n",
    "            \"sentences\": [],\n",
    "            \"entities\": []\n",
    "        }\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "        for sent in doc.sents:\n",
    "            sentence_data = {\n",
    "                \"text\": sent.text,\n",
    "                \"tokens\": []\n",
    "            }\n",
    "            \n",
    "            for token in sent.tokens:\n",
    "                token_data = {\n",
    "                    \"text\": token.text,\n",
    "                    \"lemma\": token.lemma,\n",
    "                    \"pos\": token.pos,\n",
    "                    \"start\": token.start,\n",
    "                    \"stop\": token.stop\n",
    "                }\n",
    "                sentence_data[\"tokens\"].append(token_data)\n",
    "            \n",
    "            output[\"sentences\"].append(sentence_data)\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        for span in doc.spans:\n",
    "            entity_data = {\n",
    "                \"text\": span.text,\n",
    "                \"normal\": span.normal,\n",
    "                \"type\": span.type,\n",
    "                \"start\": span.start,\n",
    "                \"stop\": span.stop\n",
    "            }\n",
    "            output[\"entities\"].append(entity_data)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = TextPreprocessor()\n",
    "    # sample_text = \"–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø–æ—Å–µ—Ç–∏–ª –∑–∞–≤–æ–¥ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ.\"\n",
    "    sample_text = \"–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà\"\n",
    "    \n",
    "    result = preprocessor.process(sample_text)\n",
    "    print(\"–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\", result[\"clean_text\"])\n",
    "    print(\"\\n–¢–æ–∫–µ–Ω—ã –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:\")\n",
    "    for token in result[\"sentences\"][0][\"tokens\"]:\n",
    "        print(f\"{token['text']} -> {token['lemma']} ({token['pos']})\")\n",
    "    \n",
    "    print(\"\\n–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"{entity['text']} -> {entity['normal']} ({entity['type']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, Doc)\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class NatashaPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Natasha.\n",
    "    \n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞,\n",
    "    –≤–∫–ª—é—á–∞—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, \n",
    "    –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é.\n",
    "    \n",
    "    –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å \n",
    "    —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞.\n",
    "        –°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Natasha.\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.embedding = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.embedding)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.embedding)\n",
    "        self.ner_tagger = NewsNERTagger(self.embedding)\n",
    "\n",
    "    def preprocess(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞.\n",
    "        \n",
    "        –ü—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∫–ª—é—á–∞–µ—Ç:\n",
    "        1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)\n",
    "        2. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n",
    "        3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞)\n",
    "        4. –ú–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑ (—á–∞—Å—Ç–∏ —Ä–µ—á–∏, –≥—Ä–∞–º–º–µ–º—ã)\n",
    "        5. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞)\n",
    "        6. NER (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π)\n",
    "        7. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        \n",
    "        Args:\n",
    "            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30 —Å–ª–æ–≤.\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"–î–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ 30 —Å–ª–æ–≤\")\n",
    "        \n",
    "        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø–µ—Ä–µ–≤–æ–¥ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[\\W_]+', ' ', text)  # —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Doc –¥–ª—è Natasha\n",
    "        doc = Doc(text)\n",
    "\n",
    "        # 2. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è\n",
    "        doc.segment(self.segmenter)\n",
    "\n",
    "        # 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        doc.tokens\n",
    "\n",
    "        # 4. –ú–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "\n",
    "        # 5. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize()\n",
    "\n",
    "        # 6. NER\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "\n",
    "        # 7. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ)\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_tagger)\n",
    "            entities.append({'text': span.normal, 'type': span.type, 'start': span.start, 'stop': span.stop})\n",
    "\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ LLM (—Å–ø–∏—Å–æ–∫ –ª–µ–º–º –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –ø–æ–∑–∏—Ü–∏—è–º–∏)\n",
    "        lemmas = [token.lemma for token in doc.tokens]\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏—è—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è—Ö\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            tokens_info.append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma,\n",
    "                'start': token.start,\n",
    "                'stop': token.stop\n",
    "            })\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è (–¥–æ 2 —Å–ª–æ–≤)\n",
    "        phrases = []\n",
    "        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                'text': token['text'],\n",
    "                'lemma': token['lemma'],\n",
    "                'start': token['start'],\n",
    "                'stop': token['stop'],\n",
    "                'length': 1\n",
    "            })\n",
    "        \n",
    "        # –°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è –∏–∑ 2 —Å–ª–æ–≤\n",
    "        for i in range(len(tokens_info) - 1):\n",
    "            phrases.append({\n",
    "                'text': f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                'lemma': f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                'start': tokens_info[i]['start'],\n",
    "                'stop': tokens_info[i+1]['stop'],\n",
    "                'length': 2\n",
    "            })\n",
    "\n",
    "        result = {\n",
    "            'lemmas': lemmas,\n",
    "            'entities': entities,\n",
    "            'tokens': tokens_info,\n",
    "            'phrases': phrases\n",
    "        }\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NatashaPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DocToken.lemmatize() missing 1 required positional argument: 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 73\u001b[0m, in \u001b[0;36mNatashaPreprocessor.preprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 5. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mtokens:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 6. NER\u001b[39;00m\n\u001b[1;32m     76\u001b[0m doc\u001b[38;5;241m.\u001b[39mtag_ner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tagger)\n",
      "\u001b[0;31mTypeError\u001b[0m: DocToken.lemmatize() missing 1 required positional argument: 'vocab'"
     ]
    }
   ],
   "source": [
    "x.preprocess(\"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–µ–º–º—ã: ['–º–æ—Å–∫–≤–∞', '—Å—Ç–æ–ª–∏—Ü–∞', '—Ä–æ—Å—Å–∏—è', '–≤–ª–∞–¥–∏–º–∏—Ä', '–ø—É—Ç–∏–Ω', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–≤–µ—â–∞–Ω–∏–µ']\n",
      "\n",
      "–°—É—â–Ω–æ—Å—Ç–∏:\n",
      "\n",
      "–°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è:\n",
      "–ú–æ—Å–∫–≤–∞  —Å—Ç–æ–ª–∏—Ü -> –º–æ—Å–∫–≤–∞ —Å—Ç–æ–ª–∏—Ü–∞\n",
      " —Å—Ç–æ–ª–∏—Ü  –†–æ—Å—Å–∏ -> —Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏—è\n",
      " –†–æ—Å—Å–∏ . –í–ª–∞–¥–∏–º -> —Ä–æ—Å—Å–∏—è –≤–ª–∞–¥–∏–º–∏—Ä\n",
      ". –í–ª–∞–¥–∏–º —Ä –ü—É—Ç -> –≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω\n",
      "—Ä –ü—É—Ç –Ω –ø—Ä–æ–≤ -> –ø—É—Ç–∏–Ω –ø—Ä–æ–≤–µ—Å—Ç–∏\n",
      "–Ω –ø—Ä–æ–≤ –ª —Å–æ–≤–µ—â–∞–Ω -> –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–æ–≤–µ—â–∞–Ω–∏–µ\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor,\n",
    "    NewsSyntaxParser,\n",
    "    PER,\n",
    "    LOC,\n",
    "    ORG\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Ö–∞–∫–∞—Ç–æ–Ω–∞.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –¥–µ—Ñ–∏—Å–æ–≤/–ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum():\n",
    "                cleaned.append(char.lower())\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            elif char.isspace():\n",
    "                cleaned.append(' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏/–¥–µ—Ñ–∏—Å–æ–≤, –Ω–æ –Ω–µ –≤–∫–ª—é—á–∞–µ–º –≤ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "                original_to_clean.append(None)\n",
    "\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    \n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\"\"\"\n",
    "        orig_start = None\n",
    "        orig_stop = None\n",
    "        \n",
    "        for i, val in enumerate(mapping):\n",
    "            if val == start:\n",
    "                orig_start = i\n",
    "            if val == stop-1:\n",
    "                orig_stop = i+1\n",
    "            if orig_start is not None and orig_stop is not None:\n",
    "                break\n",
    "                \n",
    "        return (orig_start, orig_stop) if orig_start is not None and orig_stop is not None else (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏\"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≤ 30 —Å–ª–æ–≤\")\n",
    "        \n",
    "        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Natasha\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        doc.parse_syntax(NewsSyntaxParser(self.emb))\n",
    "\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        for span in doc.spans:\n",
    "            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –í–°–ï —Å—É—â–Ω–æ—Å—Ç–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ PER\n",
    "            span.normalize(self.morph_vocab)\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤\n",
    "            if span.type in [PER, LOC, ORG]:\n",
    "                if span.type == PER:\n",
    "                    span.extract_fact(self.names_extractor)\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,  # <- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –±—ã–ª–æ span.type\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            orig_start, orig_stop = self._get_original_positions(\n",
    "                token.start, token.stop, pos_mapping\n",
    "            )\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[orig_start:orig_stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": orig_start,\n",
    "                \"stop\": orig_stop\n",
    "            })\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è (1-2 —Å–ª–æ–≤–∞)\n",
    "        phrases = []\n",
    "        used_spans = set()\n",
    "        \n",
    "        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        \n",
    "        # –ü–∞—Ä—ã —Å–ª–æ–≤\n",
    "        for i in range(len(tokens_info)-1):\n",
    "            phrase = {\n",
    "                \"type\": \"phrase\",\n",
    "                \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i+1]['stop'],\n",
    "                \"length\": 2\n",
    "            }\n",
    "            phrases.append(phrase)\n",
    "        \n",
    "        # –°—É—â–Ω–æ—Å—Ç–∏ –∏–∑ NER\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "            entities.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": span.normal,\n",
    "                \"type\": span.type,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop,\n",
    "                \"length\": len(span.tokens)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"–ú–æ—Å–∫–≤–∞ - —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"–õ–µ–º–º—ã:\", result[\"lemmas\"])\n",
    "        print(\"\\n–°—É—â–Ω–æ—Å—Ç–∏:\")\n",
    "        for ent in result[\"entities\"]:\n",
    "            print(f\"{ent['text']} ({ent['type']}): {ent['lemma']}\")\n",
    "        \n",
    "        print(\"\\n–°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è:\")\n",
    "        for phrase in result[\"phrases\"]:\n",
    "            if phrase[\"length\"] == 2:\n",
    "                print(f\"{phrase['text']} -> {phrase['lemma']}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–µ–º–º—ã: ['–º–æ—Å–∫–≤–∞', '—Å—Ç–æ–ª–∏—Ü–∞', '—Ä–æ—Å—Å–∏—è', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–≤–ª–∞–¥–∏–º–∏—Ä', '–ø—É—Ç–∏–Ω', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–≤–µ—â–∞–Ω–∏–µ']\n",
      "\n",
      "–°—É—â–Ω–æ—Å—Ç–∏:\n",
      "\n",
      "–°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è:\n",
      "–ú–æ—Å–∫–≤–∞  —Å—Ç–æ–ª–∏—Ü -> –º–æ—Å–∫–≤–∞ —Å—Ç–æ–ª–∏—Ü–∞\n",
      " —Å—Ç–æ–ª–∏—Ü  –†–æ—Å—Å–∏ -> —Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏—è\n",
      " –†–æ—Å—Å–∏  –ü—Ä–µ–∑–∏–¥–µ -> —Ä–æ—Å—Å–∏—è –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç\n",
      " –ü—Ä–µ–∑–∏–¥–µ —Ç –í–ª–∞–¥–∏–º -> –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –≤–ª–∞–¥–∏–º–∏—Ä\n",
      "—Ç –í–ª–∞–¥–∏–º —Ä –ü—É—Ç -> –≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω\n",
      "—Ä –ü—É—Ç –Ω –ø—Ä–æ–≤ -> –ø—É—Ç–∏–Ω –ø—Ä–æ–≤–µ—Å—Ç–∏\n",
      "–Ω –ø—Ä–æ–≤ –ª —Å–æ–≤–µ—â–∞–Ω -> –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–æ–≤–µ—â–∞–Ω–∏–µ\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Ö–∞–∫–∞—Ç–æ–Ω–∞:\n",
    "    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (30 —Å–ª–æ–≤)\n",
    "    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (PER, LOC, ORG)\n",
    "    - –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π (1-2 —Å–ª–æ–≤–∞)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # –£–±—Ä–∞–ª –ø–∞—Ä–∞–º–µ—Ç—Ä labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"\n",
    "        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π:\n",
    "        - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        - –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ –ø—Ä–æ–±–µ–ª–æ–≤)\n",
    "        - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –º–∞–ø–ø–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π\n",
    "        \"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\n",
    "        —Å —É—á–µ—Ç–æ–º —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # –ò—â–µ–º –ø–µ—Ä–≤—É—é –Ω–µ-None –ø–æ–∑–∏—Ü–∏—é –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ\n",
    "            orig_start = next(mapping[i] for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ-None –ø–æ–∑–∏—Ü–∏—é\n",
    "            orig_stop = next(mapping[i] for i in reversed(range(stop)) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"\n",
    "        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\n",
    "        - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        - –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Natasha\n",
    "        - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        words = text.split()\n",
    "        if len(words) > 30:\n",
    "            raise ValueError(\"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –≤ 30 —Å–ª–æ–≤\")\n",
    "        \n",
    "        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Natasha\n",
    "        doc.segment(self.segmenter)       # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        doc.tag_morph(self.morph_tagger)  # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n",
    "        doc.parse_syntax(self.syntax_parser)  # –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–í–ê–ñ–ù–û –¥–ª—è NER)\n",
    "        doc.tag_ner(self.ner_tagger)      # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        \n",
    "        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == PER:\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"\n",
    "        –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö:\n",
    "        - –õ–µ–º–º—ã\n",
    "        - –¢–æ–∫–µ–Ω—ã —Å –ø–æ–∑–∏—Ü–∏—è–º–∏\n",
    "        - –°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è\n",
    "        - –°—É—â–Ω–æ—Å—Ç–∏\n",
    "        \"\"\"\n",
    "        # –°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            orig_start, orig_stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[orig_start:orig_stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": orig_start,\n",
    "                \"stop\": orig_stop\n",
    "            })\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π (1-2 —Å–ª–æ–≤–∞)\n",
    "        phrases = []\n",
    "        \n",
    "        # 1. –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        \n",
    "        # 2. –ü–∞—Ä—ã —Å–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n",
    "        current_sentence_end = 0\n",
    "        for sent in doc.sents:\n",
    "            sentence_tokens = [t for t in tokens_info if t[\"start\"] >= current_sentence_end]\n",
    "            if sentence_tokens:\n",
    "                current_sentence_end = sentence_tokens[-1][\"stop\"]\n",
    "                \n",
    "                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–∞—Ä—ã —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "                for i in range(len(sentence_tokens) - 1):\n",
    "                    token1 = sentence_tokens[i]\n",
    "                    token2 = sentence_tokens[i+1]\n",
    "                    phrases.append({\n",
    "                        \"type\": \"phrase\",\n",
    "                        \"text\": f\"{token1['text']} {token2['text']}\",\n",
    "                        \"lemma\": f\"{token1['lemma']} {token2['lemma']}\",\n",
    "                        \"start\": token1[\"start\"],\n",
    "                        \"stop\": token2[\"stop\"],\n",
    "                        \"length\": 2\n",
    "                    })\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "            \n",
    "            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Ç–∏–ø–æ–≤ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,  # —Ç–∏–ø —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    sample_text = \"–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(sample_text)\n",
    "        print(\"–õ–µ–º–º—ã:\", result[\"lemmas\"])\n",
    "        print(\"\\n–°—É—â–Ω–æ—Å—Ç–∏:\")\n",
    "        for ent in result[\"entities\"]:\n",
    "            print(f\"{ent['text']} ({ent['type']}): {ent['lemma']}\")\n",
    "        \n",
    "        print(\"\\n–°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è:\")\n",
    "        for phrase in result[\"phrases\"]:\n",
    "            if phrase[\"length\"] == 2:\n",
    "                print(f\"{phrase['text']} -> {phrase['lemma']}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemmas': ['–º–æ—Å–∫–≤–∞', '—Å—Ç–æ–ª–∏—Ü–∞', '—Ä–æ—Å—Å–∏—è', '–≤–ª–∞–¥–∏–º–∏—Ä', '–ø—É—Ç–∏–Ω', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–≤–µ—â–∞–Ω–∏–µ'], 'tokens': [{'text': '–ú–æ—Å–∫–≤–∞', 'lemma': '–º–æ—Å–∫–≤–∞', 'pos': 'VERB', 'start': 0, 'stop': 6}, {'text': ' —Å—Ç–æ–ª–∏—Ü', 'lemma': '—Å—Ç–æ–ª–∏—Ü–∞', 'pos': 'NOUN', 'start': 8, 'stop': 15}, {'text': ' –†–æ—Å—Å–∏', 'lemma': '—Ä–æ—Å—Å–∏—è', 'pos': 'NOUN', 'start': 16, 'stop': 22}, {'text': '–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º', 'lemma': '–≤–ª–∞–¥–∏–º–∏—Ä', 'pos': 'NOUN', 'start': None, 'stop': 31}, {'text': '—Ä –ü—É—Ç', 'lemma': '–ø—É—Ç–∏–Ω', 'pos': 'PROPN', 'start': 32, 'stop': 37}, {'text': '–Ω –ø—Ä–æ–≤', 'lemma': '–ø—Ä–æ–≤–µ—Å—Ç–∏', 'pos': 'VERB', 'start': 38, 'stop': 44}, {'text': '–ª —Å–æ–≤–µ—â–∞–Ω', 'lemma': '—Å–æ–≤–µ—â–∞–Ω–∏–µ', 'pos': 'NOUN', 'start': 45, 'stop': 54}], 'phrases': [{'type': 'word', 'text': '–ú–æ—Å–∫–≤–∞', 'lemma': '–º–æ—Å–∫–≤–∞', 'start': 0, 'stop': 6, 'length': 1}, {'type': 'word', 'text': ' —Å—Ç–æ–ª–∏—Ü', 'lemma': '—Å—Ç–æ–ª–∏—Ü–∞', 'start': 8, 'stop': 15, 'length': 1}, {'type': 'word', 'text': ' –†–æ—Å—Å–∏', 'lemma': '—Ä–æ—Å—Å–∏—è', 'start': 16, 'stop': 22, 'length': 1}, {'type': 'word', 'text': '–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º', 'lemma': '–≤–ª–∞–¥–∏–º–∏—Ä', 'start': None, 'stop': 31, 'length': 1}, {'type': 'word', 'text': '—Ä –ü—É—Ç', 'lemma': '–ø—É—Ç–∏–Ω', 'start': 32, 'stop': 37, 'length': 1}, {'type': 'word', 'text': '–Ω –ø—Ä–æ–≤', 'lemma': '–ø—Ä–æ–≤–µ—Å—Ç–∏', 'start': 38, 'stop': 44, 'length': 1}, {'type': 'word', 'text': '–ª —Å–æ–≤–µ—â–∞–Ω', 'lemma': '—Å–æ–≤–µ—â–∞–Ω–∏–µ', 'start': 45, 'stop': 54, 'length': 1}, {'type': 'phrase', 'text': '–ú–æ—Å–∫–≤–∞  —Å—Ç–æ–ª–∏—Ü', 'lemma': '–º–æ—Å–∫–≤–∞ —Å—Ç–æ–ª–∏—Ü–∞', 'start': 0, 'stop': 15, 'length': 2}, {'type': 'phrase', 'text': ' —Å—Ç–æ–ª–∏—Ü  –†–æ—Å—Å–∏', 'lemma': '—Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏—è', 'start': 8, 'stop': 22, 'length': 2}, {'type': 'phrase', 'text': ' –†–æ—Å—Å–∏ –ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º', 'lemma': '—Ä–æ—Å—Å–∏—è –≤–ª–∞–¥–∏–º–∏—Ä', 'start': 16, 'stop': 31, 'length': 2}, {'type': 'phrase', 'text': '–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º —Ä –ü—É—Ç', 'lemma': '–≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω', 'start': None, 'stop': 37, 'length': 2}, {'type': 'phrase', 'text': '—Ä –ü—É—Ç –Ω –ø—Ä–æ–≤', 'lemma': '–ø—É—Ç–∏–Ω –ø—Ä–æ–≤–µ—Å—Ç–∏', 'start': 32, 'stop': 44, 'length': 2}, {'type': 'phrase', 'text': '–Ω –ø—Ä–æ–≤ –ª —Å–æ–≤–µ—â–∞–Ω', 'lemma': '–ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–æ–≤–µ—â–∞–Ω–∏–µ', 'start': 38, 'stop': 54, 'length': 2}], 'entities': [], 'original_text': '–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.', 'clean_text': '–º–æ—Å–∫–≤–∞  —Å—Ç–æ–ª–∏—Ü–∞ —Ä–æ—Å—Å–∏–∏ –≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ'}\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: —É–±—Ä–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        clean_pos = 0\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "                clean_pos += 1\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        try:\n",
    "            orig_start = mapping[start]\n",
    "            orig_stop = mapping[stop-1] + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except (IndexError, StopIteration):\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30 —Å–ª–æ–≤\")\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        phrases = []\n",
    "        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        for token in tokens_info:\n",
    "            phrases.append({\n",
    "                \"type\": \"word\",\n",
    "                \"text\": token[\"text\"],\n",
    "                \"lemma\": token[\"lemma\"],\n",
    "                \"start\": token[\"start\"],\n",
    "                \"stop\": token[\"stop\"],\n",
    "                \"length\": 1\n",
    "            })\n",
    "        # –ë–∏–≥—Ä–∞–º–º—ã\n",
    "        for i in range(len(tokens_info)-1):\n",
    "            phrases.append({\n",
    "                \"type\": \"phrase\",\n",
    "                \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i+1]['stop'],\n",
    "                \"length\": 2\n",
    "            })\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        return {\n",
    "            \"lemmas\": [token[\"lemma\"] for token in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.\"\n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(result)\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—É—â–Ω–æ—Å—Ç–∏: []\n",
      "–õ–µ–º–º—ã: ['–º–æ—Å–∫–≤–∞', '—Å—Ç–æ–ª–∏—Ü–∞', '—Ä–æ—Å—Å–∏—è', '–≤–ª–∞–¥–∏–º–∏—Ä', '–ø—É—Ç–∏–Ω', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–≤–µ—â–∞–Ω–∏–µ']\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)  # –î–æ–±–∞–≤–ª–µ–Ω —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        try:\n",
    "            orig_start = next(i for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            orig_stop = next(i for i in reversed(range(stop, len(mapping))) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30 —Å–ª–æ–≤\")\n",
    "        \n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —ç—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è NER\n",
    "        doc.tag_ner(self.ner_tagger)\n",
    "        \n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π\n",
    "        phrases = []\n",
    "        for i in range(len(tokens_info)):\n",
    "            phrases.append({\n",
    "                \"text\": tokens_info[i]['text'],\n",
    "                \"lemma\": tokens_info[i]['lemma'],\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i]['stop'],\n",
    "                \"length\": 1\n",
    "            })\n",
    "            if i < len(tokens_info)-1:\n",
    "                phrases.append({\n",
    "                    \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                    \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                    \"start\": tokens_info[i]['start'],\n",
    "                    \"stop\": tokens_info[i+1]['stop'],\n",
    "                    \"length\": 2\n",
    "                })\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [t['lemma'] for t in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"–°—É—â–Ω–æ—Å—Ç–∏:\", [(ent[\"text\"], ent[\"type\"]) for ent in result[\"entities\"]])\n",
    "        print(\"–õ–µ–º–º—ã:\", result[\"lemmas\"])\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—É—â–Ω–æ—Å—Ç–∏: []\n",
      "–õ–µ–º–º—ã: ['–º–æ—Å–∫–≤–∞', '—Å—Ç–æ–ª–∏—Ü–∞', '—Ä–æ—Å—Å–∏—è', '–≤–ª–∞–¥–∏–º–∏—Ä', '–ø—É—Ç–∏–Ω', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '—Å–æ–≤–µ—â–∞–Ω–∏–µ']\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    Doc,\n",
    "    NamesExtractor\n",
    ")\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.ner_tagger = NewsNERTagger(self.emb)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: —É–±—Ä–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä labels\n",
    "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π\"\"\"\n",
    "        cleaned = []\n",
    "        original_to_clean = []\n",
    "        for orig_pos, char in enumerate(text):\n",
    "            if char.isalnum() or char.isspace():\n",
    "                cleaned.append(char.lower() if char.isalnum() else ' ')\n",
    "                original_to_clean.append(orig_pos)\n",
    "            else:\n",
    "                original_to_clean.append(None)\n",
    "        return ''.join(cleaned), original_to_clean\n",
    "\n",
    "    def _get_original_positions(self, start, stop, mapping):\n",
    "        \"\"\"–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π —Å —É—á–µ—Ç–æ–º —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\"\"\"\n",
    "        try:\n",
    "            orig_start = next(i for i in range(start, len(mapping)) if mapping[i] is not None)\n",
    "            orig_stop = next(i for i in reversed(range(stop)) if mapping[i] is not None) + 1\n",
    "            return (orig_start, orig_stop)\n",
    "        except StopIteration:\n",
    "            return (start, stop)\n",
    "\n",
    "    def process(self, text):\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        if len(text.split()) > 30:\n",
    "            raise ValueError(\"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30 —Å–ª–æ–≤\")\n",
    "        \n",
    "        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "        cleaned_text, pos_mapping = self._clean_text(text)\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        doc.segment(self.segmenter)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        doc.parse_syntax(self.syntax_parser)  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è NER\n",
    "        doc.tag_ner(self.ner_tagger)  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π —à–∞–≥\n",
    "        \n",
    "        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "        for span in doc.spans:\n",
    "            span.normalize(self.morph_vocab)\n",
    "            if span.type == 'PER':  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–æ–∫–µ\n",
    "                span.extract_fact(self.names_extractor)\n",
    "        \n",
    "        return self._build_output(doc, text, pos_mapping)\n",
    "\n",
    "    def _build_output(self, doc, orig_text, pos_mapping):\n",
    "        \"\"\"–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\"\"\"\n",
    "        # –¢–æ–∫–µ–Ω—ã\n",
    "        tokens_info = []\n",
    "        for token in doc.tokens:\n",
    "            start, stop = self._get_original_positions(token.start, token.stop, pos_mapping)\n",
    "            tokens_info.append({\n",
    "                \"text\": orig_text[start:stop],\n",
    "                \"lemma\": token.lemma,\n",
    "                \"pos\": token.pos,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            })\n",
    "        \n",
    "        # –°–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è\n",
    "        phrases = []\n",
    "        for i in range(len(tokens_info)):\n",
    "            # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "            phrases.append({\n",
    "                \"text\": tokens_info[i]['text'],\n",
    "                \"lemma\": tokens_info[i]['lemma'],\n",
    "                \"start\": tokens_info[i]['start'],\n",
    "                \"stop\": tokens_info[i]['stop'],\n",
    "                \"length\": 1\n",
    "            })\n",
    "            # –ë–∏–≥—Ä–∞–º–º—ã\n",
    "            if i < len(tokens_info)-1:\n",
    "                phrases.append({\n",
    "                    \"text\": f\"{tokens_info[i]['text']} {tokens_info[i+1]['text']}\",\n",
    "                    \"lemma\": f\"{tokens_info[i]['lemma']} {tokens_info[i+1]['lemma']}\",\n",
    "                    \"start\": tokens_info[i]['start'],\n",
    "                    \"stop\": tokens_info[i+1]['stop'],\n",
    "                    \"length\": 2\n",
    "                })\n",
    "        \n",
    "        # –°—É—â–Ω–æ—Å—Ç–∏\n",
    "        entities = []\n",
    "        for span in doc.spans:\n",
    "            if span.type in ['PER', 'LOC', 'ORG']:  # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–º —Ç–∏–ø–∞–º\n",
    "                start, stop = self._get_original_positions(span.start, span.stop, pos_mapping)\n",
    "                entities.append({\n",
    "                    \"text\": orig_text[start:stop],\n",
    "                    \"lemma\": span.normal,\n",
    "                    \"type\": span.type,\n",
    "                    \"start\": start,\n",
    "                    \"stop\": stop,\n",
    "                    \"length\": len(span.tokens)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"lemmas\": [t['lemma'] for t in tokens_info],\n",
    "            \"tokens\": tokens_info,\n",
    "            \"phrases\": phrases,\n",
    "            \"entities\": entities,\n",
    "            \"original_text\": orig_text,\n",
    "            \"clean_text\": doc.text\n",
    "        }\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextPreprocessor()\n",
    "    text = \"–ú–æ—Å–∫–≤–∞ ‚Äî —Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏. –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –ø—Ä–æ–≤–µ–ª —Å–æ–≤–µ—â–∞–Ω–∏–µ.\"\n",
    "    \n",
    "    try:\n",
    "        result = processor.process(text)\n",
    "        print(\"–°—É—â–Ω–æ—Å—Ç–∏:\", [(ent[\"text\"], ent[\"type\"]) for ent in result[\"entities\"]])\n",
    "        print(\"–õ–µ–º–º—ã:\", result[\"lemmas\"])\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "19823.33s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"info\"\n"
     ]
    }
   ],
   "source": [
    "!pip info natasha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
