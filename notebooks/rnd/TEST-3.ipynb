{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libamdhip64.so: cannot enable executable stack as shared object requires: Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROCm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mroc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rocm/lib/python3.10/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libamdhip64.so: cannot enable executable stack as shared object requires: Invalid argument"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"ROCm: {torch.version.roc}\")\n",
    "print(f\"GPU доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"Название GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Количество видеокарт: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для очистки и предобработки текста.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\n",
    "        \"url_pattern\",\n",
    "        \"emoji_pattern\",\n",
    "        \"html_pattern\",\n",
    "        \"non_letter_pattern\",\n",
    "        \"telegram_pattern\",\n",
    "        \"spaces_pattern\",\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация класса TextPreprocessor.\n",
    "        Предкомпилирует регулярные выражения для очистки текста.\n",
    "        \"\"\"\n",
    "        self.url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001f600-\\U0001f64f\"  # эмоции\n",
    "            \"\\U0001f300-\\U0001f5ff\"  # символы\n",
    "            \"\\U0001f680-\\U0001f6ff\"  # транспорт\n",
    "            \"\\U0001f700-\\U0001f77f\"  # алхимия\n",
    "            \"\\U0001f780-\\U0001f7ff\"  # геометрические фигуры\n",
    "            \"\\U0001f800-\\U0001f8ff\"  # дополнительные символы\n",
    "            \"\\U0001f900-\\U0001f9ff\"  # дополнительные символы-2\n",
    "            \"\\U0001fa00-\\U0001fa6f\"  # шахматы\n",
    "            \"\\U0001fa70-\\U0001faff\"  # дополнительные символы-3\n",
    "            \"\\U00002702-\\U000027b0\"  # Dingbats\n",
    "            \"\\U000024c2-\\U0001f251\"  # Enclosed\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.html_pattern = re.compile(r\"&[a-z]+;\")\n",
    "        self.non_letter_pattern = re.compile(r\"[^a-zа-я\\s]\")\n",
    "        self.telegram_pattern = re.compile(r\"@\\w+|/\\w+\")\n",
    "        self.spaces_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "    def __preprocess(\n",
    "        self,\n",
    "        text,\n",
    "        lowercase=True,\n",
    "        replace_yo=True,\n",
    "        remove_urls=True,\n",
    "        remove_emoji=True,\n",
    "        remove_html=True,\n",
    "        remove_punctuation=True,\n",
    "        remove_telegram=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Приватный метод базовой предобработки текста.\n",
    "\n",
    "        :param text: Исходный текст для предобработки.\n",
    "        :param lowercase: Флаг для приведения текста к нижнему регистру.\n",
    "        :param replace_yo: Флаг для замены буквы \"ё\" на \"е\".\n",
    "        :param remove_urls: Флаг для удаления URL-ссылок.\n",
    "        :param remove_emoji: Флаг для удаления эмодзи.\n",
    "        :param remove_html: Флаг для удаления HTML-сущностей.\n",
    "        :param remove_punctuation: Флаг для удаления пунктуации и не-буквенных символов.\n",
    "        :param remove_telegram: Флаг для удаления телеграм-упоминаний и бот-команд.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        result = text\n",
    "\n",
    "        if lowercase:\n",
    "            result = result.lower()\n",
    "\n",
    "        if replace_yo:\n",
    "            result = result.replace(\"ё\", \"е\")\n",
    "\n",
    "        if remove_urls:\n",
    "            result = self.url_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_emoji:\n",
    "            result = self.emoji_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_html:\n",
    "            result = self.html_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_punctuation:\n",
    "            result = self.non_letter_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_telegram:\n",
    "            result = self.telegram_pattern.sub(\" \", result)\n",
    "\n",
    "        return self.spaces_pattern.sub(\" \", result).strip()\n",
    "\n",
    "    def clear_text(self, text):\n",
    "        \"\"\"\n",
    "        Полная очистка текста для семантического поиска.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text)\n",
    "\n",
    "    def clean_for_search(self, text):\n",
    "        \"\"\"\n",
    "        Очистка текста для поисковых запросов.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text, remove_punctuation=False, remove_telegram=False)\n",
    "\n",
    "    def clean_for_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Очистка текста перед получением эмбеддингов.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text, remove_emoji=True, remove_html=True, remove_punctuation=True\n",
    "        )\n",
    "\n",
    "    def clean_for_display(self, text):\n",
    "        \"\"\"\n",
    "        Лёгкая очистка для отображения пользователю.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text,\n",
    "            lowercase=False,\n",
    "            replace_yo=False,\n",
    "            remove_urls=False,\n",
    "            remove_emoji=False,\n",
    "            remove_punctuation=False,\n",
    "        )\n",
    "\n",
    "    def get_text_stats(self, text):\n",
    "        \"\"\"\n",
    "        Возвращает статистику по тексту: длина текста, количество слов,\n",
    "        уникальные слова и другие полезные метрики.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст для анализа\n",
    "\n",
    "        Returns:\n",
    "            dict: Словарь с различными статистическими метриками текста:\n",
    "                - length: общая длина текста в символах\n",
    "                - word_count: количество слов в тексте\n",
    "                - unique_word_count: количество уникальных слов\n",
    "                - word_frequencies: словарь частоты встречаемости слов\n",
    "                - avg_word_length: средняя длина слова\n",
    "                - short_words_count: количество слов длиной менее 4 символов\n",
    "                - long_words_count: количество слов длиной более 7 символов\n",
    "        \"\"\"\n",
    "        # Проверяем входные данные\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                \"length\": 0,\n",
    "                \"word_count\": 0,\n",
    "                \"unique_word_count\": 0,\n",
    "                \"word_frequencies\": {},\n",
    "            }\n",
    "\n",
    "        # Очищаем текст \n",
    "        cleaned_text = self.clear_text(text)\n",
    "\n",
    "        # Разбиваем текст на слова\n",
    "        words = cleaned_text.split()\n",
    "\n",
    "        # Рассчитываем основные метрики\n",
    "        word_count = len(words)\n",
    "        unique_words = set(words)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # Рассчитываем частоту слов\n",
    "        word_frequencies = {}\n",
    "        for word in words:\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "        # Рассчитываем дополнительные метрики\n",
    "        total_chars = sum(len(word) for word in words)\n",
    "        avg_word_length = total_chars / word_count if word_count > 0 else 0\n",
    "        short_words_count = sum(1 for word in words if len(word) < 4)\n",
    "        long_words_count = sum(1 for word in words if len(word) > 7)\n",
    "\n",
    "        # Собираем все метрики в словарь\n",
    "        stats = {\n",
    "            \"length\": len(cleaned_text),  # Длина текста в символах\n",
    "            \"word_count\": word_count,  # Количество слов\n",
    "            \"unique_word_count\": unique_word_count,  # Количество уникальных слов\n",
    "            \"word_frequencies\": word_frequencies,  # Частота слов\n",
    "            \"avg_word_length\": round(avg_word_length, 2),  # Средняя длина слова\n",
    "            \"short_words_count\": short_words_count,  # Количество коротких слов (< 4 букв)\n",
    "            \"long_words_count\": long_words_count,  # Количество длинных слов (> 7 букв)\n",
    "        }\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Сегодня (26.07) премьера в 18.00 (мск) - долгожданный выпуск с [id1868874|Евгений Егоров]   https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "text_rreprocessor = TextPreprocessor()\n",
    "print(text_rreprocessor.clear_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_rreprocessor.get_text_stats(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "class NatashaLinguisticAnalyzer:\n",
    "    \"\"\"\n",
    "    Класс для лингвистического анализа русского текста с использованием Natasha.\n",
    "    \n",
    "    Выполняет токенизацию, лемматизацию, определение частей речи и\n",
    "    сохраняет позиции символов каждого токена в оригинальном тексте.\n",
    "    \"\"\"\n",
    "    \n",
    "    __slots__ = ('segmenter', 'morph_vocab', 'emb', 'morph_tagger', 'text_preprocessor')\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализирует компоненты Natasha для лингвистического анализа.\n",
    "        \n",
    "        Используемые компоненты:\n",
    "        - Segmenter: Для сегментации текста на токены\n",
    "        - MorphVocab: Для морфологического словаря и лемматизации\n",
    "        - NewsEmbedding: Для векторных представлений\n",
    "        - NewsMorphTagger: Для морфологической разметки (определения частей речи)\n",
    "        - TextPreprocessor: Для предобработки текста\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "\n",
    "        # Инициализация экземпляра TextPreprocessor (композиция)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "    \n",
    "    def clear_text_with_mapping(self, text: str) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Очищает текст и возвращает маппинг индексов из очищенного текста в исходный.\n",
    "        \"\"\"\n",
    "        # Получаем очищенный текст с помощью TextPreprocessor\n",
    "        cleaned_text = self.text_preprocessor.clear_text(text)\n",
    "        \n",
    "        # Создаем маппинг индексов из очищенного текста в исходный\n",
    "        mapping = []\n",
    "        orig_index = 0\n",
    "        \n",
    "        for clean_char in cleaned_text:\n",
    "            found = False\n",
    "            # Пропускаем символы в исходном тексте до нахождения соответствия\n",
    "            while orig_index < len(text) and not found:\n",
    "                if text[orig_index].lower() == clean_char:\n",
    "                    mapping.append(orig_index)\n",
    "                    orig_index += 1\n",
    "                    found = True\n",
    "                else:\n",
    "                    orig_index += 1\n",
    "            \n",
    "            # Если не найдено соответствие, используем последний известный индекс\n",
    "            if not found:\n",
    "                # Добавляем последний возможный индекс или -1, если текст пустой\n",
    "                mapping.append(len(text) - 1 if text else -1)\n",
    "        \n",
    "        return cleaned_text, mapping\n",
    "\n",
    "    \n",
    "    def map_clean_to_original(self, clean_start: int, clean_end: int, mapping: List[int]) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Преобразует позицию в очищенном тексте в позицию в исходном тексте.\n",
    "        \"\"\"\n",
    "        if not mapping:\n",
    "            return -1, -1\n",
    "        \n",
    "        # Проверка границ\n",
    "        if clean_start >= len(mapping):\n",
    "            return -1, -1\n",
    "        \n",
    "        orig_start = mapping[clean_start]\n",
    "        \n",
    "        # Обработка случая, когда clean_end выходит за границы\n",
    "        if clean_end - 1 >= len(mapping):\n",
    "            # Используем последний элемент маппинга + некоторое смещение, \n",
    "            # чтобы захватить позиции после последнего маппированного символа\n",
    "            orig_end = mapping[-1] + 2  # +2 для захвата большего контекста\n",
    "        else:\n",
    "            orig_end = mapping[clean_end - 1] + 1\n",
    "        \n",
    "        return orig_start, orig_end\n",
    "\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Выполняет лингвистический анализ текста с использованием Natasha.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Входной текст для анализа.\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Словарь с результатами анализа:\n",
    "                - 'tokens': Список кортежей с информацией о токенах\n",
    "                  (token, lemma, pos_tag, start_clean, end_clean, start_orig, end_orig)\n",
    "                - 'original_text': Оригинальный текст (до очистки)\n",
    "                - 'cleaned_text': Очищенный текст (после предобработки)\n",
    "                - 'mapping': Маппинг индексов из очищенного текста в исходный\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {'tokens': [], 'original_text': '', 'cleaned_text': '', 'mapping': []}\n",
    "        \n",
    "        # Сохраняем оригинальный текст\n",
    "        original_text = text\n",
    "        \n",
    "        # Базовая предобработка с TextPreprocessor и получение маппинга\n",
    "        cleaned_text, mapping = self.clear_text_with_mapping(text)\n",
    "\n",
    "        # Создаем объект Doc из Natasha\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Сегментация текста на токены\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # Морфологический анализ (определение частей речи)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # Лемматизация токенов\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Создаем результирующий список с позициями как в очищенном, так и в исходном тексте\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            # Позиции в очищенном тексте\n",
    "            start_clean = token.start\n",
    "            end_clean = token.stop\n",
    "            \n",
    "            # Маппинг в позиции в исходном тексте\n",
    "            start_orig, end_orig = self.map_clean_to_original(start_clean, end_clean, mapping)\n",
    "            \n",
    "            tokens.append((\n",
    "                token.text,          # Оригинальный токен\n",
    "                token.lemma,         # Лемматизированная форма\n",
    "                token.pos,           # Часть речи\n",
    "                start_clean,         # Начальный индекс в очищенном тексте\n",
    "                end_clean,           # Конечный индекс в очищенном тексте\n",
    "                start_orig,          # Начальный индекс в исходном тексте\n",
    "                end_orig             # Конечный индекс в исходном тексте\n",
    "            ))\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'mapping': mapping\n",
    "        }\n",
    "    \n",
    "    def analyze_query(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Анализирует поисковый запрос и возвращает информацию о токенах и текстах.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Исходный текст поискового запроса\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Словарь с результатами анализа (аналогично методу analyze)\n",
    "        \"\"\"\n",
    "        return self.analyze(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Создаем анализатор\n",
    "analyzer = NatashaLinguisticAnalyzer()\n",
    "\n",
    "# Пример текста\n",
    "txt = 'Сегодня (26.07) премьера в 18.00 (мск) - долгожданный выпуск с [id1868874|Евгений Егоров] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "\n",
    "\n",
    "\n",
    "# Выполняем анализ\n",
    "result = analyzer.analyze(txt)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Результат анализа:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = 'Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполняем анализ\n",
    "result = analyzer.analyze(xx)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Результат анализа:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Union\n",
    "\n",
    "class SbertLargeNLU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sberbank-ai/sbert_large_nlu_ru',\n",
    "        model_dir: str = None,\n",
    "        max_length: int = 512,\n",
    "        enable_rocm: bool = True\n",
    "    ):\n",
    "        self.device = self._get_device(enable_rocm)\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.root_dir = self._get_root_dir()\n",
    "        self.model_dir = self._init_model_dir(model_dir)\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "\n",
    "    def _get_device(self, enable_rocm: bool) -> str:\n",
    "        if enable_rocm and torch.cuda.is_available():\n",
    "            os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "            return 'cuda'\n",
    "        return 'cpu'\n",
    "\n",
    "    def _get_root_dir(self) -> Path:\n",
    "        try:\n",
    "            current_file = Path(__file__).resolve()\n",
    "            return current_file.parent.parent if \"src\" in current_file.parts else current_file.parent\n",
    "        except NameError:\n",
    "            return Path(os.getcwd()).resolve()\n",
    "\n",
    "    def _init_model_dir(self, model_dir: str) -> Path:\n",
    "        path = Path(model_dir) if model_dir else self.root_dir / \"models/sbert\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "    def _load_model(self):\n",
    "        local_path = self.model_dir / self.model_name.replace(\"/\", \"__\")\n",
    "        try:\n",
    "            if (local_path / \"config.json\").exists():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "                model = AutoModel.from_pretrained(local_path)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                model = AutoModel.from_pretrained(self.model_name)\n",
    "                model.save_pretrained(local_path, safe_serialization=True)\n",
    "                tokenizer.save_pretrained(local_path)\n",
    "            \n",
    "            return tokenizer, model.to(self.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    def create_embeddings(self, texts: Union[str, List[str]]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "            \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        if attention_mask.sum() == 0:\n",
    "            return torch.zeros((1, model_output.last_hidden_state.size(-1)))\n",
    "            \n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = (\n",
    "            attention_mask\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SbertLargeNLU()\n",
    "print(model.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
