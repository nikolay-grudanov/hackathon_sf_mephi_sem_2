{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+rocm6.3\n",
      "GPU доступна: True\n",
      "Название GPU: AMD Radeon RX 7800 XT\n",
      "Количество видеокарт: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"Название GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Количество видеокарт: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для очистки и предобработки текста.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\n",
    "        \"url_pattern\",\n",
    "        \"emoji_pattern\",\n",
    "        \"html_pattern\",\n",
    "        \"non_letter_pattern\",\n",
    "        \"telegram_pattern\",\n",
    "        \"spaces_pattern\",\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация класса TextPreprocessor.\n",
    "        Предкомпилирует регулярные выражения для очистки текста.\n",
    "        \"\"\"\n",
    "        self.url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001f600-\\U0001f64f\"  # эмоции\n",
    "            \"\\U0001f300-\\U0001f5ff\"  # символы\n",
    "            \"\\U0001f680-\\U0001f6ff\"  # транспорт\n",
    "            \"\\U0001f700-\\U0001f77f\"  # алхимия\n",
    "            \"\\U0001f780-\\U0001f7ff\"  # геометрические фигуры\n",
    "            \"\\U0001f800-\\U0001f8ff\"  # дополнительные символы\n",
    "            \"\\U0001f900-\\U0001f9ff\"  # дополнительные символы-2\n",
    "            \"\\U0001fa00-\\U0001fa6f\"  # шахматы\n",
    "            \"\\U0001fa70-\\U0001faff\"  # дополнительные символы-3\n",
    "            \"\\U00002702-\\U000027b0\"  # Dingbats\n",
    "            \"\\U000024c2-\\U0001f251\"  # Enclosed\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.html_pattern = re.compile(r\"&[a-z]+;\")\n",
    "        self.non_letter_pattern = re.compile(r\"[^a-zа-я\\s]\")\n",
    "        self.telegram_pattern = re.compile(r\"@\\w+|/\\w+\")\n",
    "        self.spaces_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "    def __preprocess(\n",
    "        self,\n",
    "        text,\n",
    "        lowercase=True,\n",
    "        replace_yo=True,\n",
    "        remove_urls=True,\n",
    "        remove_emoji=True,\n",
    "        remove_html=True,\n",
    "        remove_punctuation=True,\n",
    "        remove_telegram=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Приватный метод базовой предобработки текста.\n",
    "\n",
    "        :param text: Исходный текст для предобработки.\n",
    "        :param lowercase: Флаг для приведения текста к нижнему регистру.\n",
    "        :param replace_yo: Флаг для замены буквы \"ё\" на \"е\".\n",
    "        :param remove_urls: Флаг для удаления URL-ссылок.\n",
    "        :param remove_emoji: Флаг для удаления эмодзи.\n",
    "        :param remove_html: Флаг для удаления HTML-сущностей.\n",
    "        :param remove_punctuation: Флаг для удаления пунктуации и не-буквенных символов.\n",
    "        :param remove_telegram: Флаг для удаления телеграм-упоминаний и бот-команд.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        result = text\n",
    "\n",
    "        if lowercase:\n",
    "            result = result.lower()\n",
    "\n",
    "        if replace_yo:\n",
    "            result = result.replace(\"ё\", \"е\")\n",
    "\n",
    "        if remove_urls:\n",
    "            result = self.url_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_emoji:\n",
    "            result = self.emoji_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_html:\n",
    "            result = self.html_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_punctuation:\n",
    "            result = self.non_letter_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_telegram:\n",
    "            result = self.telegram_pattern.sub(\" \", result)\n",
    "\n",
    "        return self.spaces_pattern.sub(\" \", result).strip()\n",
    "\n",
    "    def clear_text(self, text):\n",
    "        \"\"\"\n",
    "        Полная очистка текста для семантического поиска.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text)\n",
    "\n",
    "    def clean_for_search(self, text):\n",
    "        \"\"\"\n",
    "        Очистка текста для поисковых запросов.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text, remove_punctuation=False, remove_telegram=False)\n",
    "\n",
    "    def clean_for_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Очистка текста перед получением эмбеддингов.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text, remove_emoji=True, remove_html=True, remove_punctuation=True\n",
    "        )\n",
    "\n",
    "    def clean_for_display(self, text):\n",
    "        \"\"\"\n",
    "        Лёгкая очистка для отображения пользователю.\n",
    "\n",
    "        :param text: Исходный текст для очистки.\n",
    "        :return: Очищенный текст.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text,\n",
    "            lowercase=False,\n",
    "            replace_yo=False,\n",
    "            remove_urls=False,\n",
    "            remove_emoji=False,\n",
    "            remove_punctuation=False,\n",
    "        )\n",
    "\n",
    "    def get_text_stats(self, text):\n",
    "        \"\"\"\n",
    "        Возвращает статистику по тексту: длина текста, количество слов,\n",
    "        уникальные слова и другие полезные метрики.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст для анализа\n",
    "\n",
    "        Returns:\n",
    "            dict: Словарь с различными статистическими метриками текста:\n",
    "                - length: общая длина текста в символах\n",
    "                - word_count: количество слов в тексте\n",
    "                - unique_word_count: количество уникальных слов\n",
    "                - word_frequencies: словарь частоты встречаемости слов\n",
    "                - avg_word_length: средняя длина слова\n",
    "                - short_words_count: количество слов длиной менее 4 символов\n",
    "                - long_words_count: количество слов длиной более 7 символов\n",
    "        \"\"\"\n",
    "        # Проверяем входные данные\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                \"length\": 0,\n",
    "                \"word_count\": 0,\n",
    "                \"unique_word_count\": 0,\n",
    "                \"word_frequencies\": {},\n",
    "            }\n",
    "\n",
    "        # Очищаем текст \n",
    "        cleaned_text = self.clear_text(text)\n",
    "\n",
    "        # Разбиваем текст на слова\n",
    "        words = cleaned_text.split()\n",
    "\n",
    "        # Рассчитываем основные метрики\n",
    "        word_count = len(words)\n",
    "        unique_words = set(words)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # Рассчитываем частоту слов\n",
    "        word_frequencies = {}\n",
    "        for word in words:\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "        # Рассчитываем дополнительные метрики\n",
    "        total_chars = sum(len(word) for word in words)\n",
    "        avg_word_length = total_chars / word_count if word_count > 0 else 0\n",
    "        short_words_count = sum(1 for word in words if len(word) < 4)\n",
    "        long_words_count = sum(1 for word in words if len(word) > 7)\n",
    "\n",
    "        # Собираем все метрики в словарь\n",
    "        stats = {\n",
    "            \"length\": len(cleaned_text),  # Длина текста в символах\n",
    "            \"word_count\": word_count,  # Количество слов\n",
    "            \"unique_word_count\": unique_word_count,  # Количество уникальных слов\n",
    "            \"word_frequencies\": word_frequencies,  # Частота слов\n",
    "            \"avg_word_length\": round(avg_word_length, 2),  # Средняя длина слова\n",
    "            \"short_words_count\": short_words_count,  # Количество коротких слов (< 4 букв)\n",
    "            \"long_words_count\": long_words_count,  # Количество длинных слов (> 7 букв)\n",
    "        }\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сегодня премьера в мск долгожданный выпуск с id евгений егоров\n"
     ]
    }
   ],
   "source": [
    "txt = 'Сегодня (26.07) премьера в 18.00 (мск) - долгожданный выпуск с [id1868874|Евгений Егоров]   https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "text_rreprocessor = TextPreprocessor()\n",
    "print(text_rreprocessor.clear_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 62, 'word_count': 10, 'unique_word_count': 10, 'word_frequencies': {'сегодня': 1, 'премьера': 1, 'в': 1, 'мск': 1, 'долгожданный': 1, 'выпуск': 1, 'с': 1, 'id': 1, 'евгений': 1, 'егоров': 1}, 'avg_word_length': 5.3, 'short_words_count': 4, 'long_words_count': 2}\n"
     ]
    }
   ],
   "source": [
    "print(text_rreprocessor.get_text_stats(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "class NatashaLinguisticAnalyzer:\n",
    "    \"\"\"\n",
    "    Класс для лингвистического анализа русского текста с использованием Natasha.\n",
    "    \n",
    "    Выполняет токенизацию, лемматизацию, определение частей речи и\n",
    "    сохраняет позиции символов каждого токена в оригинальном тексте.\n",
    "    \"\"\"\n",
    "    \n",
    "    __slots__ = ('segmenter', 'morph_vocab', 'emb', 'morph_tagger', 'text_preprocessor')\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализирует компоненты Natasha для лингвистического анализа.\n",
    "        \n",
    "        Используемые компоненты:\n",
    "        - Segmenter: Для сегментации текста на токены\n",
    "        - MorphVocab: Для морфологического словаря и лемматизации\n",
    "        - NewsEmbedding: Для векторных представлений\n",
    "        - NewsMorphTagger: Для морфологической разметки (определения частей речи)\n",
    "        - TextPreprocessor: Для предобработки текста\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "\n",
    "        # Инициализация экземпляра TextPreprocessor (композиция)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "    \n",
    "    def clear_text_with_mapping(self, text: str) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Очищает текст и возвращает маппинг индексов из очищенного текста в исходный.\n",
    "        \"\"\"\n",
    "        # Получаем очищенный текст с помощью TextPreprocessor\n",
    "        cleaned_text = self.text_preprocessor.clear_text(text)\n",
    "        \n",
    "        # Создаем маппинг индексов из очищенного текста в исходный\n",
    "        mapping = []\n",
    "        orig_index = 0\n",
    "        \n",
    "        for clean_char in cleaned_text:\n",
    "            found = False\n",
    "            # Пропускаем символы в исходном тексте до нахождения соответствия\n",
    "            while orig_index < len(text) and not found:\n",
    "                if text[orig_index].lower() == clean_char:\n",
    "                    mapping.append(orig_index)\n",
    "                    orig_index += 1\n",
    "                    found = True\n",
    "                else:\n",
    "                    orig_index += 1\n",
    "            \n",
    "            # Если не найдено соответствие, используем последний известный индекс\n",
    "            if not found:\n",
    "                # Добавляем последний возможный индекс или -1, если текст пустой\n",
    "                mapping.append(len(text) - 1 if text else -1)\n",
    "        \n",
    "        return cleaned_text, mapping\n",
    "\n",
    "    \n",
    "    def map_clean_to_original(self, clean_start: int, clean_end: int, mapping: List[int]) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Преобразует позицию в очищенном тексте в позицию в исходном тексте.\n",
    "        \"\"\"\n",
    "        if not mapping:\n",
    "            return -1, -1\n",
    "        \n",
    "        # Проверка границ\n",
    "        if clean_start >= len(mapping):\n",
    "            return -1, -1\n",
    "        \n",
    "        orig_start = mapping[clean_start]\n",
    "        \n",
    "        # Обработка случая, когда clean_end выходит за границы\n",
    "        if clean_end - 1 >= len(mapping):\n",
    "            # Используем последний элемент маппинга + некоторое смещение, \n",
    "            # чтобы захватить позиции после последнего маппированного символа\n",
    "            orig_end = mapping[-1] + 2  # +2 для захвата большего контекста\n",
    "        else:\n",
    "            orig_end = mapping[clean_end - 1] + 1\n",
    "        \n",
    "        return orig_start, orig_end\n",
    "\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Выполняет лингвистический анализ текста с использованием Natasha.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Входной текст для анализа.\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Словарь с результатами анализа:\n",
    "                - 'tokens': Список кортежей с информацией о токенах\n",
    "                  (token, lemma, pos_tag, start_clean, end_clean, start_orig, end_orig)\n",
    "                - 'original_text': Оригинальный текст (до очистки)\n",
    "                - 'cleaned_text': Очищенный текст (после предобработки)\n",
    "                - 'mapping': Маппинг индексов из очищенного текста в исходный\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {'tokens': [], 'original_text': '', 'cleaned_text': '', 'mapping': []}\n",
    "        \n",
    "        # Сохраняем оригинальный текст\n",
    "        original_text = text\n",
    "        \n",
    "        # Базовая предобработка с TextPreprocessor и получение маппинга\n",
    "        cleaned_text, mapping = self.clear_text_with_mapping(text)\n",
    "\n",
    "        # Создаем объект Doc из Natasha\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # Сегментация текста на токены\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # Морфологический анализ (определение частей речи)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # Лемматизация токенов\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # Создаем результирующий список с позициями как в очищенном, так и в исходном тексте\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            # Позиции в очищенном тексте\n",
    "            start_clean = token.start\n",
    "            end_clean = token.stop\n",
    "            \n",
    "            # Маппинг в позиции в исходном тексте\n",
    "            start_orig, end_orig = self.map_clean_to_original(start_clean, end_clean, mapping)\n",
    "            \n",
    "            tokens.append((\n",
    "                token.text,          # Оригинальный токен\n",
    "                token.lemma,         # Лемматизированная форма\n",
    "                token.pos,           # Часть речи\n",
    "                start_clean,         # Начальный индекс в очищенном тексте\n",
    "                end_clean,           # Конечный индекс в очищенном тексте\n",
    "                start_orig,          # Начальный индекс в исходном тексте\n",
    "                end_orig             # Конечный индекс в исходном тексте\n",
    "            ))\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'mapping': mapping\n",
    "        }\n",
    "    \n",
    "    def analyze_query(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Анализирует поисковый запрос и возвращает информацию о токенах и текстах.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Исходный текст поискового запроса\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Словарь с результатами анализа (аналогично методу analyze)\n",
    "        \"\"\"\n",
    "        return self.analyze(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат анализа:\n",
      "{'tokens': [('сегодня', 'сегодня', 'ADV', 0, 7, 0, 7), ('премьера', 'премьера', 'NOUN', 8, 16, 16, 24), ('в', 'в', 'ADP', 17, 18, 25, 26), ('мск', 'мск', 'NOUN', 19, 22, 34, 37), ('долгожданный', 'долгожданный', 'ADJ', 23, 35, 41, 53), ('выпуск', 'выпуск', 'NOUN', 36, 42, 54, 60), ('с', 'с', 'ADP', 43, 44, 61, 62), ('id', 'id', 'X', 45, 47, 64, 66), ('евгений', 'евгений', 'ADJ', 48, 55, 82, 138), ('егоров', 'егоров', 'NOUN', 56, 62, 137, 138)], 'original_text': 'Сегодня (26.07) премьера в 18.00 (мск) - долгожданный выпуск с [id1868874|Евгений Егоров] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp', 'cleaned_text': 'сегодня премьера в мск долгожданный выпуск с id евгений егоров', 'mapping': [0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 34, 35, 36, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 81, 82, 87, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Создаем анализатор\n",
    "analyzer = NatashaLinguisticAnalyzer()\n",
    "\n",
    "# Пример текста\n",
    "txt = 'Сегодня (26.07) премьера в 18.00 (мск) - долгожданный выпуск с [id1868874|Евгений Егоров] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "\n",
    "\n",
    "\n",
    "# Выполняем анализ\n",
    "result = analyzer.analyze(txt)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Результат анализа:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = 'Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат анализа:\n",
      "{'tokens': [('твой', 'твой', 'DET', 0, 4, 0, 4), ('лучший', 'хороший', 'ADJ', 5, 11, 5, 11), ('секс', 'секс', 'NOUN', 12, 16, 12, 16), ('спрятан', 'спрятать', 'VERB', 17, 24, 17, 24), ('здесь', 'здесь', 'ADV', 25, 30, 25, 30), ('делюсь', 'делиться', 'VERB', 31, 37, 34, 40), ('каналом', 'канал', 'NOUN', 38, 45, 41, 48), ('дипломированного', 'дипломированный', 'ADJ', 46, 62, 49, 65), ('сексолога', 'сексолог', 'ADJ', 63, 72, 66, 75), ('крис', 'крис', 'NOUN', 73, 77, 77, 81), ('взломала', 'взломать', 'VERB', 78, 86, 82, 90), ('код', 'код', 'NOUN', 87, 90, 91, 94), ('классного', 'классный', 'ADJ', 91, 100, 95, 104), ('секса', 'секс', 'NOUN', 101, 106, 105, 110), ('мастерски', 'мастерски', 'ADV', 107, 116, 112, 121), ('раскрепощает', 'раскрепощать', 'NOUN', 117, 129, 122, 134), ('знает', 'знать', 'VERB', 130, 135, 136, 141), ('миллион', 'миллион', 'NOUN', 136, 143, 142, 149), ('горячих', 'горячий', 'ADJ', 144, 151, 150, 157), ('техник', 'техника', 'NOUN', 152, 158, 158, 164), ('и', 'и', 'CCONJ', 159, 160, 165, 166), ('лучшие', 'хороший', 'ADJ', 161, 167, 167, 173), ('девайсы', 'девайс', 'NOUN', 168, 175, 174, 181), ('для', 'для', 'ADP', 176, 179, 182, 185), ('взрослых', 'взрослый', 'NOUN', 180, 188, 186, 194), ('самые', 'самый', 'ADJ', 189, 194, 198, 203), ('полезные', 'полезный', 'ADJ', 195, 203, 204, 212), ('посты', 'пост', 'NOUN', 204, 209, 213, 218), ('здесь', 'здесь', 'ADV', 210, 215, 219, 224), ('отрезвляющий', 'отрезвлять', 'ADJ', 216, 228, 228, 240), ('пост', 'пост', 'NOUN', 229, 233, 241, 245), ('я', 'я', 'PRON', 234, 235, 247, 248), ('все', 'весь', 'PRON', 236, 239, 249, 252), ('сама', 'сам', 'ADJ', 240, 244, 253, 257), ('прокачай', 'прокачать', 'ADJ', 245, 253, 262, 270), ('наездницу', 'наездница', 'ADJ', 254, 263, 271, 280), ('ролевая', 'ролевый', 'ADJ', 264, 271, 282, 289), ('игра', 'игра', 'NOUN', 272, 276, 290, 294), ('vip', 'vip', 'X', 277, 280, 296, 299), ('кинотеатр', 'кинотеатр', 'NOUN', 281, 290, 300, 309), ('техника', 'техника', 'NOUN', 291, 298, 313, 320), ('оральных', 'оральный', 'ADJ', 299, 307, 321, 329), ('ласк', 'ласка', 'NOUN', 308, 312, 330, 334), ('как', 'как', 'SCONJ', 313, 316, 339, 342), ('занимается', 'заниматься', 'VERB', 317, 327, 343, 353), ('с', 'с', 'ADP', 328, 329, 354, 355), ('e', 'e', 'X', 329, 330, 355, 356), ('ксом', 'ксом', 'ADJ', 330, 334, 356, 360), ('неудобная', 'неудобный', 'ADJ', 335, 344, 361, 370), ('женщина', 'женщина', 'NOUN', 345, 352, 371, 378), ('кстати', 'кстати', 'ADV', 353, 359, 381, 387), ('крис', 'крис', 'NOUN', 360, 364, 389, 393), ('провела', 'провести', 'VERB', 365, 372, 394, 401), ('трехдневный', 'трехдневный', 'ADJ', 373, 384, 402, 413), ('безоплатный', 'безоплатный', 'ADJ', 385, 396, 414, 425), ('онлайн', 'онлайн', 'ADV', 397, 403, 426, 432), ('интенсив', 'интенсив', 'NOUN', 404, 412, 433, 441), ('от', 'от', 'ADP', 413, 415, 454, 472), ('бревна', 'бревно', 'NOUN', 416, 422, 606, 607), ('до', 'до', 'ADP', 423, 425, 606, 607), ('богини', 'богиня', 'NOUN', 426, 432, 606, 607), ('совместно', 'совместно', 'ADV', 433, 442, 606, 607), ('с', 'с', 'ADP', 443, 444, 606, 607), ('врачом', 'врач', 'NOUN', 445, 451, 606, 607), ('и', 'и', 'CCONJ', 452, 453, 606, 607), ('владельцем', 'владелец', 'NOUN', 454, 464, 606, 607), ('секс', 'секс', 'NOUN', 465, 469, 606, 607), ('шопа', 'шоп', 'NOUN', 470, 474, 606, 607), ('скорее', 'скорый', 'ADV', 475, 481, 606, 607), ('смотри', 'смотри', 'ADV', 482, 488, 606, 607), ('записи', 'запись', 'NOUN', 489, 495, 606, 607), ('пока', 'пока', 'ADV', 496, 500, 606, 607), ('не', 'не', 'PART', 501, 503, 606, 607), ('удалила', 'удалить', 'VERB', 504, 511, 606, 607), ('здесь', 'здесь', 'ADV', 512, 517, 606, 607), ('жарче', 'жарче', 'ADJ', 518, 523, 606, 607), ('чем', 'чем', 'SCONJ', 524, 527, 606, 607), ('в', 'в', 'ADP', 528, 529, 606, 607), ('аду', 'ад', 'NOUN', 530, 533, 606, 607)], 'original_text': 'Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈', 'cleaned_text': 'твой лучший секс спрятан здесь делюсь каналом дипломированного сексолога крис взломала код классного секса мастерски раскрепощает знает миллион горячих техник и лучшие девайсы для взрослых самые полезные посты здесь отрезвляющий пост я все сама прокачай наездницу ролевая игра vip кинотеатр техника оральных ласк как занимается сeксом неудобная женщина кстати крис провела трехдневный безоплатный онлайн интенсив от бревна до богини совместно с врачом и владельцем секс шопа скорее смотри записи пока не удалила здесь жарче чем в аду', 'mapping': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 381, 382, 383, 384, 385, 386, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 445, 454, 471, 474, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606]}\n"
     ]
    }
   ],
   "source": [
    "# Выполняем анализ\n",
    "result = analyzer.analyze(xx)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Результат анализа:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Union\n",
    "\n",
    "class SbertLargeNLU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sberbank-ai/sbert_large_nlu_ru',\n",
    "        model_dir: str = None,\n",
    "        max_length: int = 512,\n",
    "        enable_rocm: bool = True\n",
    "    ):\n",
    "        self.device = self._get_device(enable_rocm)\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.root_dir = self._get_root_dir()\n",
    "        self.model_dir = self._init_model_dir(model_dir)\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "\n",
    "    def _get_device(self, enable_rocm: bool) -> str:\n",
    "        if enable_rocm and torch.cuda.is_available():\n",
    "            os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "            return 'cuda'\n",
    "        return 'cpu'\n",
    "\n",
    "    def _get_root_dir(self) -> Path:\n",
    "        try:\n",
    "            current_file = Path(__file__).resolve()\n",
    "            return current_file.parent.parent if \"src\" in current_file.parts else current_file.parent\n",
    "        except NameError:\n",
    "            return Path(os.getcwd()).resolve()\n",
    "\n",
    "    def _init_model_dir(self, model_dir: str) -> Path:\n",
    "        path = Path(model_dir) if model_dir else self.root_dir / \"models/sbert\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "    def _load_model(self):\n",
    "        local_path = self.model_dir / self.model_name.replace(\"/\", \"__\")\n",
    "        try:\n",
    "            if (local_path / \"config.json\").exists():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "                model = AutoModel.from_pretrained(local_path)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                model = AutoModel.from_pretrained(self.model_name)\n",
    "                model.save_pretrained(local_path, safe_serialization=True)\n",
    "                tokenizer.save_pretrained(local_path)\n",
    "            \n",
    "            return tokenizer, model.to(self.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    def create_embeddings(self, texts: Union[str, List[str]]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "            \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        if attention_mask.sum() == 0:\n",
    "            return torch.zeros((1, model_output.last_hidden_state.size(-1)))\n",
    "            \n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = (\n",
    "            attention_mask\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = SbertLargeNLU()\n",
    "print(model.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
