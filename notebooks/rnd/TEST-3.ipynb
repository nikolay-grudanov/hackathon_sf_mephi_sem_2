{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+rocm6.3\n",
      "GPU –¥–æ—Å—Ç—É–ø–Ω–∞: True\n",
      "–ù–∞–∑–≤–∞–Ω–∏–µ GPU: AMD Radeon RX 7800 XT\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}\")\n",
    "print(f\"–ù–∞–∑–≤–∞–Ω–∏–µ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\n",
    "        \"url_pattern\",\n",
    "        \"emoji_pattern\",\n",
    "        \"html_pattern\",\n",
    "        \"non_letter_pattern\",\n",
    "        \"telegram_pattern\",\n",
    "        \"spaces_pattern\",\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ TextPreprocessor.\n",
    "        –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "        \"\"\"\n",
    "        self.url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001f600-\\U0001f64f\"  # —ç–º–æ—Ü–∏–∏\n",
    "            \"\\U0001f300-\\U0001f5ff\"  # —Å–∏–º–≤–æ–ª—ã\n",
    "            \"\\U0001f680-\\U0001f6ff\"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n",
    "            \"\\U0001f700-\\U0001f77f\"  # –∞–ª—Ö–∏–º–∏—è\n",
    "            \"\\U0001f780-\\U0001f7ff\"  # –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏–≥—É—Ä—ã\n",
    "            \"\\U0001f800-\\U0001f8ff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\n",
    "            \"\\U0001f900-\\U0001f9ff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-2\n",
    "            \"\\U0001fa00-\\U0001fa6f\"  # —à–∞—Ö–º–∞—Ç—ã\n",
    "            \"\\U0001fa70-\\U0001faff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-3\n",
    "            \"\\U00002702-\\U000027b0\"  # Dingbats\n",
    "            \"\\U000024c2-\\U0001f251\"  # Enclosed\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.html_pattern = re.compile(r\"&[a-z]+;\")\n",
    "        self.non_letter_pattern = re.compile(r\"[^a-z–∞-—è\\s]\")\n",
    "        self.telegram_pattern = re.compile(r\"@\\w+|/\\w+\")\n",
    "        self.spaces_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "    def __preprocess(\n",
    "        self,\n",
    "        text,\n",
    "        lowercase=True,\n",
    "        replace_yo=True,\n",
    "        remove_urls=True,\n",
    "        remove_emoji=True,\n",
    "        remove_html=True,\n",
    "        remove_punctuation=True,\n",
    "        remove_telegram=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        –ü—Ä–∏–≤–∞—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –±–∞–∑–æ–≤–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "        :param lowercase: –§–ª–∞–≥ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.\n",
    "        :param replace_yo: –§–ª–∞–≥ –¥–ª—è –∑–∞–º–µ–Ω—ã –±—É–∫–≤—ã \"—ë\" –Ω–∞ \"–µ\".\n",
    "        :param remove_urls: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è URL-—Å—Å—ã–ª–æ–∫.\n",
    "        :param remove_emoji: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —ç–º–æ–¥–∑–∏.\n",
    "        :param remove_html: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π.\n",
    "        :param remove_punctuation: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.\n",
    "        :param remove_telegram: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ–ª–µ–≥—Ä–∞–º-—É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ –±–æ—Ç-–∫–æ–º–∞–Ω–¥.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        result = text\n",
    "\n",
    "        if lowercase:\n",
    "            result = result.lower()\n",
    "\n",
    "        if replace_yo:\n",
    "            result = result.replace(\"—ë\", \"–µ\")\n",
    "\n",
    "        if remove_urls:\n",
    "            result = self.url_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_emoji:\n",
    "            result = self.emoji_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_html:\n",
    "            result = self.html_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_punctuation:\n",
    "            result = self.non_letter_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_telegram:\n",
    "            result = self.telegram_pattern.sub(\" \", result)\n",
    "\n",
    "        return self.spaces_pattern.sub(\" \", result).strip()\n",
    "\n",
    "    def clear_text(self, text):\n",
    "        \"\"\"\n",
    "        –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text)\n",
    "\n",
    "    def clean_for_search(self, text):\n",
    "        \"\"\"\n",
    "        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text, remove_punctuation=False, remove_telegram=False)\n",
    "\n",
    "    def clean_for_embedding(self, text):\n",
    "        \"\"\"\n",
    "        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text, remove_emoji=True, remove_html=True, remove_punctuation=True\n",
    "        )\n",
    "\n",
    "    def clean_for_display(self, text):\n",
    "        \"\"\"\n",
    "        –õ—ë–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text,\n",
    "            lowercase=False,\n",
    "            replace_yo=False,\n",
    "            remove_urls=False,\n",
    "            remove_emoji=False,\n",
    "            remove_punctuation=False,\n",
    "        )\n",
    "\n",
    "    def get_text_stats(self, text):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–∫—Å—Ç—É: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤,\n",
    "        —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.\n",
    "\n",
    "        Args:\n",
    "            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "\n",
    "        Returns:\n",
    "            dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–µ–∫—Å—Ç–∞:\n",
    "                - length: –æ–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
    "                - word_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "                - unique_word_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "                - word_frequencies: —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤\n",
    "                - avg_word_length: —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "                - short_words_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª–∏–Ω–æ–π –º–µ–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "                - long_words_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª–∏–Ω–æ–π –±–æ–ª–µ–µ 7 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                \"length\": 0,\n",
    "                \"word_count\": 0,\n",
    "                \"unique_word_count\": 0,\n",
    "                \"word_frequencies\": {},\n",
    "            }\n",
    "\n",
    "        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç \n",
    "        cleaned_text = self.clear_text(text)\n",
    "\n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "        words = cleaned_text.split()\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        word_count = len(words)\n",
    "        unique_words = set(words)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤\n",
    "        word_frequencies = {}\n",
    "        for word in words:\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        total_chars = sum(len(word) for word in words)\n",
    "        avg_word_length = total_chars / word_count if word_count > 0 else 0\n",
    "        short_words_count = sum(1 for word in words if len(word) < 4)\n",
    "        long_words_count = sum(1 for word in words if len(word) > 7)\n",
    "\n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "        stats = {\n",
    "            \"length\": len(cleaned_text),  # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
    "            \"word_count\": word_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤\n",
    "            \"unique_word_count\": unique_word_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "            \"word_frequencies\": word_frequencies,  # –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤\n",
    "            \"avg_word_length\": round(avg_word_length, 2),  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "            \"short_words_count\": short_words_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (< 4 –±—É–∫–≤)\n",
    "            \"long_words_count\": long_words_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (> 7 –±—É–∫–≤)\n",
    "        }\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—Å–µ–≥–æ–¥–Ω—è –ø—Ä–µ–º—å–µ—Ä–∞ –≤ –º—Å–∫ –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å id –µ–≤–≥–µ–Ω–∏–π –µ–≥–æ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "txt = '–°–µ–≥–æ–¥–Ω—è (26.07) –ø—Ä–µ–º—å–µ—Ä–∞ –≤ 18.00 (–º—Å–∫) - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å [id1868874|–ï–≤–≥–µ–Ω–∏–π –ï–≥–æ—Ä–æ–≤]   https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "text_rreprocessor = TextPreprocessor()\n",
    "print(text_rreprocessor.clear_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 62, 'word_count': 10, 'unique_word_count': 10, 'word_frequencies': {'—Å–µ–≥–æ–¥–Ω—è': 1, '–ø—Ä–µ–º—å–µ—Ä–∞': 1, '–≤': 1, '–º—Å–∫': 1, '–¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π': 1, '–≤—ã–ø—É—Å–∫': 1, '—Å': 1, 'id': 1, '–µ–≤–≥–µ–Ω–∏–π': 1, '–µ–≥–æ—Ä–æ–≤': 1}, 'avg_word_length': 5.3, 'short_words_count': 4, 'long_words_count': 2}\n"
     ]
    }
   ],
   "source": [
    "print(text_rreprocessor.get_text_stats(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "class NatashaLinguisticAnalyzer:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha.\n",
    "    \n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –∏\n",
    "    —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.\n",
    "    \"\"\"\n",
    "    \n",
    "    __slots__ = ('segmenter', 'morph_vocab', 'emb', 'morph_tagger', 'text_preprocessor')\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Natasha –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n",
    "        \n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
    "        - Segmenter: –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        - MorphVocab: –î–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "        - NewsEmbedding: –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π\n",
    "        - NewsMorphTagger: –î–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏)\n",
    "        - TextPreprocessor: –î–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ TextPreprocessor (–∫–æ–º–ø–æ–∑–∏—Ü–∏—è)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "    \n",
    "    def clear_text_with_mapping(self, text: str) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π.\n",
    "        \"\"\"\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é TextPreprocessor\n",
    "        cleaned_text = self.text_preprocessor.clear_text(text)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\n",
    "        mapping = []\n",
    "        orig_index = 0\n",
    "        \n",
    "        for clean_char in cleaned_text:\n",
    "            found = False\n",
    "            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –¥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è\n",
    "            while orig_index < len(text) and not found:\n",
    "                if text[orig_index].lower() == clean_char:\n",
    "                    mapping.append(orig_index)\n",
    "                    orig_index += 1\n",
    "                    found = True\n",
    "                else:\n",
    "                    orig_index += 1\n",
    "            \n",
    "            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "            if not found:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–∑–º–æ–∂–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–ª–∏ -1, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π\n",
    "                mapping.append(len(text) - 1 if text else -1)\n",
    "        \n",
    "        return cleaned_text, mapping\n",
    "\n",
    "    \n",
    "    def map_clean_to_original(self, clean_start: int, clean_end: int, mapping: List[int]) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–∑–∏—Ü–∏—é –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –≤ –ø–æ–∑–∏—Ü–∏—é –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.\n",
    "        \"\"\"\n",
    "        if not mapping:\n",
    "            return -1, -1\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü\n",
    "        if clean_start >= len(mapping):\n",
    "            return -1, -1\n",
    "        \n",
    "        orig_start = mapping[clean_start]\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ clean_end –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã\n",
    "        if clean_end - 1 >= len(mapping):\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –º–∞–ø–ø–∏–Ω–≥–∞ + –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, \n",
    "            # —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "            orig_end = mapping[-1] + 2  # +2 –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "        else:\n",
    "            orig_end = mapping[clean_end - 1] + 1\n",
    "        \n",
    "        return orig_start, orig_end\n",
    "\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha.\n",
    "        \n",
    "        Args:\n",
    "            text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n",
    "            \n",
    "        Returns:\n",
    "            Dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞:\n",
    "                - 'tokens': –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "                  (token, lemma, pos_tag, start_clean, end_clean, start_orig, end_orig)\n",
    "                - 'original_text': –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–æ –æ—á–∏—Å—Ç–∫–∏)\n",
    "                - 'cleaned_text': –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏)\n",
    "                - 'mapping': –ú–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {'tokens': [], 'original_text': '', 'cleaned_text': '', 'mapping': []}\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "        original_text = text\n",
    "        \n",
    "        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å TextPreprocessor –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞\n",
    "        cleaned_text, mapping = self.clear_text_with_mapping(text)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Doc –∏–∑ Natasha\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ø–∏—Å–æ–∫ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ –∫–∞–∫ –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º, —Ç–∞–∫ –∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            # –ü–æ–∑–∏—Ü–∏–∏ –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            start_clean = token.start\n",
    "            end_clean = token.stop\n",
    "            \n",
    "            # –ú–∞–ø–ø–∏–Ω–≥ –≤ –ø–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            start_orig, end_orig = self.map_clean_to_original(start_clean, end_clean, mapping)\n",
    "            \n",
    "            tokens.append((\n",
    "                token.text,          # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω\n",
    "                token.lemma,         # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞\n",
    "                token.pos,           # –ß–∞—Å—Ç—å —Ä–µ—á–∏\n",
    "                start_clean,         # –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                end_clean,           # –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                start_orig,          # –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                end_orig             # –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            ))\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'mapping': mapping\n",
    "        }\n",
    "    \n",
    "    def analyze_query(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –∏ —Ç–µ–∫—Å—Ç–∞—Ö.\n",
    "        \n",
    "        Args:\n",
    "            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            \n",
    "        Returns:\n",
    "            Dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É analyze)\n",
    "        \"\"\"\n",
    "        return self.analyze(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\n",
      "{'tokens': [('—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–≥–æ–¥–Ω—è', 'ADV', 0, 7, 0, 7), ('–ø—Ä–µ–º—å–µ—Ä–∞', '–ø—Ä–µ–º—å–µ—Ä–∞', 'NOUN', 8, 16, 16, 24), ('–≤', '–≤', 'ADP', 17, 18, 25, 26), ('–º—Å–∫', '–º—Å–∫', 'NOUN', 19, 22, 34, 37), ('–¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π', '–¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π', 'ADJ', 23, 35, 41, 53), ('–≤—ã–ø—É—Å–∫', '–≤—ã–ø—É—Å–∫', 'NOUN', 36, 42, 54, 60), ('—Å', '—Å', 'ADP', 43, 44, 61, 62), ('id', 'id', 'X', 45, 47, 64, 66), ('–µ–≤–≥–µ–Ω–∏–π', '–µ–≤–≥–µ–Ω–∏–π', 'ADJ', 48, 55, 82, 138), ('–µ–≥–æ—Ä–æ–≤', '–µ–≥–æ—Ä–æ–≤', 'NOUN', 56, 62, 137, 138)], 'original_text': '–°–µ–≥–æ–¥–Ω—è (26.07) –ø—Ä–µ–º—å–µ—Ä–∞ –≤ 18.00 (–º—Å–∫) - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å [id1868874|–ï–≤–≥–µ–Ω–∏–π –ï–≥–æ—Ä–æ–≤] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp', 'cleaned_text': '—Å–µ–≥–æ–¥–Ω—è –ø—Ä–µ–º—å–µ—Ä–∞ –≤ –º—Å–∫ –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å id –µ–≤–≥–µ–Ω–∏–π –µ–≥–æ—Ä–æ–≤', 'mapping': [0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 34, 35, 36, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 81, 82, 87, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä\n",
    "analyzer = NatashaLinguisticAnalyzer()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
    "txt = '–°–µ–≥–æ–¥–Ω—è (26.07) –ø—Ä–µ–º—å–µ—Ä–∞ –≤ 18.00 (–º—Å–∫) - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å [id1868874|–ï–≤–≥–µ–Ω–∏–π –ï–≥–æ—Ä–æ–≤] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "\n",
    "\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑\n",
    "result = analyzer.analyze(txt)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = '–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\n",
      "{'tokens': [('—Ç–≤–æ–π', '—Ç–≤–æ–π', 'DET', 0, 4, 0, 4), ('–ª—É—á—à–∏–π', '—Ö–æ—Ä–æ—à–∏–π', 'ADJ', 5, 11, 5, 11), ('—Å–µ–∫—Å', '—Å–µ–∫—Å', 'NOUN', 12, 16, 12, 16), ('—Å–ø—Ä—è—Ç–∞–Ω', '—Å–ø—Ä—è—Ç–∞—Ç—å', 'VERB', 17, 24, 17, 24), ('–∑–¥–µ—Å—å', '–∑–¥–µ—Å—å', 'ADV', 25, 30, 25, 30), ('–¥–µ–ª—é—Å—å', '–¥–µ–ª–∏—Ç—å—Å—è', 'VERB', 31, 37, 34, 40), ('–∫–∞–Ω–∞–ª–æ–º', '–∫–∞–Ω–∞–ª', 'NOUN', 38, 45, 41, 48), ('–¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ', '–¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', 'ADJ', 46, 62, 49, 65), ('—Å–µ–∫—Å–æ–ª–æ–≥–∞', '—Å–µ–∫—Å–æ–ª–æ–≥', 'ADJ', 63, 72, 66, 75), ('–∫—Ä–∏—Å', '–∫—Ä–∏—Å', 'NOUN', 73, 77, 77, 81), ('–≤–∑–ª–æ–º–∞–ª–∞', '–≤–∑–ª–æ–º–∞—Ç—å', 'VERB', 78, 86, 82, 90), ('–∫–æ–¥', '–∫–æ–¥', 'NOUN', 87, 90, 91, 94), ('–∫–ª–∞—Å—Å–Ω–æ–≥–æ', '–∫–ª–∞—Å—Å–Ω—ã–π', 'ADJ', 91, 100, 95, 104), ('—Å–µ–∫—Å–∞', '—Å–µ–∫—Å', 'NOUN', 101, 106, 105, 110), ('–º–∞—Å—Ç–µ—Ä—Å–∫–∏', '–º–∞—Å—Ç–µ—Ä—Å–∫–∏', 'ADV', 107, 116, 112, 121), ('—Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç', '—Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞—Ç—å', 'NOUN', 117, 129, 122, 134), ('–∑–Ω–∞–µ—Ç', '–∑–Ω–∞—Ç—å', 'VERB', 130, 135, 136, 141), ('–º–∏–ª–ª–∏–æ–Ω', '–º–∏–ª–ª–∏–æ–Ω', 'NOUN', 136, 143, 142, 149), ('–≥–æ—Ä—è—á–∏—Ö', '–≥–æ—Ä—è—á–∏–π', 'ADJ', 144, 151, 150, 157), ('—Ç–µ—Ö–Ω–∏–∫', '—Ç–µ—Ö–Ω–∏–∫–∞', 'NOUN', 152, 158, 158, 164), ('–∏', '–∏', 'CCONJ', 159, 160, 165, 166), ('–ª—É—á—à–∏–µ', '—Ö–æ—Ä–æ—à–∏–π', 'ADJ', 161, 167, 167, 173), ('–¥–µ–≤–∞–π—Å—ã', '–¥–µ–≤–∞–π—Å', 'NOUN', 168, 175, 174, 181), ('–¥–ª—è', '–¥–ª—è', 'ADP', 176, 179, 182, 185), ('–≤–∑—Ä–æ—Å–ª—ã—Ö', '–≤–∑—Ä–æ—Å–ª—ã–π', 'NOUN', 180, 188, 186, 194), ('—Å–∞–º—ã–µ', '—Å–∞–º—ã–π', 'ADJ', 189, 194, 198, 203), ('–ø–æ–ª–µ–∑–Ω—ã–µ', '–ø–æ–ª–µ–∑–Ω—ã–π', 'ADJ', 195, 203, 204, 212), ('–ø–æ—Å—Ç—ã', '–ø–æ—Å—Ç', 'NOUN', 204, 209, 213, 218), ('–∑–¥–µ—Å—å', '–∑–¥–µ—Å—å', 'ADV', 210, 215, 219, 224), ('–æ—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π', '–æ—Ç—Ä–µ–∑–≤–ª—è—Ç—å', 'ADJ', 216, 228, 228, 240), ('–ø–æ—Å—Ç', '–ø–æ—Å—Ç', 'NOUN', 229, 233, 241, 245), ('—è', '—è', 'PRON', 234, 235, 247, 248), ('–≤—Å–µ', '–≤–µ—Å—å', 'PRON', 236, 239, 249, 252), ('—Å–∞–º–∞', '—Å–∞–º', 'ADJ', 240, 244, 253, 257), ('–ø—Ä–æ–∫–∞—á–∞–π', '–ø—Ä–æ–∫–∞—á–∞—Ç—å', 'ADJ', 245, 253, 262, 270), ('–Ω–∞–µ–∑–¥–Ω–∏—Ü—É', '–Ω–∞–µ–∑–¥–Ω–∏—Ü–∞', 'ADJ', 254, 263, 271, 280), ('—Ä–æ–ª–µ–≤–∞—è', '—Ä–æ–ª–µ–≤—ã–π', 'ADJ', 264, 271, 282, 289), ('–∏–≥—Ä–∞', '–∏–≥—Ä–∞', 'NOUN', 272, 276, 290, 294), ('vip', 'vip', 'X', 277, 280, 296, 299), ('–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä', '–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä', 'NOUN', 281, 290, 300, 309), ('—Ç–µ—Ö–Ω–∏–∫–∞', '—Ç–µ—Ö–Ω–∏–∫–∞', 'NOUN', 291, 298, 313, 320), ('–æ—Ä–∞–ª—å–Ω—ã—Ö', '–æ—Ä–∞–ª—å–Ω—ã–π', 'ADJ', 299, 307, 321, 329), ('–ª–∞—Å–∫', '–ª–∞—Å–∫–∞', 'NOUN', 308, 312, 330, 334), ('–∫–∞–∫', '–∫–∞–∫', 'SCONJ', 313, 316, 339, 342), ('–∑–∞–Ω–∏–º–∞–µ—Ç—Å—è', '–∑–∞–Ω–∏–º–∞—Ç—å—Å—è', 'VERB', 317, 327, 343, 353), ('—Å', '—Å', 'ADP', 328, 329, 354, 355), ('e', 'e', 'X', 329, 330, 355, 356), ('–∫—Å–æ–º', '–∫—Å–æ–º', 'ADJ', 330, 334, 356, 360), ('–Ω–µ—É–¥–æ–±–Ω–∞—è', '–Ω–µ—É–¥–æ–±–Ω—ã–π', 'ADJ', 335, 344, 361, 370), ('–∂–µ–Ω—â–∏–Ω–∞', '–∂–µ–Ω—â–∏–Ω–∞', 'NOUN', 345, 352, 371, 378), ('–∫—Å—Ç–∞—Ç–∏', '–∫—Å—Ç–∞—Ç–∏', 'ADV', 353, 359, 381, 387), ('–∫—Ä–∏—Å', '–∫—Ä–∏—Å', 'NOUN', 360, 364, 389, 393), ('–ø—Ä–æ–≤–µ–ª–∞', '–ø—Ä–æ–≤–µ—Å—Ç–∏', 'VERB', 365, 372, 394, 401), ('—Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π', '—Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π', 'ADJ', 373, 384, 402, 413), ('–±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π', '–±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π', 'ADJ', 385, 396, 414, 425), ('–æ–Ω–ª–∞–π–Ω', '–æ–Ω–ª–∞–π–Ω', 'ADV', 397, 403, 426, 432), ('–∏–Ω—Ç–µ–Ω—Å–∏–≤', '–∏–Ω—Ç–µ–Ω—Å–∏–≤', 'NOUN', 404, 412, 433, 441), ('–æ—Ç', '–æ—Ç', 'ADP', 413, 415, 454, 472), ('–±—Ä–µ–≤–Ω–∞', '–±—Ä–µ–≤–Ω–æ', 'NOUN', 416, 422, 606, 607), ('–¥–æ', '–¥–æ', 'ADP', 423, 425, 606, 607), ('–±–æ–≥–∏–Ω–∏', '–±–æ–≥–∏–Ω—è', 'NOUN', 426, 432, 606, 607), ('—Å–æ–≤–º–µ—Å—Ç–Ω–æ', '—Å–æ–≤–º–µ—Å—Ç–Ω–æ', 'ADV', 433, 442, 606, 607), ('—Å', '—Å', 'ADP', 443, 444, 606, 607), ('–≤—Ä–∞—á–æ–º', '–≤—Ä–∞—á', 'NOUN', 445, 451, 606, 607), ('–∏', '–∏', 'CCONJ', 452, 453, 606, 607), ('–≤–ª–∞–¥–µ–ª—å—Ü–µ–º', '–≤–ª–∞–¥–µ–ª–µ—Ü', 'NOUN', 454, 464, 606, 607), ('—Å–µ–∫—Å', '—Å–µ–∫—Å', 'NOUN', 465, 469, 606, 607), ('—à–æ–ø–∞', '—à–æ–ø', 'NOUN', 470, 474, 606, 607), ('—Å–∫–æ—Ä–µ–µ', '—Å–∫–æ—Ä—ã–π', 'ADV', 475, 481, 606, 607), ('—Å–º–æ—Ç—Ä–∏', '—Å–º–æ—Ç—Ä–∏', 'ADV', 482, 488, 606, 607), ('–∑–∞–ø–∏—Å–∏', '–∑–∞–ø–∏—Å—å', 'NOUN', 489, 495, 606, 607), ('–ø–æ–∫–∞', '–ø–æ–∫–∞', 'ADV', 496, 500, 606, 607), ('–Ω–µ', '–Ω–µ', 'PART', 501, 503, 606, 607), ('—É–¥–∞–ª–∏–ª–∞', '—É–¥–∞–ª–∏—Ç—å', 'VERB', 504, 511, 606, 607), ('–∑–¥–µ—Å—å', '–∑–¥–µ—Å—å', 'ADV', 512, 517, 606, 607), ('–∂–∞—Ä—á–µ', '–∂–∞—Ä—á–µ', 'ADJ', 518, 523, 606, 607), ('—á–µ–º', '—á–µ–º', 'SCONJ', 524, 527, 606, 607), ('–≤', '–≤', 'ADP', 528, 529, 606, 607), ('–∞–¥—É', '–∞–¥', 'NOUN', 530, 533, 606, 607)], 'original_text': '–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà', 'cleaned_text': '—Ç–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å –¥–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞ –∫—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞ –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö —Å–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å –æ—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç —è –≤—Å–µ —Å–∞–º–∞ –ø—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É —Ä–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ vip –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä —Ç–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ –∫–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞ –∫—Å—Ç–∞—Ç–∏ –∫—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤ –æ—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –±–æ–≥–∏–Ω–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å —à–æ–ø–∞ —Å–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏ –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ –∑–¥–µ—Å—å –∂–∞—Ä—á–µ —á–µ–º –≤ –∞–¥—É', 'mapping': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 381, 382, 383, 384, 385, 386, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 445, 454, 471, 474, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606, 606]}\n"
     ]
    }
   ],
   "source": [
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑\n",
    "result = analyzer.analyze(xx)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Union\n",
    "\n",
    "class SbertLargeNLU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sberbank-ai/sbert_large_nlu_ru',\n",
    "        model_dir: str = None,\n",
    "        max_length: int = 512,\n",
    "        enable_rocm: bool = True\n",
    "    ):\n",
    "        self.device = self._get_device(enable_rocm)\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.root_dir = self._get_root_dir()\n",
    "        self.model_dir = self._init_model_dir(model_dir)\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "\n",
    "    def _get_device(self, enable_rocm: bool) -> str:\n",
    "        if enable_rocm and torch.cuda.is_available():\n",
    "            os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "            return 'cuda'\n",
    "        return 'cpu'\n",
    "\n",
    "    def _get_root_dir(self) -> Path:\n",
    "        try:\n",
    "            current_file = Path(__file__).resolve()\n",
    "            return current_file.parent.parent if \"src\" in current_file.parts else current_file.parent\n",
    "        except NameError:\n",
    "            return Path(os.getcwd()).resolve()\n",
    "\n",
    "    def _init_model_dir(self, model_dir: str) -> Path:\n",
    "        path = Path(model_dir) if model_dir else self.root_dir / \"models/sbert\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "    def _load_model(self):\n",
    "        local_path = self.model_dir / self.model_name.replace(\"/\", \"__\")\n",
    "        try:\n",
    "            if (local_path / \"config.json\").exists():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "                model = AutoModel.from_pretrained(local_path)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                model = AutoModel.from_pretrained(self.model_name)\n",
    "                model.save_pretrained(local_path, safe_serialization=True)\n",
    "                tokenizer.save_pretrained(local_path)\n",
    "            \n",
    "            return tokenizer, model.to(self.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    def create_embeddings(self, texts: Union[str, List[str]]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "            \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        if attention_mask.sum() == 0:\n",
    "            return torch.zeros((1, model_output.last_hidden_state.size(-1)))\n",
    "            \n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = (\n",
    "            attention_mask\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = SbertLargeNLU()\n",
    "print(model.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
