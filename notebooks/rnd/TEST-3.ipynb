{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libamdhip64.so: cannot enable executable stack as shared object requires: Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROCm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mroc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rocm/lib/python3.10/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libamdhip64.so: cannot enable executable stack as shared object requires: Invalid argument"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"ROCm: {torch.version.roc}\")\n",
    "print(f\"GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}\")\n",
    "print(f\"–ù–∞–∑–≤–∞–Ω–∏–µ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\n",
    "        \"url_pattern\",\n",
    "        \"emoji_pattern\",\n",
    "        \"html_pattern\",\n",
    "        \"non_letter_pattern\",\n",
    "        \"telegram_pattern\",\n",
    "        \"spaces_pattern\",\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ TextPreprocessor.\n",
    "        –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "        \"\"\"\n",
    "        self.url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001f600-\\U0001f64f\"  # —ç–º–æ—Ü–∏–∏\n",
    "            \"\\U0001f300-\\U0001f5ff\"  # —Å–∏–º–≤–æ–ª—ã\n",
    "            \"\\U0001f680-\\U0001f6ff\"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n",
    "            \"\\U0001f700-\\U0001f77f\"  # –∞–ª—Ö–∏–º–∏—è\n",
    "            \"\\U0001f780-\\U0001f7ff\"  # –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏–≥—É—Ä—ã\n",
    "            \"\\U0001f800-\\U0001f8ff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\n",
    "            \"\\U0001f900-\\U0001f9ff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-2\n",
    "            \"\\U0001fa00-\\U0001fa6f\"  # —à–∞—Ö–º–∞—Ç—ã\n",
    "            \"\\U0001fa70-\\U0001faff\"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã-3\n",
    "            \"\\U00002702-\\U000027b0\"  # Dingbats\n",
    "            \"\\U000024c2-\\U0001f251\"  # Enclosed\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.html_pattern = re.compile(r\"&[a-z]+;\")\n",
    "        self.non_letter_pattern = re.compile(r\"[^a-z–∞-—è\\s]\")\n",
    "        self.telegram_pattern = re.compile(r\"@\\w+|/\\w+\")\n",
    "        self.spaces_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "    def __preprocess(\n",
    "        self,\n",
    "        text,\n",
    "        lowercase=True,\n",
    "        replace_yo=True,\n",
    "        remove_urls=True,\n",
    "        remove_emoji=True,\n",
    "        remove_html=True,\n",
    "        remove_punctuation=True,\n",
    "        remove_telegram=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        –ü—Ä–∏–≤–∞—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –±–∞–∑–æ–≤–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "        :param lowercase: –§–ª–∞–≥ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.\n",
    "        :param replace_yo: –§–ª–∞–≥ –¥–ª—è –∑–∞–º–µ–Ω—ã –±—É–∫–≤—ã \"—ë\" –Ω–∞ \"–µ\".\n",
    "        :param remove_urls: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è URL-—Å—Å—ã–ª–æ–∫.\n",
    "        :param remove_emoji: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —ç–º–æ–¥–∑–∏.\n",
    "        :param remove_html: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π.\n",
    "        :param remove_punctuation: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.\n",
    "        :param remove_telegram: –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ç–µ–ª–µ–≥—Ä–∞–º-—É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ –±–æ—Ç-–∫–æ–º–∞–Ω–¥.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        result = text\n",
    "\n",
    "        if lowercase:\n",
    "            result = result.lower()\n",
    "\n",
    "        if replace_yo:\n",
    "            result = result.replace(\"—ë\", \"–µ\")\n",
    "\n",
    "        if remove_urls:\n",
    "            result = self.url_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_emoji:\n",
    "            result = self.emoji_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_html:\n",
    "            result = self.html_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_punctuation:\n",
    "            result = self.non_letter_pattern.sub(\" \", result)\n",
    "\n",
    "        if remove_telegram:\n",
    "            result = self.telegram_pattern.sub(\" \", result)\n",
    "\n",
    "        return self.spaces_pattern.sub(\" \", result).strip()\n",
    "\n",
    "    def clear_text(self, text):\n",
    "        \"\"\"\n",
    "        –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text)\n",
    "\n",
    "    def clean_for_search(self, text):\n",
    "        \"\"\"\n",
    "        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(text, remove_punctuation=False, remove_telegram=False)\n",
    "\n",
    "    def clean_for_embedding(self, text):\n",
    "        \"\"\"\n",
    "        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text, remove_emoji=True, remove_html=True, remove_punctuation=True\n",
    "        )\n",
    "\n",
    "    def clean_for_display(self, text):\n",
    "        \"\"\"\n",
    "        –õ—ë–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.\n",
    "\n",
    "        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.\n",
    "        :return: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        \"\"\"\n",
    "        return self.__preprocess(\n",
    "            text,\n",
    "            lowercase=False,\n",
    "            replace_yo=False,\n",
    "            remove_urls=False,\n",
    "            remove_emoji=False,\n",
    "            remove_punctuation=False,\n",
    "        )\n",
    "\n",
    "    def get_text_stats(self, text):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–∫—Å—Ç—É: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤,\n",
    "        —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.\n",
    "\n",
    "        Args:\n",
    "            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "\n",
    "        Returns:\n",
    "            dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–µ–∫—Å—Ç–∞:\n",
    "                - length: –æ–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
    "                - word_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "                - unique_word_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "                - word_frequencies: —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤\n",
    "                - avg_word_length: —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "                - short_words_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª–∏–Ω–æ–π –º–µ–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "                - long_words_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª–∏–Ω–æ–π –±–æ–ª–µ–µ 7 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                \"length\": 0,\n",
    "                \"word_count\": 0,\n",
    "                \"unique_word_count\": 0,\n",
    "                \"word_frequencies\": {},\n",
    "            }\n",
    "\n",
    "        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç \n",
    "        cleaned_text = self.clear_text(text)\n",
    "\n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "        words = cleaned_text.split()\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        word_count = len(words)\n",
    "        unique_words = set(words)\n",
    "        unique_word_count = len(unique_words)\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤\n",
    "        word_frequencies = {}\n",
    "        for word in words:\n",
    "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "\n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        total_chars = sum(len(word) for word in words)\n",
    "        avg_word_length = total_chars / word_count if word_count > 0 else 0\n",
    "        short_words_count = sum(1 for word in words if len(word) < 4)\n",
    "        long_words_count = sum(1 for word in words if len(word) > 7)\n",
    "\n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "        stats = {\n",
    "            \"length\": len(cleaned_text),  # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
    "            \"word_count\": word_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤\n",
    "            \"unique_word_count\": unique_word_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
    "            \"word_frequencies\": word_frequencies,  # –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤\n",
    "            \"avg_word_length\": round(avg_word_length, 2),  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "            \"short_words_count\": short_words_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (< 4 –±—É–∫–≤)\n",
    "            \"long_words_count\": long_words_count,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (> 7 –±—É–∫–≤)\n",
    "        }\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '–°–µ–≥–æ–¥–Ω—è (26.07) –ø—Ä–µ–º—å–µ—Ä–∞ –≤ 18.00 (–º—Å–∫) - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å [id1868874|–ï–≤–≥–µ–Ω–∏–π –ï–≥–æ—Ä–æ–≤]   https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "text_rreprocessor = TextPreprocessor()\n",
    "print(text_rreprocessor.clear_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_rreprocessor.get_text_stats(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "class NatashaLinguisticAnalyzer:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha.\n",
    "    \n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –∏\n",
    "    —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.\n",
    "    \"\"\"\n",
    "    \n",
    "    __slots__ = ('segmenter', 'morph_vocab', 'emb', 'morph_tagger', 'text_preprocessor')\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Natasha –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n",
    "        \n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
    "        - Segmenter: –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        - MorphVocab: –î–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "        - NewsEmbedding: –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π\n",
    "        - NewsMorphTagger: –î–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏)\n",
    "        - TextPreprocessor: –î–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "        \"\"\"\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ TextPreprocessor (–∫–æ–º–ø–æ–∑–∏—Ü–∏—è)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "    \n",
    "    def clear_text_with_mapping(self, text: str) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π.\n",
    "        \"\"\"\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é TextPreprocessor\n",
    "        cleaned_text = self.text_preprocessor.clear_text(text)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\n",
    "        mapping = []\n",
    "        orig_index = 0\n",
    "        \n",
    "        for clean_char in cleaned_text:\n",
    "            found = False\n",
    "            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –¥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è\n",
    "            while orig_index < len(text) and not found:\n",
    "                if text[orig_index].lower() == clean_char:\n",
    "                    mapping.append(orig_index)\n",
    "                    orig_index += 1\n",
    "                    found = True\n",
    "                else:\n",
    "                    orig_index += 1\n",
    "            \n",
    "            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "            if not found:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–∑–º–æ–∂–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–ª–∏ -1, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π\n",
    "                mapping.append(len(text) - 1 if text else -1)\n",
    "        \n",
    "        return cleaned_text, mapping\n",
    "\n",
    "    \n",
    "    def map_clean_to_original(self, clean_start: int, clean_end: int, mapping: List[int]) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–∑–∏—Ü–∏—é –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –≤ –ø–æ–∑–∏—Ü–∏—é –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.\n",
    "        \"\"\"\n",
    "        if not mapping:\n",
    "            return -1, -1\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü\n",
    "        if clean_start >= len(mapping):\n",
    "            return -1, -1\n",
    "        \n",
    "        orig_start = mapping[clean_start]\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ clean_end –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã\n",
    "        if clean_end - 1 >= len(mapping):\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –º–∞–ø–ø–∏–Ω–≥–∞ + –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, \n",
    "            # —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "            orig_end = mapping[-1] + 2  # +2 –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "        else:\n",
    "            orig_end = mapping[clean_end - 1] + 1\n",
    "        \n",
    "        return orig_start, orig_end\n",
    "\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha.\n",
    "        \n",
    "        Args:\n",
    "            text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n",
    "            \n",
    "        Returns:\n",
    "            Dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞:\n",
    "                - 'tokens': –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "                  (token, lemma, pos_tag, start_clean, end_clean, start_orig, end_orig)\n",
    "                - 'original_text': –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–æ –æ—á–∏—Å—Ç–∫–∏)\n",
    "                - 'cleaned_text': –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏)\n",
    "                - 'mapping': –ú–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏–∑ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {'tokens': [], 'original_text': '', 'cleaned_text': '', 'mapping': []}\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "        original_text = text\n",
    "        \n",
    "        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å TextPreprocessor –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞\n",
    "        cleaned_text, mapping = self.clear_text_with_mapping(text)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Doc –∏–∑ Natasha\n",
    "        doc = Doc(cleaned_text)\n",
    "        \n",
    "        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        doc.segment(self.segmenter)\n",
    "        \n",
    "        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏)\n",
    "        doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(self.morph_vocab)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ø–∏—Å–æ–∫ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ –∫–∞–∫ –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º, —Ç–∞–∫ –∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            # –ü–æ–∑–∏—Ü–∏–∏ –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            start_clean = token.start\n",
    "            end_clean = token.stop\n",
    "            \n",
    "            # –ú–∞–ø–ø–∏–Ω–≥ –≤ –ø–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            start_orig, end_orig = self.map_clean_to_original(start_clean, end_clean, mapping)\n",
    "            \n",
    "            tokens.append((\n",
    "                token.text,          # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω\n",
    "                token.lemma,         # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞\n",
    "                token.pos,           # –ß–∞—Å—Ç—å —Ä–µ—á–∏\n",
    "                start_clean,         # –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                end_clean,           # –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                start_orig,          # –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "                end_orig             # –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            ))\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'mapping': mapping\n",
    "        }\n",
    "    \n",
    "    def analyze_query(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –∏ —Ç–µ–∫—Å—Ç–∞—Ö.\n",
    "        \n",
    "        Args:\n",
    "            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            \n",
    "        Returns:\n",
    "            Dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É analyze)\n",
    "        \"\"\"\n",
    "        return self.analyze(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä\n",
    "analyzer = NatashaLinguisticAnalyzer()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\n",
    "txt = '–°–µ–≥–æ–¥–Ω—è (26.07) –ø—Ä–µ–º—å–µ—Ä–∞ –≤ 18.00 (–º—Å–∫) - –¥–æ–ª–≥–æ–∂–¥–∞–Ω–Ω—ã–π –≤—ã–ø—É—Å–∫ —Å [id1868874|–ï–≤–≥–µ–Ω–∏–π –ï–≥–æ—Ä–æ–≤] https://youtu.be/a4HBdcgpjzI?si=kRUzKulyQ2GP_8fp'\n",
    "\n",
    "\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑\n",
    "result = analyzer.analyze(txt)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = '–¢–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –î–µ–ª—é—Å—å –∫–∞–Ω–∞–ª–æ–º –¥–∏–ø–ª–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ–∫—Å–æ–ª–æ–≥–∞. –ö—Ä–∏—Å –≤–∑–ª–æ–º–∞–ª–∞ –∫–æ–¥ –∫–ª–∞—Å—Å–Ω–æ–≥–æ —Å–µ–∫—Å–∞, –º–∞—Å—Ç–µ—Ä—Å–∫–∏ —Ä–∞—Å–∫—Ä–µ–ø–æ—â–∞–µ—Ç, –∑–Ω–∞–µ—Ç –º–∏–ª–ª–∏–æ–Ω –≥–æ—Ä—è—á–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –ª—É—á—à–∏–µ –¥–µ–≤–∞–π—Å—ã –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö üòª  –°–∞–º—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–æ—Å—Ç—ã –∑–¥–µ—Å—å:   –û—Ç—Ä–µ–∑–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç ¬´–Ø –≤—Å–µ —Å–∞–º–∞!¬ª   –ü—Ä–æ–∫–∞—á–∞–π –Ω–∞–µ–∑–¥–Ω–∏—Ü—É  –†–æ–ª–µ–≤–∞—è –∏–≥—Ä–∞ ¬´VIP –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä¬ª   –¢–µ—Ö–Ω–∏–∫–∞ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫ üí£   –ö–∞–∫ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Åe–∫—Å–æ–º –Ω–µ—É–¥–æ–±–Ω–∞—è –∂–µ–Ω—â–∏–Ω–∞   –ö—Å—Ç–∞—Ç–∏, –ö—Ä–∏—Å –ø—Ä–æ–≤–µ–ª–∞ —Ç—Ä–µ—Ö–¥–Ω–µ–≤–Ω—ã–π –±–µ–∑–æ–ø–ª–∞—Ç–Ω—ã–π –æ–Ω–ª–∞–π–Ω –∏–Ω—Ç–µ–Ω—Å–∏–≤-¬´–û—Ç –±—Ä–µ–≤–Ω–∞ –¥–æ –ë–æ–≥–∏–Ω–∏¬ª. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤—Ä–∞—á–æ–º –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —Å–µ–∫—Å-—à–æ–ø–∞.   –°–∫–æ—Ä–µ–µ —Å–º–æ—Ç—Ä–∏ –∑–∞–ø–∏—Å–∏, –ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª–∏–ª–∞ üîû  https://t.me/sekretskris/1048   –ó–¥–µ—Å—å –∂–∞—Ä—á–µ, —á–µ–º –≤ –∞–¥—É üòà'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑\n",
    "result = analyzer.analyze(xx)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Union\n",
    "\n",
    "class SbertLargeNLU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'sberbank-ai/sbert_large_nlu_ru',\n",
    "        model_dir: str = None,\n",
    "        max_length: int = 512,\n",
    "        enable_rocm: bool = True\n",
    "    ):\n",
    "        self.device = self._get_device(enable_rocm)\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.root_dir = self._get_root_dir()\n",
    "        self.model_dir = self._init_model_dir(model_dir)\n",
    "        self.tokenizer, self.model = self._load_model()\n",
    "\n",
    "    def _get_device(self, enable_rocm: bool) -> str:\n",
    "        if enable_rocm and torch.cuda.is_available():\n",
    "            os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "            return 'cuda'\n",
    "        return 'cpu'\n",
    "\n",
    "    def _get_root_dir(self) -> Path:\n",
    "        try:\n",
    "            current_file = Path(__file__).resolve()\n",
    "            return current_file.parent.parent if \"src\" in current_file.parts else current_file.parent\n",
    "        except NameError:\n",
    "            return Path(os.getcwd()).resolve()\n",
    "\n",
    "    def _init_model_dir(self, model_dir: str) -> Path:\n",
    "        path = Path(model_dir) if model_dir else self.root_dir / \"models/sbert\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "    def _load_model(self):\n",
    "        local_path = self.model_dir / self.model_name.replace(\"/\", \"__\")\n",
    "        try:\n",
    "            if (local_path / \"config.json\").exists():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "                model = AutoModel.from_pretrained(local_path)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                model = AutoModel.from_pretrained(self.model_name)\n",
    "                model.save_pretrained(local_path, safe_serialization=True)\n",
    "                tokenizer.save_pretrained(local_path)\n",
    "            \n",
    "            return tokenizer, model.to(self.device)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    def create_embeddings(self, texts: Union[str, List[str]]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "            \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]\n",
    "        if not texts:\n",
    "            return torch.empty((0,))\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        if attention_mask.sum() == 0:\n",
    "            return torch.zeros((1, model_output.last_hidden_state.size(-1)))\n",
    "            \n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = (\n",
    "            attention_mask\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SbertLargeNLU()\n",
    "print(model.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
