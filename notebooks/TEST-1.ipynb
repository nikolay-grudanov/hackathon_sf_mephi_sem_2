{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+rocm6.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'  # Для AMD GPU\n",
    "import torch\n",
    "print(torch.__version__)  # Должно вывести: 2.5.1+rocm6.2\n",
    "print(torch.cuda.is_available())  # Проверка работы ROCm (True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия PyTorch: 2.5.1+rocm6.2\n",
      "Доступно GPU: True\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "print(\"Версия PyTorch:\", torch.__version__)\n",
    "print(\"Доступно GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация модели для русских текстов\n",
    "model = SentenceTransformer(\"sberbank-ai/sbert_large_mt_nlu_ru\")\n",
    "print(\"Модель загружена:\", model.device)  # Проверка использования GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▌ Результаты BM25:\n",
      "Запрос: 'привет обучение'\n",
      "\n",
      "Оценки релевантности:\n",
      "• 'привет мир': 0.89\n",
      "• 'семантический поиск': 0.00\n",
      "• 'машинное обучение': 0.89\n",
      "• 'большие языковые модели': 0.00\n",
      "\n",
      "Топ-2 документа:\n",
      "1. машинное обучение\n",
      "2. привет мир\n"
     ]
    }
   ],
   "source": [
    "# Пример использования BM25\n",
    "# Подготовка корпуса документов\n",
    "corpus = [\"привет мир\", \"семантический поиск\", \"машинное обучение\", \"большие языковые модели\"]\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Токенизация корпуса\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "\n",
    "# Инициализация модели BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Пример запроса\n",
    "query = \"привет обучение\"\n",
    "tokenized_query = query.split()\n",
    "\n",
    "# Расчет релевантности\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "top_docs = bm25.get_top_n(tokenized_query, corpus, n=2)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"\\n▌ Результаты BM25:\")\n",
    "print(f\"Запрос: '{query}'\")\n",
    "print(\"\\nОценки релевантности:\")\n",
    "for doc, score in zip(corpus, scores):\n",
    "    print(f\"• '{doc}': {score:.2f}\")\n",
    "\n",
    "print(\"\\nТоп-2 документа:\")\n",
    "for i, doc in enumerate(top_docs, 1):\n",
    "    print(f\"{i}. {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Леммы: ['машинный', 'обучение', 'позволять', 'компьютер', 'обучаться', 'на', 'данные', '.', 'искусственный', 'интеллект', 'менять', 'мир', '.']\n",
      "\n",
      "Триплеты: []\n",
      "\n",
      "Сходство между предложениями:\n",
      "1 vs 2: 0.58\n",
      "1 vs 3: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gna/anaconda3/envs/rocm/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at ../aten/src/ATen/Context.cpp:296.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    }
   ],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Инициализация моделей\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "sbert_model = model # Используйте предварительно загруженную модель\n",
    "\n",
    "# Пример текста\n",
    "text = \"Машинное обучение позволяет компьютерам обучаться на данных. Искусственный интеллект меняет мир.\"\n",
    "\n",
    "# Обработка текста с Natasha\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "# Лемматизация\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "lemmas = [token.lemma for token in doc.tokens]\n",
    "print(\"Леммы:\", lemmas)\n",
    "\n",
    "# Извлечение триплетов (субъект-действие-объект)\n",
    "triplets = []\n",
    "for sent in doc.sents:\n",
    "    subj = [token.lemma for token in sent.tokens if 'Subj' in sent.morph]\n",
    "    obj = [token.lemma for token in sent.tokens if 'Obj' in sent.morph]\n",
    "    if subj and obj:\n",
    "        triplets.append((' '.join(subj), 'действие', ' '.join(obj)))\n",
    "print(\"\\nТриплеты:\", triplets)\n",
    "\n",
    "# Семантическое сравнение с SBERT\n",
    "sentences = [\n",
    "    \"нейронные сети анализируют информацию\",\n",
    "    \"искусственный интеллект обрабатывает данные\",\n",
    "    \"погода в Москве сегодня солнечная\"\n",
    "]\n",
    "\n",
    "embeddings = sbert_model.encode(sentences)\n",
    "similarity = cosine_similarity([embeddings[0]], embeddings[1:])\n",
    "\n",
    "print(\"\\nСходство между предложениями:\")\n",
    "print(f\"1 vs 2: {similarity[0][0]:.2f}\")\n",
    "print(f\"1 vs 3: {similarity[0][1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст:\n",
      "  Машинное обучение позволяет компьютерам обучаться на данных. Искусственный интеллект меняет мир.\n",
      "Леммы: ['машинный', 'обучение', 'позволять', 'компьютер', 'обучаться', 'на', 'данные', 'искусственный', 'интеллект', 'менять', 'мир']\n",
      "\n",
      "Триплеты: [('интеллект', 'менять', 'мир')]\n",
      "\n",
      "Сходство между предложениями:\n",
      "1 vs 2: 0.58\n",
      "1 vs 3: -0.02\n"
     ]
    }
   ],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Инициализация моделей\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)  # Добавляем синтаксический парсер\n",
    "morph_vocab = MorphVocab()\n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "sbert_model = model # Используйте предварительно загруженную модель\n",
    "\n",
    "# Пример текста с исправлением\n",
    "text = \"Машинное обучение позволяет компьютерам обучаться на данных. Искусственный интеллект меняет мир.\"\n",
    "print(\"Исходный текст:\\n \", text)\n",
    "\n",
    "# Обработка текста с Natasha\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)  # Добавляем синтаксический анализ\n",
    "\n",
    "# Лемматизация с фильтрацией пунктуации\n",
    "lemmas = []\n",
    "for token in doc.tokens:\n",
    "    if token.pos != 'PUNCT':  # Игнорируем пунктуацию\n",
    "        token.lemmatize(morph_vocab)\n",
    "        lemmas.append(token.lemma)\n",
    "print(\"Леммы:\", lemmas)\n",
    "\n",
    "# Извлечение триплетов (исправленная версия)\n",
    "triplets = []\n",
    "for sent in doc.sents:\n",
    "    subj = [token.lemma for token in sent.tokens if token.rel == 'nsubj']\n",
    "    obj = [token.lemma for token in sent.tokens if token.rel == 'obj']\n",
    "    verb = [token.lemma for token in sent.tokens if token.rel == 'root']\n",
    "    \n",
    "    if subj and verb and obj:\n",
    "        triplets.append((' '.join(subj), ' '.join(verb), ' '.join(obj)))\n",
    "\n",
    "print(\"\\nТриплеты:\", triplets)\n",
    "\n",
    "# Семантическое сравнение с SBERT\n",
    "sentences = [\n",
    "    \"нейронные сети анализируют информацию\",\n",
    "    \"искусственный интеллект обрабатывает данные\",\n",
    "    \"погода в Москве сегодня солнечная\"\n",
    "]\n",
    "\n",
    "embeddings = sbert_model.encode(sentences)\n",
    "similarity = cosine_similarity([embeddings[0]], embeddings[1:])\n",
    "\n",
    "print(\"\\nСходство между предложениями:\")\n",
    "print(f\"1 vs 2: {similarity[0][0]:.2f}\")\n",
    "print(f\"1 vs 3: {similarity[0][1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример документов для поиска\n",
    "documents = [\n",
    "    \"Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\",\n",
    "    \"⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\",\n",
    "    \"Таро-прогноз на 23 Июля – Туз Мечей.   🔮 Совет: Меч – это достаточное количество сил для энергичных начинаний, огромная решимость действовать, начало успешной борьбы, готовность к росту, отрыву от изначального существования. Это ситуация, когда сам человек для себя многое прояснил и знает теперь, что хочет делать дальше. Это показатель силы – физической силы, силы воли, силы, приобретенной положением и умом или в силу обстоятельств. Триумф личной силы, власти над обстоятельствами. Триумф может относиться к любой стороне жизни: к работе, любви, денежным делам, духу, любым увлекающим занятиям. Этот Туз не столько начало, но и показатель завоеванного, торжества провозглашенных взглядов и принятых решений.   💰 Деньги: Процветание. Принятие однозначных и окончательных решений в этих вопросах. Возможность улучшить качество жизни, приняв вызов.   🩷 Любовь: В области отношений Туз Мечей символизирует импульсивную первобытную силу, интенсивные «завоевательные» эмоции, крайние чувства, связанные с ситуацией или человеком. Эмоции, которым покровительствует Туз Мечей, способны воспламенить таким огнем, который сожжет все препятствия на пути к цели, попутно причинив немало вреда. Мечи вообще масть холодная, и когда на горизонте в кои-то веки появляется единственная эмоция, то она заполоняет собой всё со свойственной этой масти тотальностью.  🎁🔥 РАСПРОДАЖА КУРСА «ОРАКУЛ ЛЕНОРМАН. БАЗОВЫЙ КУРС» В РАССРОЧКУ 👉🏻 http://alexeygrishin.com/lenorman_base.\",\n",
    "    \"он вообще не собирается переезжать в другое государство\",\n",
    "    \"ты не мог бы набрать меня после обеда\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Леммы: ['твой', 'хороший', 'секс', 'спрятать', 'здесь', 'делиться', 'канал', 'дипломированный', 'сексолог', 'крис', 'взломать', 'код', 'классный', 'секс', 'мастерски', 'раскрепощать', 'знать', 'миллион', 'горячий', 'техника', 'и', 'хороший', 'девайс', 'для', 'взрослый', 'самый', 'полезный', 'пост', 'здесь', 'отрезвлять', 'пост', 'я', 'весь', 'сам', 'прокачать', 'наездница', 'ролевый', 'игра', 'vip', 'кинотеатр', 'техника', 'оральный', 'ласка', 'как', 'заниматься', 'с', 'e', 'ксом', 'неудобный', 'женщина', 'кстати', 'крис', 'провести', 'трехдневный', 'безоплатный', 'онлайн', 'интенсив', 'от', 'бревно', 'до', 'богиня', 'совместно', 'с', 'врач', 'и', 'владелец', 'секс', 'шоп', 'скорый', 'смотри', 'запись', 'пока', 'не', 'удалить', 'здесь', 'жарче', 'чем', 'в', 'ад']\n",
      "Извлеченные триплеты: [('крис я игра техника женщина крис', 'спрятать взломать знать провести удалить', 'код раскрепощать техника пост пост ласка интенсив запись')]\n",
      "Леммы: ['кнопка', 'start']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['таро', 'прогноз', 'на', 'июль', 'туз', 'меч', 'совет', 'меч', 'это', 'достаточный', 'количество', 'сила', 'для', 'энергичный', 'начинание', 'огромный', 'решимость', 'действовать', 'начало', 'успешный', 'борьба', 'готовность', 'к', 'рост', 'отрыв', 'от', 'изначальный', 'существование', 'это', 'ситуация', 'когда', 'сам', 'человек', 'для', 'себя', 'многое', 'прояснить', 'и', 'знать', 'теперь', 'что', 'хотеть', 'делать', 'далекий', 'это', 'показатель', 'сила', 'физический', 'сила', 'сила', 'воля', 'сила', 'приобрести', 'положение', 'и', 'ум', 'или', 'в', 'сила', 'обстоятельство', 'триумф', 'личный', 'сила', 'власть', 'над', 'обстоятельство', 'триумф', 'мочь', 'относиться', 'к', 'любой', 'сторона', 'жизнь', 'к', 'работа', 'любовь', 'денежный', 'дело', 'дух', 'любой', 'увлекать', 'занятие', 'этот', 'туз', 'не', 'столько', 'начать', 'но', 'и', 'показатель', 'завоевать', 'торжество', 'провозгласить', 'взгляд', 'и', 'принять', 'решение', 'деньга', 'процветание', 'принятие', 'однозначный', 'и', 'окончательный', 'решение', 'в', 'этот', 'вопрос', 'возможность', 'улучшить', 'качество', 'жизнь', 'принять', 'вызов', 'любовь', 'в', 'область', 'отношение', 'туз', 'меч', 'символизировать', 'импульсивный', 'первобытный', 'сила', 'интенсивный', 'завоевательный', 'эмоция', 'крайний', 'чувство', 'связать', 'с', 'ситуация', 'или', 'человек', 'эмоция', 'который', 'покровительствовать', 'туз', 'меч', 'способный', 'воспламенить', 'такой', 'огонь', 'который', 'сжечь', 'весь', 'препятствие', 'на', 'путь', 'к', 'цель', 'попутно', 'причинить', 'немало', 'вред', 'меч', 'вообще', 'масть', 'холодный', 'и', 'когда', 'на', 'горизонт', 'в', 'кой', 'то', 'веко', 'появляться', 'единственный', 'эмоция', 'то', 'она', 'заполонять', 'себя', 'весь', 'с', 'свойственный', 'этот', 'масть', 'тотальность', 'распродажа', 'курс', 'оракул', 'ленорман', 'базовый', 'курс', 'в', 'рассрочка']\n",
      "Извлеченные триплеты: [('туз совет это решимость это ситуация человек что это власть туз деньга возможность туз туз который она распродажа', 'мочь начать символизировать способный масть холодный появляться заполонять', 'прогноз количество начало готовность многое показатель триумф торжество решение принятие качество вызов любовь сила эмоция чувство эмоция препятствие немало вред меч весь курс')]\n",
      "Леммы: ['он', 'вообще', 'не', 'собираться', 'переезжать', 'в', 'другой', 'государство']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['ты', 'не', 'мочь', 'бы', 'набрать', 'я', 'после', 'обед']\n",
      "Извлеченные триплеты: [('ты', 'мочь', 'я')]\n"
     ]
    }
   ],
   "source": [
    "# Установка зависимостей (если не установлены)\n",
    "# !pip install natasha sentence-transformers rank_bm25\n",
    "\n",
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Инициализация компонентов\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "sbert_model = model # Используйте предварительно загруженную модель\n",
    "\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"Функция предобработки текста\"\"\"\n",
    "#     # Приведение к нижнему регистру и замена ё\n",
    "#     text = text.lower().replace(\"ё\", \"е\")\n",
    "#     # Удаление пунктуации и цифр\n",
    "#     text = re.sub(r'[^\\w\\s]|[\\d]', ' ', text)\n",
    "#     # Удаление лишних пробелов\n",
    "#     return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Функция предобработки текста с расширенной очисткой\"\"\"\n",
    "    # Приведение к нижнему регистру и замена ё\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    \n",
    "    # Удаление URL-ссылок\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    # Удаление эмодзи и специальных символов\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # эмоции\n",
    "        u'\\U0001F300-\\U0001F5FF'  # символы\n",
    "        u'\\U0001F680-\\U0001F6FF'  # транспорт\n",
    "        u'\\U0001F700-\\U0001F77F'  # алхимия\n",
    "        u'\\U0001F780-\\U0001F7FF'  # геометрические фигуры\n",
    "        u'\\U0001F800-\\U0001F8FF'  # дополнительные символы\n",
    "        u'\\U0001F900-\\U0001F9FF'  # дополнительные символы-2\n",
    "        u'\\U0001FA00-\\U0001FA6F'  # шахматы\n",
    "        u'\\U0001FA70-\\U0001FAFF'  # дополнительные символы-3\n",
    "        u'\\U00002702-\\U000027B0'  # Dingbats\n",
    "        u'\\U000024C2-\\U0001F251'  # Enclosed\n",
    "        ']+', \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(' ', text)\n",
    "    \n",
    "    # Удаление HTML-сущностей и специальных символов\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)\n",
    "    \n",
    "    # Удаление пунктуации, цифр и не-буквенных символов\n",
    "    text = re.sub(r'[^a-zа-я\\s]', ' ', text)\n",
    "    \n",
    "    # Удаление телеграм-упоминаний и бот-команд\n",
    "    text = re.sub(r'@\\w+|/\\w+', ' ', text)\n",
    "    \n",
    "    # Удаление лишних пробелов и обрезка\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "# Предобработка и индексация документов\n",
    "processed_data = []\n",
    "tokenized_corpus = []\n",
    "\n",
    "for doc_id, text in enumerate(documents):\n",
    "    # Предобработка текста\n",
    "    clean_text = preprocess_text(text)\n",
    "    \n",
    "    # Обработка с Natasha\n",
    "    doc = Doc(clean_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    \n",
    "    # Лемматизация\n",
    "    lemmas = []\n",
    "    for token in doc.tokens:\n",
    "        if token.pos != 'PUNCT':\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemmas.append(token.lemma)\n",
    "    \n",
    "    print(\"Леммы:\", lemmas)\n",
    "    \n",
    "    # Для BM25\n",
    "    tokenized_corpus.append(lemmas.copy())\n",
    "    \n",
    "    # Извлечение триплетов\n",
    "    triplets = []\n",
    "    for sent in doc.sents:\n",
    "        subj = [token.lemma for token in sent.tokens if token.rel == 'nsubj']\n",
    "        obj = [token.lemma for token in sent.tokens if token.rel == 'obj']\n",
    "        verb = [token.lemma for token in sent.tokens if token.rel == 'root']\n",
    "        \n",
    "        if subj and verb and obj:\n",
    "            triplets.append((\n",
    "                ' '.join(subj),\n",
    "                ' '.join(verb),\n",
    "                ' '.join(obj)\n",
    "            ))\n",
    "    \n",
    "    # Сохранение данных\n",
    "    processed_data.append({\n",
    "        'original': text,\n",
    "        'lemmas': ' '.join(lemmas),\n",
    "        'triplets': triplets,\n",
    "        'embedding': sbert_model.encode(text)\n",
    "    })\n",
    "\n",
    "    print(\"Извлеченные триплеты:\", triplets)\n",
    "\n",
    "# Инициализация BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def hybrid_search(query, top_n=3):\n",
    "    \"\"\"Гибридный поиск с использованием BM25, триплетов и SBERT\"\"\"\n",
    "    # Предобработка запроса\n",
    "    clean_query = preprocess_text(query)\n",
    "    \n",
    "    # Обработка запроса с Natasha\n",
    "    query_doc = Doc(clean_query)\n",
    "    query_doc.segment(segmenter)\n",
    "    query_doc.tag_morph(morph_tagger)\n",
    "    query_doc.parse_syntax(syntax_parser)\n",
    "    \n",
    "    # Лемматизация запроса\n",
    "    query_lemmas = []\n",
    "    for token in query_doc.tokens:\n",
    "        if token.pos != 'PUNCT':\n",
    "            token.lemmatize(morph_vocab)\n",
    "            query_lemmas.append(token.lemma)\n",
    "    \n",
    "    # Поиск по BM25\n",
    "    bm25_scores = bm25.get_scores(query_lemmas)\n",
    "    bm25_indices = np.argsort(bm25_scores)[-top_n*2:][::-1]\n",
    "    \n",
    "    # Извлечение триплетов запроса\n",
    "    query_triplets = []\n",
    "    for sent in query_doc.sents:\n",
    "        subj = [token.lemma for token in sent.tokens if token.rel == 'nsubj']\n",
    "        obj = [token.lemma for token in sent.tokens if token.rel == 'obj']\n",
    "        verb = [token.lemma for token in sent.tokens if token.rel == 'root']\n",
    "        \n",
    "        if subj and verb and obj:\n",
    "            query_triplets.append((\n",
    "                ' '.join(subj),\n",
    "                ' '.join(verb),\n",
    "                ' '.join(obj)\n",
    "            ))\n",
    "    \n",
    "    # Поиск по триплетам в топе BM25\n",
    "    triplet_matches = []\n",
    "    for idx in bm25_indices:\n",
    "        doc = processed_data[idx]\n",
    "        match_count = sum(1 for t1 in query_triplets for t2 in doc['triplets'] if t1 == t2)\n",
    "        if match_count > 0:\n",
    "            triplet_matches.append(idx)\n",
    "    \n",
    "    # Семантический поиск\n",
    "    query_embedding = sbert_model.encode(query)\n",
    "    similarities = [\n",
    "        cosine_similarity([query_embedding], [doc['embedding']])[0][0]\n",
    "        for doc in processed_data\n",
    "    ]\n",
    "    \n",
    "    # Комбинирование результатов\n",
    "    results = []\n",
    "    for doc_id in bm25_indices:\n",
    "        score = 0.3*bm25_scores[doc_id] + 0.7*similarities[doc_id]\n",
    "        if doc_id in triplet_matches:\n",
    "            score *= 1.2  # Бонус за совпадение триплетов\n",
    "        results.append((doc_id, score))\n",
    "    \n",
    "    # Сортировка и удаление дубликатов\n",
    "    seen = set()\n",
    "    final_results = []\n",
    "    for doc_id, score in sorted(results, key=lambda x: -x[1]):\n",
    "        if doc_id not in seen:\n",
    "            seen.add(doc_id)\n",
    "            final_results.append((doc_id, score))\n",
    "            if len(final_results) >= top_n:\n",
    "                break\n",
    "    \n",
    "    return [(processed_data[i]['original'], score) for i, score in final_results]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: звонить\n",
      "\n",
      "Результат 1 (рейтинг: 0.26):\n",
      "⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\n",
      "\n",
      "Результат 2 (рейтинг: 0.12):\n",
      "ты не мог бы набрать меня после обеда\n",
      "\n",
      "Результат 3 (рейтинг: 0.10):\n",
      "он вообще не собирается переезжать в другое государство\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"звонить\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: гадалка\n",
      "\n",
      "Результат 1 (рейтинг: 0.08):\n",
      "⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\n",
      "\n",
      "Результат 2 (рейтинг: 0.01):\n",
      "Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\n",
      "\n",
      "Результат 3 (рейтинг: 0.01):\n",
      "ты не мог бы набрать меня после обеда\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"гадалка\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: секс\n",
      "\n",
      "Результат 1 (рейтинг: 0.78):\n",
      "Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\n",
      "\n",
      "Результат 2 (рейтинг: 0.15):\n",
      "Таро-прогноз на 23 Июля – Туз Мечей.   🔮 Совет: Меч – это достаточное количество сил для энергичных начинаний, огромная решимость действовать, начало успешной борьбы, готовность к росту, отрыву от изначального существования. Это ситуация, когда сам человек для себя многое прояснил и знает теперь, что хочет делать дальше. Это показатель силы – физической силы, силы воли, силы, приобретенной положением и умом или в силу обстоятельств. Триумф личной силы, власти над обстоятельствами. Триумф может относиться к любой стороне жизни: к работе, любви, денежным делам, духу, любым увлекающим занятиям. Этот Туз не столько начало, но и показатель завоеванного, торжества провозглашенных взглядов и принятых решений.   💰 Деньги: Процветание. Принятие однозначных и окончательных решений в этих вопросах. Возможность улучшить качество жизни, приняв вызов.   🩷 Любовь: В области отношений Туз Мечей символизирует импульсивную первобытную силу, интенсивные «завоевательные» эмоции, крайние чувства, связанные с ситуацией или человеком. Эмоции, которым покровительствует Туз Мечей, способны воспламенить таким огнем, который сожжет все препятствия на пути к цели, попутно причинив немало вреда. Мечи вообще масть холодная, и когда на горизонте в кои-то веки появляется единственная эмоция, то она заполоняет собой всё со свойственной этой масти тотальностью.  🎁🔥 РАСПРОДАЖА КУРСА «ОРАКУЛ ЛЕНОРМАН. БАЗОВЫЙ КУРС» В РАССРОЧКУ 👉🏻 http://alexeygrishin.com/lenorman_base.\n",
      "\n",
      "Результат 3 (рейтинг: 0.06):\n",
      "⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"секс\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Установка данных NLTK](https://www.nltk.org/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('ru-wordnet') # Загрузка русского WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ruwordnet\n",
    "!ruwordnet download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruwordnet import RuWordNet\n",
    "wn = RuWordNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"126228-N\", title=\"СРЕДНЕВЕКОВЫЙ ЗАМОК\")\n",
      "Synset(id=\"114707-N\", title=\"ЗАМОК ДЛЯ ЗАПИРАНИЯ\")\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('замок'):\n",
    "    print(sense.synset)\n",
    "# Synset(id=\"126228-N\", title=\"СРЕДНЕВЕКОВЫЙ ЗАМОК\")\n",
    "# Synset(id=\"114707-N\", title=\"ЗАМОК ДЛЯ ЗАПИРАНИЯ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset(id=\"4454-N\", title=\"СОБАКА\") ['СОБАКА', 'ПЕС', 'СОБАЧКА', 'СОБАЧОНКА', 'ПСИНА', 'ЧЕТВЕРОНОГИЙ ДРУГ', 'ПЕСИК']\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.get_senses('собака'):\n",
    "    print(sense.synset, [synonym.name for synonym in sense.synset.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример документов для поиска\n",
    "documents = [\n",
    "    \"Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\",\n",
    "    \"⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\",\n",
    "    \"Таро-прогноз на 23 Июля – Туз Мечей.   🔮 Совет: Меч – это достаточное количество сил для энергичных начинаний, огромная решимость действовать, начало успешной борьбы, готовность к росту, отрыву от изначального существования. Это ситуация, когда сам человек для себя многое прояснил и знает теперь, что хочет делать дальше. Это показатель силы – физической силы, силы воли, силы, приобретенной положением и умом или в силу обстоятельств. Триумф личной силы, власти над обстоятельствами. Триумф может относиться к любой стороне жизни: к работе, любви, денежным делам, духу, любым увлекающим занятиям. Этот Туз не столько начало, но и показатель завоеванного, торжества провозглашенных взглядов и принятых решений.   💰 Деньги: Процветание. Принятие однозначных и окончательных решений в этих вопросах. Возможность улучшить качество жизни, приняв вызов.   🩷 Любовь: В области отношений Туз Мечей символизирует импульсивную первобытную силу, интенсивные «завоевательные» эмоции, крайние чувства, связанные с ситуацией или человеком. Эмоции, которым покровительствует Туз Мечей, способны воспламенить таким огнем, который сожжет все препятствия на пути к цели, попутно причинив немало вреда. Мечи вообще масть холодная, и когда на горизонте в кои-то веки появляется единственная эмоция, то она заполоняет собой всё со свойственной этой масти тотальностью.  🎁🔥 РАСПРОДАЖА КУРСА «ОРАКУЛ ЛЕНОРМАН. БАЗОВЫЙ КУРС» В РАССРОЧКУ 👉🏻 http://alexeygrishin.com/lenorman_base.\",\n",
    "    \"он вообще не собирается переезжать в другое государство\",\n",
    "    \"ты не мог бы набрать меня после обеда\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Леммы: ['доченька', 'твой', 'совсем', 'больший', 'стать']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['весь', 'дорога', 'забить', 'дерево', 'и', 'цвет']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['в', 'следующий', 'воскресение', 'я', 'собираться', 'в', 'питер']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['у', 'я', 'сломаться', 'стиралка', 'прикинуть']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['садиться', 'в', 'машина', 'и', 'поехать', 'уже']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['сколько', 'стоить', 'ремонт', 'стиральный', 'машина']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['ты', 'когда', 'собираться', 'звонить', 'препод']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['взять', 'пистолет', 'в', 'тумбочка', 'понять', 'я']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['ты', 'не', 'мочь', 'бы', 'набрать', 'я', 'после', 'обед']\n",
      "Извлеченные триплеты: [('ты', 'мочь', 'я')]\n",
      "Леммы: ['ты', 'взять', 'корзина', 'прежде', 'чем', 'набрать', 'продукт']\n",
      "Извлеченные триплеты: [('ты', 'взять', 'корзина продукт')]\n",
      "Леммы: ['он', 'сегодня', 'утро', 'отвезти', 'в', 'близкий', 'госпиталь']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['в', 'этот', 'год', 'смартфон', 'подорожать', 'на', 'процент']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['я', 'еще', 'долг', 'не', 'смочь', 'вернуться', 'в', 'рф']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['мужчина', 'средний', 'год', 'с', 'серый', 'рюкзак', 'и', 'шапка']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['в', 'какой', 'университет', 'собираться', 'поступать', 'твой', 'сын']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['он', 'вообще', 'не', 'собираться', 'переезжать', 'в', 'другой', 'государство']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['сегодня', 'ночь', 'наш', 'судно', 'отплывать', 'по', 'расписание']\n",
      "Извлеченные триплеты: []\n",
      "Леммы: ['его', 'оригинальный', 'портрет', 'выставить', 'на', 'продажа']\n",
      "Извлеченные триплеты: []\n"
     ]
    }
   ],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, MorphVocab\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ruwordnet import RuWordNet\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Инициализация компонентов\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "# Инициализация RuWordNet\n",
    "wn = RuWordNet()  \n",
    "# sbert_model = SentenceTransformer('ai-forever/sbert_large_mt_nlu_ru')\n",
    "sbert_model = model # Используйте предварительно загруженную модель\n",
    "\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"Функция предобработки текста\"\"\"\n",
    "#     # Приведение к нижнему регистру и замена ё\n",
    "#     text = text.lower().replace(\"ё\", \"е\")\n",
    "#     # Удаление пунктуации и цифр\n",
    "#     text = re.sub(r'[^\\w\\s]|[\\d]', ' ', text)\n",
    "#     # Удаление лишних пробелов\n",
    "#     return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Функция предобработки текста с расширенной очисткой\"\"\"\n",
    "    # Приведение к нижнему регистру и замена ё\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    \n",
    "    # Удаление URL-ссылок\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    # Удаление эмодзи и специальных символов\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # эмоции\n",
    "        u'\\U0001F300-\\U0001F5FF'  # символы\n",
    "        u'\\U0001F680-\\U0001F6FF'  # транспорт\n",
    "        u'\\U0001F700-\\U0001F77F'  # алхимия\n",
    "        u'\\U0001F780-\\U0001F7FF'  # геометрические фигуры\n",
    "        u'\\U0001F800-\\U0001F8FF'  # дополнительные символы\n",
    "        u'\\U0001F900-\\U0001F9FF'  # дополнительные символы-2\n",
    "        u'\\U0001FA00-\\U0001FA6F'  # шахматы\n",
    "        u'\\U0001FA70-\\U0001FAFF'  # дополнительные символы-3\n",
    "        u'\\U00002702-\\U000027B0'  # Dingbats\n",
    "        u'\\U000024C2-\\U0001F251'  # Enclosed\n",
    "        ']+', \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(' ', text)\n",
    "    \n",
    "    # Удаление HTML-сущностей и специальных символов\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)\n",
    "    \n",
    "    # Удаление пунктуации, цифр и не-буквенных символов\n",
    "    text = re.sub(r'[^a-zа-я\\s]', ' ', text)\n",
    "    \n",
    "    # Удаление телеграм-упоминаний и бот-команд\n",
    "    text = re.sub(r'@\\w+|/\\w+', ' ', text)\n",
    "    \n",
    "    # Удаление лишних пробелов и обрезка\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "# Предобработка и индексация документов\n",
    "processed_data = []\n",
    "tokenized_corpus = []\n",
    "\n",
    "for doc_id, text in enumerate(documents):\n",
    "    # Предобработка текста\n",
    "    clean_text = preprocess_text(text)\n",
    "    \n",
    "    # Обработка с Natasha\n",
    "    doc = Doc(clean_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    \n",
    "    # Лемматизация\n",
    "    lemmas = []\n",
    "    for token in doc.tokens:\n",
    "        if token.pos != 'PUNCT':\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemmas.append(token.lemma)\n",
    "    \n",
    "    print(\"Леммы:\", lemmas)\n",
    "    \n",
    "    # Для BM25\n",
    "    tokenized_corpus.append(lemmas.copy())\n",
    "    \n",
    "    # Извлечение триплетов\n",
    "    triplets = []\n",
    "    for sent in doc.sents:\n",
    "        subj = [token.lemma for token in sent.tokens if token.rel == 'nsubj']\n",
    "        obj = [token.lemma for token in sent.tokens if token.rel == 'obj']\n",
    "        verb = [token.lemma for token in sent.tokens if token.rel == 'root']\n",
    "        \n",
    "        if subj and verb and obj:\n",
    "            triplets.append((\n",
    "                ' '.join(subj),\n",
    "                ' '.join(verb),\n",
    "                ' '.join(obj)\n",
    "            ))\n",
    "    \n",
    "    # Сохранение данных\n",
    "    processed_data.append({\n",
    "        'original': text,\n",
    "        'lemmas': ' '.join(lemmas),\n",
    "        'triplets': triplets,\n",
    "        'embedding': sbert_model.encode(text)\n",
    "    })\n",
    "\n",
    "    print(\"Извлеченные триплеты:\", triplets)\n",
    "\n",
    "# Инициализация BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def expand_with_ruwordnet(lemmas):\n",
    "    \"\"\"Расширение лемм с помощью RuWordNet\"\"\"\n",
    "    expanded = []\n",
    "    for lemma in lemmas:\n",
    "        expanded.append(lemma)\n",
    "        # Поиск синсетов для леммы\n",
    "        synsets = wn.get_synsets(lemma)\n",
    "        for synset in synsets:\n",
    "            # Добавляем все синонимы из синсета\n",
    "            expanded.extend([sense.name.lower() for sense in synset.senses])\n",
    "    return list(set(expanded))  # Удаляем дубликаты\n",
    "\n",
    "def hybrid_search(query, top_n=3):\n",
    "    \"\"\"Гибридный поиск с расширением синонимов\"\"\"\n",
    "    # Предобработка запроса\n",
    "    clean_query = preprocess_text(query)\n",
    "    \n",
    "    # Обработка запроса с Natasha\n",
    "    query_doc = Doc(clean_query)\n",
    "    query_doc.segment(segmenter)\n",
    "    query_doc.tag_morph(morph_tagger)\n",
    "    query_doc.parse_syntax(syntax_parser)\n",
    "    \n",
    "    # Лемматизация запроса\n",
    "    query_lemmas = []\n",
    "    for token in query_doc.tokens:\n",
    "        if token.pos != 'PUNCT':\n",
    "            token.lemmatize(morph_vocab)\n",
    "            query_lemmas.append(token.lemma.lower())\n",
    "    \n",
    "    # Расширение синонимами RuWordNet\n",
    "    expanded_lemmas = expand_with_ruwordnet(query_lemmas)\n",
    "    print(f\"Расширенные леммы: {expanded_lemmas}\")\n",
    "    \n",
    "    # Поиск по BM25 с расширенными леммами\n",
    "    bm25_scores = bm25.get_scores(expanded_lemmas)\n",
    "    bm25_indices = np.argsort(bm25_scores)[-top_n*2:][::-1]\n",
    "    \n",
    "    # Извлечение триплетов запроса\n",
    "    query_triplets = []\n",
    "    for sent in query_doc.sents:\n",
    "        subj = [token.lemma for token in sent.tokens if token.rel == 'nsubj']\n",
    "        obj = [token.lemma for token in sent.tokens if token.rel == 'obj']\n",
    "        verb = [token.lemma for token in sent.tokens if token.rel == 'root']\n",
    "        \n",
    "        if subj and verb and obj:\n",
    "            query_triplets.append((\n",
    "                ' '.join(subj),\n",
    "                ' '.join(verb),\n",
    "                ' '.join(obj)\n",
    "            ))\n",
    "    \n",
    "    # Поиск по триплетам в топе BM25\n",
    "    triplet_matches = []\n",
    "    for idx in bm25_indices:\n",
    "        doc = processed_data[idx]\n",
    "        match_count = sum(1 for t1 in query_triplets for t2 in doc['triplets'] if t1 == t2)\n",
    "        if match_count > 0:\n",
    "            triplet_matches.append(idx)\n",
    "    \n",
    "    # Семантический поиск\n",
    "    query_embedding = sbert_model.encode(query)\n",
    "    similarities = [\n",
    "        cosine_similarity([query_embedding], [doc['embedding']])[0][0]\n",
    "        for doc in processed_data\n",
    "    ]\n",
    "    \n",
    "    # Комбинирование результатов\n",
    "    # results = []\n",
    "    # for doc_id in bm25_indices:\n",
    "    #     score = 0.3*bm25_scores[doc_id] + 0.7*similarities[doc_id]\n",
    "    #     if doc_id in triplet_matches:\n",
    "    #         score *= 1.2  # Бонус за совпадение триплетов\n",
    "    #     results.append((doc_id, score))\n",
    "    \n",
    "    # Комбинирование результатов\n",
    "    results = []\n",
    "    for doc_id in bm25_indices:\n",
    "        doc = processed_data[doc_id]\n",
    "        \n",
    "        # Бонусы\n",
    "        length_boost = 1.5 if len(doc['lemmas'].split()) < 10 else 1.0\n",
    "        exact_match = 1.2 if any(lem in expanded_lemmas for lem in doc['lemmas'].split()) else 1.0\n",
    "        \n",
    "        score = (\n",
    "            0.3 * bm25_scores[doc_id] +\n",
    "            0.7 * cosine_similarity([query_embedding], [doc['embedding']])[0][0]\n",
    "        ) * length_boost * exact_match\n",
    "        \n",
    "        results.append((doc_id, score))\n",
    "\n",
    "    # Сортировка и удаление дубликатов\n",
    "    seen = set()\n",
    "    final_results = []\n",
    "    for doc_id, score in sorted(results, key=lambda x: -x[1]):\n",
    "        if doc_id not in seen:\n",
    "            seen.add(doc_id)\n",
    "            final_results.append((doc_id, score))\n",
    "            if len(final_results) >= top_n:\n",
    "                break\n",
    "    \n",
    "    return [(processed_data[i]['original'], score) for i, score in final_results]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: звонить\n",
      "Расширенные леммы: ['позвонить по телефону', 'говорить по телефону', 'звякнуть', 'отзванивать', 'созваниваться', 'издавать звон', 'созвониться', 'зазвонить', 'беседовать по телефону', 'звякнуть по телефону', 'издать звон', 'разговаривать по телефону', 'пользоваться телефоном', 'отзвониться', 'звонить', 'перезваниваться', 'звонить по телефону', 'позвонить', 'названивать', 'отзвонить']\n",
      "\n",
      "Результат 1 (рейтинг: 0.39):\n",
      "⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\n",
      "\n",
      "Результат 2 (рейтинг: 0.18):\n",
      "ты не мог бы набрать меня после обеда\n",
      "\n",
      "Результат 3 (рейтинг: 0.15):\n",
      "он вообще не собирается переезжать в другое государство\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"звонить\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: гадалка\n",
      "Расширенные леммы: ['гадалка', 'ворожея']\n",
      "\n",
      "Результат 1 (рейтинг: 0.12):\n",
      "⭐️  Кнопка: ⭐️START⭐️(https://t.me/major/start?startapp=1972869792)\n",
      "\n",
      "Результат 2 (рейтинг: 0.01):\n",
      "ты не мог бы набрать меня после обеда\n",
      "\n",
      "Результат 3 (рейтинг: 0.01):\n",
      "Твой лучший секс спрятан здесь 🔞  Делюсь каналом дипломированного сексолога. Крис взломала код классного секса, мастерски раскрепощает, знает миллион горячих техник и лучшие девайсы для взрослых 😻  Самые полезные посты здесь:   Отрезвляющий пост «Я все сама!»   Прокачай наездницу  Ролевая игра «VIP кинотеатр»   Техника оральных ласк 💣   Как занимается сeксом неудобная женщина   Кстати, Крис провела трехдневный безоплатный онлайн интенсив-«От бревна до Богини». Совместно с врачом и владельцем секс-шопа.   Скорее смотри записи, пока не удалила 🔞  https://t.me/sekretskris/1048   Здесь жарче, чем в аду 😈\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"гадалка\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример документов для поиска\n",
    "documents = [\n",
    "    \"доченька твоя совсем большая стала\",\n",
    "    \"вся дорога забита деревьями и цветами\",\n",
    "    \"в следующее воскресенье я собираюсь в питер\",\n",
    "    \"у меня сломалась стиралка прикинь\",\n",
    "    \"садись в машину и поехали уже\",\n",
    "    \"сколько стоит ремонт стиральной машины\",\n",
    "    \"ты когда собираешься звонить преподу\",\n",
    "    \"возьмешь пистолет в тумбочке понял меня\",\n",
    "    \"ты не мог бы набрать меня после обеда\",\n",
    "    \"ты возьми корзину прежде чем набрать продукты\",\n",
    "    \"его сегодня утром отвезли в ближайший госпиталь\",\n",
    "    \"в этом году смартфоны подорожают на 30 процентов\",\n",
    "    \"я еще долгу не смогу вернуться в рф\",\n",
    "    \"мужчина средних лет с серым рюкзаком и шапкой\",\n",
    "    \"в какой университет собирается поступать твой сын\",\n",
    "    \"он вообще не собирается переезжать в другое государство\",\n",
    "    \"сегодня ночью наше судно отплывает по расписанию\",\n",
    "    \"его оригинальный портрет выставлен на продажу\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: звонить\n",
      "Расширенные леммы: ['позвонить по телефону', 'говорить по телефону', 'звякнуть', 'отзванивать', 'созваниваться', 'издавать звон', 'созвониться', 'зазвонить', 'беседовать по телефону', 'звякнуть по телефону', 'издать звон', 'разговаривать по телефону', 'пользоваться телефоном', 'отзвониться', 'звонить', 'перезваниваться', 'звонить по телефону', 'позвонить', 'названивать', 'отзвонить']\n",
      "\n",
      "Результат 1 (рейтинг: 1.89):\n",
      "ты когда собираешься звонить преподу\n",
      "\n",
      "Результат 2 (рейтинг: 0.15):\n",
      "в какой университет собирается поступать твой сын\n",
      "\n",
      "Результат 3 (рейтинг: 0.15):\n",
      "он вообще не собирается переезжать в другое государство\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"звонить\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: подлинная картина\n",
      "Расширенные леммы: ['реальная картина', 'картина спектакля', 'киноальманах', 'кинематографическая продукция', 'произведение живописи', 'живопись', 'кинокартина', 'подлинный', 'кинопродукция', 'кинопроизведение', 'зрелище', 'картина художника', 'киноработа', 'киношка', 'кинематографическое произведение', 'полотно художника', 'словесная картина', 'киноэпопея', 'полотно', 'кинолента', 'картина', 'картина положения', 'неподдельный', 'живописное произведение', 'живописная работа', 'киносериал', 'картина происходящего', 'настоящий', 'живописное полотно', 'кинофильм', 'полная картина', 'картина состояния', 'общая картина', 'продукция киностудий', 'действительный', 'целостная картина']\n",
      "\n",
      "Результат 1 (рейтинг: 0.57):\n",
      "его оригинальный портрет выставлен на продажу\n",
      "\n",
      "Результат 2 (рейтинг: 0.14):\n",
      "сегодня ночью наше судно отплывает по расписанию\n",
      "\n",
      "Результат 3 (рейтинг: 0.08):\n",
      "я еще долгу не смогу вернуться в рф\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"подлинная картина\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: стиральная машина\n",
      "Расширенные леммы: ['технический инструмент', 'технологическое устройство', 'техническое приспособление', 'машинная продукция', 'машинка', 'стиральный', 'машинно-техническая продукция', 'технический аппарат', 'авто', 'средство автотранспорта', 'автомашина', 'машина', 'механизм', 'устройство', 'техника', 'аппаратура', 'техническое средство', 'автомобиль', 'техническое устройство', 'техническое оборудование', 'техническая система', 'автотранспорт', 'автотранспортное средство', 'автосредство', 'аппарат', 'техническая аппаратура', 'аппаратное средство', 'технологическая система', 'техсредство', 'система']\n",
      "\n",
      "Результат 1 (рейтинг: 3.36):\n",
      "сколько стоит ремонт стиральной машины\n",
      "\n",
      "Результат 2 (рейтинг: 1.23):\n",
      "садись в машину и поехали уже\n",
      "\n",
      "Результат 3 (рейтинг: 0.21):\n",
      "сегодня ночью наше судно отплывает по расписанию\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"стиральная машина\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска для запроса: оружие\n",
      "Расширенные леммы: ['оружие для достижения цели', 'орудие для достижения', 'орудие для достижения цели', 'оружие', 'оружие для достижения']\n",
      "\n",
      "Результат 1 (рейтинг: 0.20):\n",
      "мужчина средних лет с серым рюкзаком и шапкой\n",
      "\n",
      "Результат 2 (рейтинг: 0.17):\n",
      "в какой университет собирается поступать твой сын\n",
      "\n",
      "Результат 3 (рейтинг: 0.14):\n",
      "сегодня ночью наше судно отплывает по расписанию\n"
     ]
    }
   ],
   "source": [
    "# Пример поиска\n",
    "query = \"оружие\"\n",
    "print(\"Результаты поиска для запроса:\", query)\n",
    "for i, (text, score) in enumerate(hybrid_search(query)):\n",
    "    print(f\"\\nРезультат {i+1} (рейтинг: {score:.2f}):\")\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
